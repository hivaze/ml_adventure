{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import datasets\n",
    "import evaluate\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Util functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output.last_hidden_state  #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def compute_clm_loss(logits, labels):\n",
    "    # Classical Language modeling task (nope)\n",
    "    # Next token prediction task in causual setup\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "    shift_labels = labels[..., 1:].contiguous()\n",
    "    loss = F.cross_entropy(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1), ignore_index=-100)\n",
    "    return loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def compute_cosine_mse_loss(input_logits, output_logits, padding_mask):\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    batch_size = padding_mask.shape[0]\n",
    "    for batch_idx in range(batch_size):\n",
    "        preds = input_logits[batch_idx, :][~padding_mask[batch_idx, :]]\n",
    "        targets = output_logits[batch_idx, :][~padding_mask[batch_idx, :]]\n",
    "        # print(preds.shape, targets.shape)\n",
    "\n",
    "        loss = (1 - F.cosine_similarity(preds, targets, -1)).mean() + F.mse_loss(preds, targets)\n",
    "        losses.append(loss)\n",
    "\n",
    "    return torch.stack(losses).contiguous().mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def compute_shifted_cosine_mse_loss(input_logits, output_logits, padding_mask):\n",
    "\n",
    "    rolled_input_mask = torch.roll(padding_mask, -1)\n",
    "    shift_input = input_logits[:, :-1, :][~rolled_input_mask[:, :-1]].contiguous()\n",
    "    shift_output = output_logits[:, 1:, :][~padding_mask[:, 1:]].contiguous()\n",
    "\n",
    "    cos_loss = (1 - F.cosine_similarity(shift_input, shift_output, -1)).mean()\n",
    "    mse_loss = F.mse_loss(shift_input, shift_output)\n",
    "\n",
    "    return cos_loss + mse_loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def compute_shifted_mse_loss(input_logits, output_logits, padding_mask):\n",
    "\n",
    "    rolled_input_mask = torch.roll(padding_mask, -1)\n",
    "    shift_input = input_logits[:, :-1, :][~rolled_input_mask[:, :-1]].contiguous()\n",
    "    shift_output = output_logits[:, 1:, :][~padding_mask[:, 1:]].contiguous()\n",
    "\n",
    "    mse_loss = F.mse_loss(shift_input, shift_output)\n",
    "\n",
    "    return mse_loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def compute_shifted_cross_l2_loss(input_logits, output_logits, padding_mask):\n",
    "    losses = []\n",
    "\n",
    "    batch_size = padding_mask.shape[0]\n",
    "    for batch_idx in range(batch_size):\n",
    "\n",
    "        preds = input_logits[batch_idx, :][~padding_mask[batch_idx, :]]\n",
    "        targets = output_logits[batch_idx, :][~padding_mask[batch_idx, :]]\n",
    "\n",
    "        orig_distances = torch.cdist(targets[1:], targets[1:]).detach()\n",
    "        pred_distances = torch.cdist(preds[:-1], preds[:-1])\n",
    "\n",
    "        idx = torch.triu_indices(*orig_distances.shape)\n",
    "        loss = F.mse_loss(pred_distances[idx[0], idx[1]].view(-1), orig_distances[idx[0], idx[1]].view(-1))\n",
    "        losses.append(loss)\n",
    "\n",
    "    return torch.stack(losses).contiguous().mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Losses Playground"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "example_output = torch.rand([5, 8, 100], dtype=torch.float)\n",
    "example_input = example_output.clone()\n",
    "example_input[:, :-1] = example_output[:, 1:]\n",
    "# compute_shifted_mse_loss(example_input, example_output, torch.zeros(5, 8).bool())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(0.)"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.mse_loss(example_input[0][:-1], example_output[0][1:])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(0.)"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.mse_loss(torch.cdist(example_output[0][1:], example_output[0][1:]).view(-1),\n",
    "           torch.cdist(example_input[0][:-1], example_input[0][:-1]).view(-1))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.0000, 4.3862, 4.6061, 4.6068, 4.4111, 3.9176, 4.2555],\n        [4.3862, 0.0000, 3.6091, 4.4636, 4.3510, 4.0947, 4.1751],\n        [4.6061, 3.6091, 0.0000, 4.4537, 4.0674, 4.1071, 3.9881],\n        [4.6068, 4.4636, 4.4537, 0.0000, 4.2209, 4.4106, 3.9097],\n        [4.4111, 4.3510, 4.0674, 4.2209, 0.0000, 4.3559, 3.9257],\n        [3.9176, 4.0947, 4.1071, 4.4106, 4.3559, 0.0000, 4.0253],\n        [4.2555, 4.1751, 3.9881, 3.9097, 3.9257, 4.0253, 0.0000]])"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cdist(example_input[0][:-1], example_input[0][:-1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.0000, 4.3862, 4.6061, 4.6068, 4.4111, 3.9176, 4.2555],\n        [4.3862, 0.0000, 3.6091, 4.4636, 4.3510, 4.0947, 4.1751],\n        [4.6061, 3.6091, 0.0000, 4.4537, 4.0674, 4.1071, 3.9881],\n        [4.6068, 4.4636, 4.4537, 0.0000, 4.2209, 4.4106, 3.9097],\n        [4.4111, 4.3510, 4.0674, 4.2209, 0.0000, 4.3559, 3.9257],\n        [3.9176, 4.0947, 4.1071, 4.4106, 4.3559, 0.0000, 4.0253],\n        [4.2555, 4.1751, 3.9881, 3.9097, 3.9257, 4.0253, 0.0000]])"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cdist(example_output[0][1:], example_output[0][1:])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.0000, 4.3862, 4.6061, 4.6068, 4.4111, 3.9176, 4.2555],\n        [4.3862, 0.0000, 3.6091, 4.4636, 4.3510, 4.0947, 4.1751],\n        [4.6061, 3.6091, 0.0000, 4.4537, 4.0674, 4.1071, 3.9881],\n        [4.6068, 4.4636, 4.4537, 0.0000, 4.2209, 4.4106, 3.9097],\n        [4.4111, 4.3510, 4.0674, 4.2209, 0.0000, 4.3559, 3.9257],\n        [3.9176, 4.0947, 4.1071, 4.4106, 4.3559, 0.0000, 4.0253],\n        [4.2555, 4.1751, 3.9881, 3.9097, 3.9257, 4.0253, 0.0000]])"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cdist(example_input[0][:-1], example_output[0][1:])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using allenai/soda dataset\n",
    "This dataset contains dialogs, speakers labels, each dialog is a separate list of lines"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading readme:   0%|          | 0.00/4.92k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a8343d6e91984d7e81a02e2ef6268140"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset parquet/allenai--soda to /home/hivaze/.cache/huggingface/datasets/allenai___parquet/allenai--soda-354e990899ae2f4a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bce74bb333be4852a9e550436aaa7d7d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading data:   0%|          | 0.00/689M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b9c7602f818e40f092d85b7405e065e7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading data:   0%|          | 0.00/84.2M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2f1a1b3133744dfc889e466641209357"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading data:   0%|          | 0.00/82.9M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d0886ba9b31744559017b5bcbce1a1bd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a0a8d99345db4ee29bedf6b9ff2f34ae"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating train split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d95fa954e0634745b428f3fe3b61ab64"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating test split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fb79dd8293824d8fa95ad47915c43508"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating validation split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "13b82dd5a0654758be199f43ad85b1d4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/hivaze/.cache/huggingface/datasets/allenai___parquet/allenai--soda-354e990899ae2f4a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4e85f51921db4b5fb7d5567dd7c422c5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['head', 'relation', 'tail', 'literal', 'narrative', 'dialogue', 'speakers', 'PersonX', 'PersonY', 'PersonZ', 'original_index', 'split', 'head_answer', 'pmi_head_answer', 'relation_tail_answer', 'pmi_relation_tail_answer'],\n        num_rows: 1191582\n    })\n    test: Dataset({\n        features: ['head', 'relation', 'tail', 'literal', 'narrative', 'dialogue', 'speakers', 'PersonX', 'PersonY', 'PersonZ', 'original_index', 'split', 'head_answer', 'pmi_head_answer', 'relation_tail_answer', 'pmi_relation_tail_answer'],\n        num_rows: 148968\n    })\n    validation: Dataset({\n        features: ['head', 'relation', 'tail', 'literal', 'narrative', 'dialogue', 'speakers', 'PersonX', 'PersonY', 'PersonZ', 'original_index', 'split', 'head_answer', 'pmi_head_answer', 'relation_tail_answer', 'pmi_relation_tail_answer'],\n        num_rows: 146346\n    })\n})"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soda_dataset = datasets.load_dataset('allenai/soda')\n",
    "soda_dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['dialogue', 'speakers'],\n        num_rows: 1191582\n    })\n    test: Dataset({\n        features: ['dialogue', 'speakers'],\n        num_rows: 148968\n    })\n    validation: Dataset({\n        features: ['dialogue', 'speakers'],\n        num_rows: 146346\n    })\n})"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soda_dataset = soda_dataset.remove_columns([col for col in soda_dataset['train'].column_names if col not in ['dialogue', 'speakers']])\n",
    "soda_dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def encode_interlocutors(row):\n",
    "    first_speaker = row['speakers'][0]\n",
    "    other_speaker = list(set(row['speakers']) - {first_speaker})[0]\n",
    "    mapping = {\n",
    "        first_speaker: 2,\n",
    "        other_speaker: 3\n",
    "    }\n",
    "    fixed_speakers = list(map(lambda name: mapping.get(name, 2), row['speakers']))\n",
    "    return {\n",
    "        'speakers': fixed_speakers\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# {'ek': 34, 'mf3': 54}.get(1, 10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "Map (num_proc=11):   0%|          | 0/1191582 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "48e650642e53403db531c7ac6059bc9b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map (num_proc=11):   0%|          | 0/148968 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e4866c18cd3840e98811222fdcc8254f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map (num_proc=11):   0%|          | 0/146346 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ad8714d2cdf941448f69d4b9317d487e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['dialogue', 'speakers'],\n        num_rows: 1191582\n    })\n    test: Dataset({\n        features: ['dialogue', 'speakers'],\n        num_rows: 148968\n    })\n    validation: Dataset({\n        features: ['dialogue', 'speakers'],\n        num_rows: 146346\n    })\n})"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soda_dataset = soda_dataset.map(encode_interlocutors, num_proc=11)\n",
    "soda_dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['dialog', 'interlocutors'],\n        num_rows: 1191582\n    })\n    test: Dataset({\n        features: ['dialog', 'interlocutors'],\n        num_rows: 148968\n    })\n    validation: Dataset({\n        features: ['dialog', 'interlocutors'],\n        num_rows: 146346\n    })\n})"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soda_dataset = soda_dataset.rename_columns({'dialogue': 'dialog', 'speakers': 'interlocutors'})\n",
    "soda_dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "Filter:   0%|          | 0/1191582 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "859f4a067e404e0a915195fb77ac9804"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Filter:   0%|          | 0/148968 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "92a4f846e15e4074a5041b390cf36762"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Filter:   0%|          | 0/146346 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ed7f30a58f4e46dc814b4a722913ee85"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['dialog', 'interlocutors'],\n        num_rows: 1186423\n    })\n    test: Dataset({\n        features: ['dialog', 'interlocutors'],\n        num_rows: 148294\n    })\n    validation: Dataset({\n        features: ['dialog', 'interlocutors'],\n        num_rows: 145660\n    })\n})"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soda_dataset = soda_dataset.filter(lambda row: len(row['dialog']) == len(row['interlocutors']) and len(row['dialog']) > 1)\n",
    "soda_dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "speakers_counts = [len(set(x)) for x in soda_dataset['validation']['interlocutors']]\n",
    "plt.hist(speakers_counts)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjkAAAGdCAYAAADwjmIIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAuSElEQVR4nO3de3QUZZ7/8U8u5CLQjVySkCVAZmGAyE0ChhYvy5qldaIzCMyAg5hB1AObIEmUm2JE1hXEVQG5ZNA9E/asrMA5CwNkCGaDhFHCLZgRkEQd0aCxE1xJWjKQhHT//phfaukBhYZAk4f365w6x9TzrarvU0jnQ6WqEuT1er0CAAAwTHCgGwAAALgaCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEYi5AAAACOFBrqBQPJ4PKqsrFT79u0VFBQU6HYAAMAl8Hq9+v777xUbG6vg4B++XnNDh5zKykrFxcUFug0AAHAZjh8/rm7duv3g+A0dctq3by/pryfJZrMFuBsAAHAp3G634uLirO/jP+SGDjnNP6Ky2WyEHAAAWpmL3WrCjccAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYCRCDgAAMBIhBwAAGImQAwAAjETIAQAARgoNdAO4fvSckxfoFvz2xaKUQLcAALhOcSUHAAAYiZADAACMRMgBAABGIuQAAAAjEXIAAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYCRCDgAAMBIhBwAAGImQAwAAjETIAQAARiLkAAAAI/kdcr7++ms9/PDD6tSpkyIjIzVgwAAdOHDAGvd6vcrOzlbXrl0VGRmp5ORkffrppz77+O677zRx4kTZbDZ16NBBU6ZM0alTp3xqPvroI915552KiIhQXFycFi9efF4vGzZsUN++fRUREaEBAwboD3/4g7/TAQAAhvIr5Jw8eVIjRoxQmzZttG3bNn388cd69dVXdfPNN1s1ixcv1rJly5STk6O9e/eqbdu2cjqdOnPmjFUzceJEHTlyRAUFBdq6dat27dqlJ554whp3u90aNWqUevTooZKSEr3yyiuaP3++Vq9ebdXs3r1bDz30kKZMmaIPP/xQo0eP1ujRo3X48OErOR8AAMAQQV6v13upxXPmzNEHH3ygP/7xjxcc93q9io2N1VNPPaWnn35aklRbW6vo6Gjl5uZqwoQJOnr0qBISErR//34NHTpUkpSfn6+f/exn+uqrrxQbG6tVq1bp2WeflcvlUlhYmHXsTZs2qaysTJI0fvx41dXVaevWrdbxhw8frsGDBysnJ+eS5uN2u2W321VbWyubzXapp8FYPefkBboFv32xKCXQLQAArrFL/f7t15WczZs3a+jQofrlL3+pqKgo3XrrrXrzzTet8WPHjsnlcik5OdlaZ7fblZSUpOLiYklScXGxOnToYAUcSUpOTlZwcLD27t1r1dx1111WwJEkp9Op8vJynTx50qo59zjNNc3HuZD6+nq53W6fBQAAmMmvkPP5559r1apV6t27t7Zv365p06bpySef1Jo1ayRJLpdLkhQdHe2zXXR0tDXmcrkUFRXlMx4aGqqOHTv61FxoH+ce44dqmscvZOHChbLb7dYSFxfnz/QBAEAr4lfI8Xg8GjJkiF566SXdeuuteuKJJ/T4449f8o+HAm3u3Lmqra21luPHjwe6JQAAcJX4FXK6du2qhIQEn3X9+vVTRUWFJCkmJkaSVFVV5VNTVVVljcXExKi6utpn/OzZs/ruu+98ai60j3OP8UM1zeMXEh4eLpvN5rMAAAAz+RVyRowYofLycp91n3zyiXr06CFJio+PV0xMjAoLC61xt9utvXv3yuFwSJIcDodqampUUlJi1ezYsUMej0dJSUlWza5du9TY2GjVFBQUqE+fPtaTXA6Hw+c4zTXNxwEAADc2v0JOZmam9uzZo5deekmfffaZ1q5dq9WrVystLU2SFBQUpIyMDL344ovavHmzDh06pEceeUSxsbEaPXq0pL9e+bn33nv1+OOPa9++ffrggw+Unp6uCRMmKDY2VpL061//WmFhYZoyZYqOHDmidevWaenSpcrKyrJ6mTFjhvLz8/Xqq6+qrKxM8+fP14EDB5Sent5CpwYAALRmof4UDxs2TBs3btTcuXO1YMECxcfHa8mSJZo4caJVM2vWLNXV1emJJ55QTU2N7rjjDuXn5ysiIsKqefvtt5Wenq577rlHwcHBGjt2rJYtW2aN2+12vfvuu0pLS1NiYqI6d+6s7Oxsn3fp3H777Vq7dq3mzZunZ555Rr1799amTZvUv3//KzkfAADAEH69J8c0vCfHF+/JAQC0BlflPTkAAACtBSEHAAAYiZADAACMRMgBAABGIuQAAAAjEXIAAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYCRCDgAAMBIhBwAAGImQAwAAjETIAQAARiLkAAAAIxFyAACAkQg5AADASIQcAABgJEIOAAAwEiEHAAAYiZADAACMRMgBAABGIuQAAAAjEXIAAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYCRCDgAAMBIhBwAAGImQAwAAjORXyJk/f76CgoJ8lr59+1rjZ86cUVpamjp16qR27dpp7Nixqqqq8tlHRUWFUlJSdNNNNykqKkozZ87U2bNnfWp27typIUOGKDw8XL169VJubu55vaxYsUI9e/ZURESEkpKStG/fPn+mAgAADOf3lZxbbrlF33zzjbW8//771lhmZqa2bNmiDRs2qKioSJWVlRozZow13tTUpJSUFDU0NGj37t1as2aNcnNzlZ2dbdUcO3ZMKSkpGjlypEpLS5WRkaHHHntM27dvt2rWrVunrKwsPf/88zp48KAGDRokp9Op6urqyz0PAADAMEFer9d7qcXz58/Xpk2bVFpaet5YbW2tunTporVr12rcuHGSpLKyMvXr10/FxcUaPny4tm3bpvvvv1+VlZWKjo6WJOXk5Gj27Nk6ceKEwsLCNHv2bOXl5enw4cPWvidMmKCamhrl5+dLkpKSkjRs2DAtX75ckuTxeBQXF6fp06drzpw5lzx5t9stu92u2tpa2Wy2S97OVD3n5AW6Bb99sSgl0C0AAK6xS/3+7feVnE8//VSxsbH6yU9+ookTJ6qiokKSVFJSosbGRiUnJ1u1ffv2Vffu3VVcXCxJKi4u1oABA6yAI0lOp1Nut1tHjhyxas7dR3NN8z4aGhpUUlLiUxMcHKzk5GSrBgAAINSf4qSkJOXm5qpPnz765ptv9MILL+jOO+/U4cOH5XK5FBYWpg4dOvhsEx0dLZfLJUlyuVw+Aad5vHnsx2rcbrdOnz6tkydPqqmp6YI1ZWVlP9p/fX296uvrra/dbvelTx4AALQqfoWc++67z/rvgQMHKikpST169ND69esVGRnZ4s21tIULF+qFF14IdBsAAOAauKJHyDt06KCf/vSn+uyzzxQTE6OGhgbV1NT41FRVVSkmJkaSFBMTc97TVs1fX6zGZrMpMjJSnTt3VkhIyAVrmvfxQ+bOnava2lprOX78uN9zBgAArcMVhZxTp07pz3/+s7p27arExES1adNGhYWF1nh5ebkqKirkcDgkSQ6HQ4cOHfJ5CqqgoEA2m00JCQlWzbn7aK5p3kdYWJgSExN9ajwejwoLC62aHxIeHi6bzeazAAAAM/kVcp5++mkVFRXpiy++0O7du/Xggw8qJCREDz30kOx2u6ZMmaKsrCy99957Kikp0eTJk+VwODR8+HBJ0qhRo5SQkKBJkybpT3/6k7Zv36558+YpLS1N4eHhkqSpU6fq888/16xZs1RWVqaVK1dq/fr1yszMtPrIysrSm2++qTVr1ujo0aOaNm2a6urqNHny5BY8NQAAoDXz656cr776Sg899JD+93//V126dNEdd9yhPXv2qEuXLpKk119/XcHBwRo7dqzq6+vldDq1cuVKa/uQkBBt3bpV06ZNk8PhUNu2bZWamqoFCxZYNfHx8crLy1NmZqaWLl2qbt266a233pLT6bRqxo8frxMnTig7O1sul0uDBw9Wfn7+eTcjAwCAG5df78kxDe/J8cV7cgAArcFVe08OAABAa0DIAQAARiLkAAAAIxFyAACAkQg5AADASIQcAABgJEIOAAAwEiEHAAAYiZADAACMRMgBAABGIuQAAAAjEXIAAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYCRCDgAAMBIhBwAAGImQAwAAjETIAQAARiLkAAAAIxFyAACAkQg5AADASIQcAABgJEIOAAAwEiEHAAAYiZADAACMRMgBAABGIuQAAAAjEXIAAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEYi5AAAACMRcgAAgJGuKOQsWrRIQUFBysjIsNadOXNGaWlp6tSpk9q1a6exY8eqqqrKZ7uKigqlpKTopptuUlRUlGbOnKmzZ8/61OzcuVNDhgxReHi4evXqpdzc3POOv2LFCvXs2VMRERFKSkrSvn37rmQ6AADAIJcdcvbv36/f/va3GjhwoM/6zMxMbdmyRRs2bFBRUZEqKys1ZswYa7ypqUkpKSlqaGjQ7t27tWbNGuXm5io7O9uqOXbsmFJSUjRy5EiVlpYqIyNDjz32mLZv327VrFu3TllZWXr++ed18OBBDRo0SE6nU9XV1Zc7JQAAYJAgr9fr9XejU6dOaciQIVq5cqVefPFFDR48WEuWLFFtba26dOmitWvXaty4cZKksrIy9evXT8XFxRo+fLi2bdum+++/X5WVlYqOjpYk5eTkaPbs2Tpx4oTCwsI0e/Zs5eXl6fDhw9YxJ0yYoJqaGuXn50uSkpKSNGzYMC1fvlyS5PF4FBcXp+nTp2vOnDmXNA+32y273a7a2lrZbDZ/T4Nxes7JC3QLfvtiUUqgWwAAXGOX+v37sq7kpKWlKSUlRcnJyT7rS0pK1NjY6LO+b9++6t69u4qLiyVJxcXFGjBggBVwJMnpdMrtduvIkSNWzd/u2+l0WvtoaGhQSUmJT01wcLCSk5Otmgupr6+X2+32WQAAgJlC/d3gnXfe0cGDB7V///7zxlwul8LCwtShQwef9dHR0XK5XFbNuQGnebx57Mdq3G63Tp8+rZMnT6qpqemCNWVlZT/Y+8KFC/XCCy9c2kQBAECr5teVnOPHj2vGjBl6++23FRERcbV6umrmzp2r2tpaazl+/HigWwIAAFeJXyGnpKRE1dXVGjJkiEJDQxUaGqqioiItW7ZMoaGhio6OVkNDg2pqany2q6qqUkxMjCQpJibmvKetmr++WI3NZlNkZKQ6d+6skJCQC9Y07+NCwsPDZbPZfBYAAGAmv0LOPffco0OHDqm0tNRahg4dqokTJ1r/3aZNGxUWFlrblJeXq6KiQg6HQ5LkcDh06NAhn6egCgoKZLPZlJCQYNWcu4/mmuZ9hIWFKTEx0afG4/GosLDQqgEAADc2v+7Jad++vfr37++zrm3bturUqZO1fsqUKcrKylLHjh1ls9k0ffp0ORwODR8+XJI0atQoJSQkaNKkSVq8eLFcLpfmzZuntLQ0hYeHS5KmTp2q5cuXa9asWXr00Ue1Y8cOrV+/Xnl5//f0T1ZWllJTUzV06FDddtttWrJkierq6jR58uQrOiEAAMAMft94fDGvv/66goODNXbsWNXX18vpdGrlypXWeEhIiLZu3app06bJ4XCobdu2Sk1N1YIFC6ya+Ph45eXlKTMzU0uXLlW3bt301ltvyel0WjXjx4/XiRMnlJ2dLZfLpcGDBys/P/+8m5EBAMCN6bLek2MK3pPji/fkAABag6v6nhwAAIDrHSEHAAAYiZADAACMRMgBAABGIuQAAAAjEXIAAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYCRCDgAAMBIhBwAAGImQAwAAjETIAQAARiLkAAAAIxFyAACAkQg5AADASIQcAABgJEIOAAAwEiEHAAAYiZADAACMRMgBAABGIuQAAAAjEXIAAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYCRCDgAAMBIhBwAAGImQAwAAjETIAQAARvIr5KxatUoDBw6UzWaTzWaTw+HQtm3brPEzZ84oLS1NnTp1Urt27TR27FhVVVX57KOiokIpKSm66aabFBUVpZkzZ+rs2bM+NTt37tSQIUMUHh6uXr16KTc397xeVqxYoZ49eyoiIkJJSUnat2+fP1MBAACG8yvkdOvWTYsWLVJJSYkOHDigf/zHf9QvfvELHTlyRJKUmZmpLVu2aMOGDSoqKlJlZaXGjBljbd/U1KSUlBQ1NDRo9+7dWrNmjXJzc5WdnW3VHDt2TCkpKRo5cqRKS0uVkZGhxx57TNu3b7dq1q1bp6ysLD3//PM6ePCgBg0aJKfTqerq6is9HwAAwBBBXq/XeyU76Nixo1555RWNGzdOXbp00dq1azVu3DhJUllZmfr166fi4mINHz5c27Zt0/3336/KykpFR0dLknJycjR79mydOHFCYWFhmj17tvLy8nT48GHrGBMmTFBNTY3y8/MlSUlJSRo2bJiWL18uSfJ4PIqLi9P06dM1Z86cS+7d7XbLbrertrZWNpvtSk6DEXrOyQt0C377YlFKoFsAAFxjl/r9+7LvyWlqatI777yjuro6ORwOlZSUqLGxUcnJyVZN37591b17dxUXF0uSiouLNWDAACvgSJLT6ZTb7bauBhUXF/vso7mmeR8NDQ0qKSnxqQkODlZycrJV80Pq6+vldrt9FgAAYCa/Q86hQ4fUrl07hYeHa+rUqdq4caMSEhLkcrkUFhamDh06+NRHR0fL5XJJklwul0/AaR5vHvuxGrfbrdOnT+vbb79VU1PTBWua9/FDFi5cKLvdbi1xcXH+Th8AALQSfoecPn36qLS0VHv37tW0adOUmpqqjz/++Gr01uLmzp2r2tpaazl+/HigWwIAAFdJqL8bhIWFqVevXpKkxMRE7d+/X0uXLtX48ePV0NCgmpoan6s5VVVViomJkSTFxMSc9xRU89NX59b87RNZVVVVstlsioyMVEhIiEJCQi5Y07yPHxIeHq7w8HB/pwwAAFqhK35PjsfjUX19vRITE9WmTRsVFhZaY+Xl5aqoqJDD4ZAkORwOHTp0yOcpqIKCAtlsNiUkJFg15+6juaZ5H2FhYUpMTPSp8Xg8KiwstGoAAAD8upIzd+5c3Xffferevbu+//57rV27Vjt37tT27dtlt9s1ZcoUZWVlqWPHjrLZbJo+fbocDoeGDx8uSRo1apQSEhI0adIkLV68WC6XS/PmzVNaWpp1hWXq1Klavny5Zs2apUcffVQ7duzQ+vXrlZf3f0/+ZGVlKTU1VUOHDtVtt92mJUuWqK6uTpMnT27BUwMAAFozv0JOdXW1HnnkEX3zzTey2+0aOHCgtm/frn/6p3+SJL3++usKDg7W2LFjVV9fL6fTqZUrV1rbh4SEaOvWrZo2bZocDofatm2r1NRULViwwKqJj49XXl6eMjMztXTpUnXr1k1vvfWWnE6nVTN+/HidOHFC2dnZcrlcGjx4sPLz88+7GRkAANy4rvg9Oa0Z78nxxXtyAACtwVV/Tw4AAMD1jJADAACMRMgBAABGIuQAAAAjEXIAAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYCRCDgAAMBIhBwAAGImQAwAAjETIAQAARiLkAAAAIxFyAACAkUID3QBwo+k5Jy/QLVyWLxalBLoFAPALV3IAAICRCDkAAMBIhBwAAGAkQg4AADASNx6jVWutN/ECAK4+ruQAAAAjEXIAAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYCRCDgAAMBIhBwAAGImQAwAAjETIAQAARvIr5CxcuFDDhg1T+/btFRUVpdGjR6u8vNyn5syZM0pLS1OnTp3Url07jR07VlVVVT41FRUVSklJ0U033aSoqCjNnDlTZ8+e9anZuXOnhgwZovDwcPXq1Uu5ubnn9bNixQr17NlTERERSkpK0r59+/yZDgAAMJhfIaeoqEhpaWnas2ePCgoK1NjYqFGjRqmurs6qyczM1JYtW7RhwwYVFRWpsrJSY8aMscabmpqUkpKihoYG7d69W2vWrFFubq6ys7OtmmPHjiklJUUjR45UaWmpMjIy9Nhjj2n79u1Wzbp165SVlaXnn39eBw8e1KBBg+R0OlVdXX0l5wMAABgiyOv1ei934xMnTigqKkpFRUW66667VFtbqy5dumjt2rUaN26cJKmsrEz9+vVTcXGxhg8frm3btun+++9XZWWloqOjJUk5OTmaPXu2Tpw4obCwMM2ePVt5eXk6fPiwdawJEyaopqZG+fn5kqSkpCQNGzZMy5cvlyR5PB7FxcVp+vTpmjNnziX173a7ZbfbVVtbK5vNdrmnwRg95+QFugVcx75YlBLoFgBA0qV//76ie3Jqa2slSR07dpQklZSUqLGxUcnJyVZN37591b17dxUXF0uSiouLNWDAACvgSJLT6ZTb7daRI0esmnP30VzTvI+GhgaVlJT41AQHBys5OdmquZD6+nq53W6fBQAAmOmyQ47H41FGRoZGjBih/v37S5JcLpfCwsLUoUMHn9ro6Gi5XC6r5tyA0zzePPZjNW63W6dPn9a3336rpqamC9Y07+NCFi5cKLvdbi1xcXH+TxwAALQKlx1y0tLSdPjwYb3zzjst2c9VNXfuXNXW1lrL8ePHA90SAAC4SkIvZ6P09HRt3bpVu3btUrdu3az1MTExamhoUE1Njc/VnKqqKsXExFg1f/sUVPPTV+fW/O0TWVVVVbLZbIqMjFRISIhCQkIuWNO8jwsJDw9XeHi4/xMGAACtjl9Xcrxer9LT07Vx40bt2LFD8fHxPuOJiYlq06aNCgsLrXXl5eWqqKiQw+GQJDkcDh06dMjnKaiCggLZbDYlJCRYNefuo7mmeR9hYWFKTEz0qfF4PCosLLRqAADAjc2vKzlpaWlau3atfv/736t9+/bW/S92u12RkZGy2+2aMmWKsrKy1LFjR9lsNk2fPl0Oh0PDhw+XJI0aNUoJCQmaNGmSFi9eLJfLpXnz5iktLc26yjJ16lQtX75cs2bN0qOPPqodO3Zo/fr1ysv7v6d/srKylJqaqqFDh+q2227TkiVLVFdXp8mTJ7fUuQEAAK2YXyFn1apVkqR/+Id/8Fn/u9/9Tr/5zW8kSa+//rqCg4M1duxY1dfXy+l0auXKlVZtSEiItm7dqmnTpsnhcKht27ZKTU3VggULrJr4+Hjl5eUpMzNTS5cuVbdu3fTWW2/J6XRaNePHj9eJEyeUnZ0tl8ulwYMHKz8//7ybkQEAwI3pit6T09rxnhxfvCcHP4b35AC4XlyT9+QAAABcrwg5AADASIQcAABgJEIOAAAwEiEHAAAY6bLeeIyL40klAAACiys5AADASIQcAABgJEIOAAAwEiEHAAAYiZADAACMRMgBAABGIuQAAAAjEXIAAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYCRCDgAAMBIhBwAAGImQAwAAjETIAQAARiLkAAAAIxFyAACAkQg5AADASIQcAABgJEIOAAAwEiEHAAAYiZADAACMRMgBAABGIuQAAAAjEXIAAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEbyO+Ts2rVLDzzwgGJjYxUUFKRNmzb5jHu9XmVnZ6tr166KjIxUcnKyPv30U5+a7777ThMnTpTNZlOHDh00ZcoUnTp1yqfmo48+0p133qmIiAjFxcVp8eLF5/WyYcMG9e3bVxERERowYID+8Ic/+DsdAABgKL9DTl1dnQYNGqQVK1ZccHzx4sVatmyZcnJytHfvXrVt21ZOp1NnzpyxaiZOnKgjR46ooKBAW7du1a5du/TEE09Y4263W6NGjVKPHj1UUlKiV155RfPnz9fq1autmt27d+uhhx7SlClT9OGHH2r06NEaPXq0Dh8+7O+UAACAgYK8Xq/3sjcOCtLGjRs1evRoSX+9ihMbG6unnnpKTz/9tCSptrZW0dHRys3N1YQJE3T06FElJCRo//79Gjp0qCQpPz9fP/vZz/TVV18pNjZWq1at0rPPPiuXy6WwsDBJ0pw5c7Rp0yaVlZVJksaPH6+6ujpt3brV6mf48OEaPHiwcnJyLql/t9stu92u2tpa2Wy2yz0NF9RzTl6L7g8ItC8WpQS6BQCQdOnfv1v0npxjx47J5XIpOTnZWme325WUlKTi4mJJUnFxsTp06GAFHElKTk5WcHCw9u7da9XcddddVsCRJKfTqfLycp08edKqOfc4zTXNx7mQ+vp6ud1unwUAAJipRUOOy+WSJEVHR/usj46OtsZcLpeioqJ8xkNDQ9WxY0efmgvt49xj/FBN8/iFLFy4UHa73Vri4uL8nSIAAGglbqinq+bOnava2lprOX78eKBbAgAAV0mLhpyYmBhJUlVVlc/6qqoqaywmJkbV1dU+42fPntV3333nU3OhfZx7jB+qaR6/kPDwcNlsNp8FAACYqUVDTnx8vGJiYlRYWGitc7vd2rt3rxwOhyTJ4XCopqZGJSUlVs2OHTvk8XiUlJRk1ezatUuNjY1WTUFBgfr06aObb77Zqjn3OM01zccBAAA3Nr9DzqlTp1RaWqrS0lJJf73ZuLS0VBUVFQoKClJGRoZefPFFbd68WYcOHdIjjzyi2NhY6wmsfv366d5779Xjjz+uffv26YMPPlB6eromTJig2NhYSdKvf/1rhYWFacqUKTpy5IjWrVunpUuXKisry+pjxowZys/P16uvvqqysjLNnz9fBw4cUHp6+pWfFQAA0OqF+rvBgQMHNHLkSOvr5uCRmpqq3NxczZo1S3V1dXriiSdUU1OjO+64Q/n5+YqIiLC2efvtt5Wenq577rlHwcHBGjt2rJYtW2aN2+12vfvuu0pLS1NiYqI6d+6s7Oxsn3fp3H777Vq7dq3mzZunZ555Rr1799amTZvUv3//yzoRAADALFf0npzWjvfkAJeO9+QAuF4E5D05AAAA1wtCDgAAMBIhBwAAGImQAwAAjETIAQAARiLkAAAAIxFyAACAkQg5AADASIQcAABgJL9/rQMAtBat8c3jvFkaaDlcyQEAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYCSergJwSVrjk0oAbmxcyQEAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYCRCDgAAMBIhBwAAGImQAwAAjETIAQAARiLkAAAAIxFyAACAkQg5AADASIQcAABgJEIOAAAwEiEHAAAYiZADAACMFBroBgAA/6fnnLxAt+C3LxalBLoF4IK4kgMAAIxEyAEAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYCRCDgAAMBIhBwAAGImQAwAAjETIAQAARmr1IWfFihXq2bOnIiIilJSUpH379gW6JQAAcB1o1b+7at26dcrKylJOTo6SkpK0ZMkSOZ1OlZeXKyoqKtDtAcANgd+3hetVkNfr9Qa6icuVlJSkYcOGafny5ZIkj8ejuLg4TZ8+XXPmzLno9m63W3a7XbW1tbLZbC3aW2v8Sw8AuH4RzP7PpX7/brVXchoaGlRSUqK5c+da64KDg5WcnKzi4uILblNfX6/6+nrr69raWkl/PVktzVP/lxbfJwDgxnU1vle1Vs3n4mLXaVptyPn222/V1NSk6Ohon/XR0dEqKyu74DYLFy7UCy+8cN76uLi4q9IjAAAtxb4k0B1cf77//nvZ7fYfHG+1IedyzJ07V1lZWdbXHo9H3333nTp16qSgoKAWO47b7VZcXJyOHz/e4j8Gu16YPkfm1/qZPkfm1/qZPserOT+v16vvv/9esbGxP1rXakNO586dFRISoqqqKp/1VVVViomJueA24eHhCg8P91nXoUOHq9WibDabkf/jnsv0OTK/1s/0OTK/1s/0OV6t+f3YFZxmrfYR8rCwMCUmJqqwsNBa5/F4VFhYKIfDEcDOAADA9aDVXsmRpKysLKWmpmro0KG67bbbtGTJEtXV1Wny5MmBbg0AAARYqw4548eP14kTJ5SdnS2Xy6XBgwcrPz//vJuRr7Xw8HA9//zz5/1ozCSmz5H5tX6mz5H5tX6mz/F6mF+rfk8OAADAD2m19+QAAAD8GEIOAAAwEiEHAAAYiZADAACMRMi5ShYtWqSgoCBlZGQEupUW9fXXX+vhhx9Wp06dFBkZqQEDBujAgQOBbqtFNDU16bnnnlN8fLwiIyP193//9/qXf/mXi/5ulOvZrl279MADDyg2NlZBQUHatGmTz7jX61V2dra6du2qyMhIJScn69NPPw1Ms5fhx+bX2Nio2bNna8CAAWrbtq1iY2P1yCOPqLKyMnANX4aL/Rmea+rUqQoKCtKSJUuuWX9X6lLmd/ToUf385z+X3W5X27ZtNWzYMFVUVFz7Zi/DxeZ36tQppaenq1u3boqMjFRCQoJycnIC0+xlWLhwoYYNG6b27dsrKipKo0ePVnl5uU/NmTNnlJaWpk6dOqldu3YaO3bseS/yvVoIOVfB/v379dvf/lYDBw4MdCst6uTJkxoxYoTatGmjbdu26eOPP9arr76qm2++OdCttYiXX35Zq1at0vLly3X06FG9/PLLWrx4sd54441At3bZ6urqNGjQIK1YseKC44sXL9ayZcuUk5OjvXv3qm3btnI6nTpz5sw17vTy/Nj8/vKXv+jgwYN67rnndPDgQf33f/+3ysvL9fOf/zwAnV6+i/0ZNtu4caP27Nlz0dfcX28uNr8///nPuuOOO9S3b1/t3LlTH330kZ577jlFRERc404vz8Xml5WVpfz8fP3nf/6njh49qoyMDKWnp2vz5s3XuNPLU1RUpLS0NO3Zs0cFBQVqbGzUqFGjVFdXZ9VkZmZqy5Yt2rBhg4qKilRZWakxY8Zcmwa9aFHff/+9t3fv3t6CggLv3Xff7Z0xY0agW2oxs2fP9t5xxx2BbuOqSUlJ8T766KM+68aMGeOdOHFigDpqWZK8GzdutL72eDzemJgY7yuvvGKtq6mp8YaHh3v/67/+KwAdXpm/nd+F7Nu3zyvJ++WXX16bplrYD83xq6++8v7d3/2d9/Dhw94ePXp4X3/99WveW0u40PzGjx/vffjhhwPTUAu70PxuueUW74IFC3zWDRkyxPvss89ew85aTnV1tVeSt6ioyOv1/vUzpU2bNt4NGzZYNUePHvVK8hYXF1/1friS08LS0tKUkpKi5OTkQLfS4jZv3qyhQ4fql7/8paKionTrrbfqzTffDHRbLeb2229XYWGhPvnkE0nSn/70J73//vu67777AtzZ1XHs2DG5XC6f/1ftdruSkpJUXFwcwM6untraWgUFBV3V31l3rXk8Hk2aNEkzZ87ULbfcEuh2WpTH41FeXp5++tOfyul0KioqSklJST/6I7vW5vbbb9fmzZv19ddfy+v16r333tMnn3yiUaNGBbq1y1JbWytJ6tixoySppKREjY2NPp8zffv2Vffu3a/J5wwhpwW98847OnjwoBYuXBjoVq6Kzz//XKtWrVLv3r21fft2TZs2TU8++aTWrFkT6NZaxJw5czRhwgT17dtXbdq00a233qqMjAxNnDgx0K1dFS6XS5LOe0N4dHS0NWaSM2fOaPbs2XrooYeM+mWIL7/8skJDQ/Xkk08GupUWV11drVOnTmnRokW699579e677+rBBx/UmDFjVFRUFOj2WsQbb7yhhIQEdevWTWFhYbr33nu1YsUK3XXXXYFuzW8ej0cZGRkaMWKE+vfvL+mvnzNhYWHn/cPiWn3OtOpf63A9OX78uGbMmKGCgoJW87Nif3k8Hg0dOlQvvfSSJOnWW2/V4cOHlZOTo9TU1AB3d+XWr1+vt99+W2vXrtUtt9yi0tJSZWRkKDY21oj53cgaGxv1q1/9Sl6vV6tWrQp0Oy2mpKRES5cu1cGDBxUUFBTodlqcx+ORJP3iF79QZmamJGnw4MHavXu3cnJydPfddweyvRbxxhtvaM+ePdq8ebN69OihXbt2KS0tTbGxsa3uJwJpaWk6fPiw3n///UC3YuFKTgspKSlRdXW1hgwZotDQUIWGhqqoqEjLli1TaGiompqaAt3iFevatasSEhJ81vXr16/VPOVwMTNnzrSu5gwYMECTJk1SZmamsVfmYmJiJOm8pxyqqqqsMRM0B5wvv/xSBQUFRl3F+eMf/6jq6mp1797d+tz58ssv9dRTT6lnz56Bbu+Kde7cWaGhocZ+7pw+fVrPPPOMXnvtNT3wwAMaOHCg0tPTNX78eP3bv/1boNvzS3p6urZu3ar33ntP3bp1s9bHxMSooaFBNTU1PvXX6nOGkNNC7rnnHh06dEilpaXWMnToUE2cOFGlpaUKCQkJdItXbMSIEec9GvjJJ5+oR48eAeqoZf3lL39RcLDvX4mQkBDrX5OmiY+PV0xMjAoLC611brdbe/fulcPhCGBnLac54Hz66af6n//5H3Xq1CnQLbWoSZMm6aOPPvL53ImNjdXMmTO1ffv2QLd3xcLCwjRs2DBjP3caGxvV2NjYqj93vF6v0tPTtXHjRu3YsUPx8fE+44mJiWrTpo3P50x5ebkqKiquyecMP65qIe3bt7d+Btmsbdu26tSp03nrW6vMzEzdfvvteumll/SrX/1K+/bt0+rVq7V69epAt9YiHnjgAf3rv/6runfvrltuuUUffvihXnvtNT366KOBbu2ynTp1Sp999pn19bFjx1RaWqqOHTuqe/fuysjI0IsvvqjevXsrPj5ezz33nGJjYzV69OjANe2HH5tf165dNW7cOB08eFBbt25VU1OTdQ9Ax44dFRYWFqi2/XKxP8O/DW5t2rRRTEyM+vTpc61bvSwXm9/MmTM1fvx43XXXXRo5cqTy8/O1ZcsW7dy5M3BN++Fi87v77rs1c+ZMRUZGqkePHioqKtJ//Md/6LXXXgtg15cuLS1Na9eu1e9//3u1b9/e+jtmt9sVGRkpu92uKVOmKCsrSx07dpTNZtP06dPlcDg0fPjwq9/gVX9+6wZm2iPkXq/Xu2XLFm///v294eHh3r59+3pXr14d6JZajNvt9s6YMcPbvXt3b0REhPcnP/mJ99lnn/XW19cHurXL9t5773klnbekpqZ6vd6/Pkb+3HPPeaOjo73h4eHee+65x1teXh7Ypv3wY/M7duzYBccked97771At37JLvZn+Lda2yPklzK/f//3f/f26tXLGxER4R00aJB306ZNgWvYTxeb3zfffOP9zW9+442NjfVGRER4+/Tp43311Ve9Ho8nsI1foh/6O/a73/3Oqjl9+rT3n//5n70333yz96abbvI++OCD3m+++eaa9Bf0/5sEAAAwCvfkAAAAIxFyAACAkQg5AADASIQcAABgJEIOAAAwEiEHAAAYiZADAACMRMgBAABGIuQAAAAjEXIAAICRCDkAAMBIhBwAAGCk/wdt4RL/JyWc8gAAAABJRU5ErkJggg=="
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dialogs_lengths = [len(x) for x in soda_dataset['test']['dialog']]\n",
    "plt.hist(dialogs_lengths)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Phrase encoder model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading ()f39ef/.gitattributes:   0%|          | 0.00/690 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "26a7f70f8858401e994d05bd8d003990"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading ()_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8c86a360b6d54359a53f3197143c6e04"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading ()0182ff39ef/README.md:   0%|          | 0.00/3.70k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "df06b5390753419ca825966180523794"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading ()82ff39ef/config.json:   0%|          | 0.00/594 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "22675a55a259420d849e6dac923ea391"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading ()ce_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "41428d1c5475453c83f42b4f1e30d0fa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "757d1cd374844fc5a6dec792e5180ddb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading ()nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0909c6bb15524de6b5f410bbea39c767"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading ()cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8ab3933c469a43469e2db90e2637be05"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading ()f39ef/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "991e1e2bc5a74f3d98bfe7de786f7710"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading ()okenizer_config.json:   0%|          | 0.00/1.19k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3599960d8a6648fe9794eb1a20507c2e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading ()0182ff39ef/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d38566531e714a67af4d36114af14e98"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading ()2ff39ef/modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2709553e62a2433b82d7bf9ab96aea5b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "SentenceTransformer(\n  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: MPNetModel \n  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n)"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# phrase_model = 'roberta-base'\n",
    "# phrase_model = 'microsoft/deberta-v3-base'\n",
    "# phrase_model = 'sentence-transformers/all-MiniLM-L12-v2'\n",
    "# phrase_model = 'sentence-transformers/bert-base-nli-mean-tokens'\n",
    "# phrase_model = 'intfloat/e5-base'\n",
    "# phrase_model = 'cardiffnlp/twitter-xlm-roberta-base-sentiment'\n",
    "phrase_model = 'sentence-transformers/paraphrase-mpnet-base-v2'\n",
    "# phrase_model = 'sentence-transformers/sentence-t5-base'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(phrase_model)\n",
    "# model = AutoModel.from_pretrained(phrase_model).to(device)\n",
    "# model\n",
    "sent_transformer = SentenceTransformer(model_name_or_path=phrase_model, device=device).eval().half()\n",
    "sent_transformer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "SentenceTransformer(\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: MPNetModel \n  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n)"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_transformer.max_seq_length = 256\n",
    "sent_transformer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([5, 768])"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_phrases = ['Some day i will go to school',\n",
    "                'To make maximum progress on addressing these pressing problems',\n",
    "                'I will nether go to school',\n",
    "                'I like to visit school',\n",
    "                'The day will come when i will go to school']\n",
    "\n",
    "phrases_encodings = sent_transformer.encode(test_phrases, convert_to_tensor=True, normalize_embeddings=False)\n",
    "phrases_encodings.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1.0000001 , 0.07119123, 0.84428996, 0.64069384, 0.92123437],\n       [0.07119123, 1.        , 0.05582385, 0.01420632, 0.13659608],\n       [0.84428996, 0.05582385, 1.        , 0.5661354 , 0.7673865 ],\n       [0.64069384, 0.01420632, 0.5661354 , 1.        , 0.574977  ],\n       [0.92123437, 0.13659608, 0.7673865 , 0.574977  , 1.0000002 ]],\n      dtype=float32)"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(phrases_encodings.cpu(), phrases_encodings.cpu())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dialog encoder model\n",
    "\n",
    "  :\n",
    "- -  () ->   \n",
    "-   \n",
    "-     \n",
    "-   c BOS  EOS\n",
    "- causual lm crossentropy loss\n",
    "- causual  \n",
    "-     "
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "class DialogEmbeddings(nn.Module):\n",
    "    def __init__(self, encoder_hidden_dim: int,\n",
    "                 max_interlocutors_count: int,\n",
    "                 max_dialogue_length: int,\n",
    "                 dropout_p: float):\n",
    "        super(DialogEmbeddings, self).__init__()\n",
    "\n",
    "        self.padding_idx = 0  # special index for padding (must be in tokenizer)\n",
    "\n",
    "        self.position_embeddings = nn.Embedding(max_dialogue_length + 1,  # padding\n",
    "                                                encoder_hidden_dim, padding_idx=self.padding_idx)\n",
    "        self.interlocutors_embeddings = nn.Embedding(max_interlocutors_count + 2,  # padding, eos, bos\n",
    "                                                     encoder_hidden_dim, padding_idx=self.padding_idx)\n",
    "        self.norm = nn.LayerNorm(encoder_hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, interlocutors_ids: torch.LongTensor, position_ids: torch.LongTensor = None):\n",
    "        if position_ids is None:\n",
    "            position_ids = self.create_position_ids_from_input_ids(interlocutors_ids)\n",
    "\n",
    "        interlocutors_embeds = self.interlocutors_embeddings(interlocutors_ids)\n",
    "        position_embeds = self.position_embeddings(position_ids)\n",
    "\n",
    "        embeddings = interlocutors_embeds + position_embeds\n",
    "        embeddings = self.norm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    def create_position_ids_from_input_ids(self, input_ids):\n",
    "        \"\"\"\n",
    "        Replace non-padding symbols with their position numbers. Position numbers begin at padding_idx+1. Padding symbols\n",
    "        are ignored. This is modified from fairseq's `utils.make_positions`. :param torch.Tensor x: :return torch.Tensor:\n",
    "        \"\"\"\n",
    "        # The series of casts and type-conversions here are carefully balanced to both work with ONNX export and XLA.\n",
    "        mask = input_ids.ne(self.padding_idx).int()\n",
    "        incremental_indices = torch.cumsum(mask, dim=1).type_as(mask) * mask\n",
    "        return incremental_indices.long() + self.padding_idx"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "class DialogMLP(nn.Module):\n",
    "    def __init__(self,\n",
    "                 encoder_hidden_dim: int,\n",
    "                 dim_feedforward_mult: int = 3,\n",
    "                 dropout_p: float = 0.1):\n",
    "        super(DialogMLP, self).__init__()\n",
    "\n",
    "        self.inner_proj = nn.Linear(encoder_hidden_dim, dim_feedforward_mult * encoder_hidden_dim)\n",
    "        self.norm = nn.LayerNorm(dim_feedforward_mult * encoder_hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.out_proj = nn.Linear(dim_feedforward_mult * encoder_hidden_dim, encoder_hidden_dim)\n",
    "        # self.norm2 = nn.LayerNorm(encoder_hidden_dim)\n",
    "\n",
    "    def forward(self, inp: torch.FloatTensor):\n",
    "        x = F.gelu(self.inner_proj(inp))\n",
    "        x = self.norm(self.dropout(x))\n",
    "        x = self.out_proj(x)\n",
    "        x = x + inp\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "class DialogTransformer(nn.Module):\n",
    "    def __init__(self, encoder_hidden_dim: int,\n",
    "                 max_dialogue_length: int,\n",
    "                 max_interlocutors_count: int,\n",
    "                 decoder_n_layers: int = 2,\n",
    "                 decoder_n_head: int = 4,\n",
    "                 dim_feedforward_mult: int = 3,\n",
    "                 dropout_p: float = 0.1):\n",
    "        super(DialogTransformer, self).__init__()\n",
    "\n",
    "        # self.bos_vector = nn.Parameter(torch.randn([encoder_hidden_dim]), requires_grad=True)\n",
    "        self.eos_vector = nn.Parameter(torch.randn([encoder_hidden_dim]), requires_grad=True)\n",
    "\n",
    "        self.input_norm = nn.LayerNorm(encoder_hidden_dim)\n",
    "        self.input_projection = nn.Linear(encoder_hidden_dim, encoder_hidden_dim)\n",
    "        # self.input_projection = DialogMLP(encoder_hidden_dim, dim_feedforward_mult, dropout_p)\n",
    "        self.dialogue_embeddings = DialogEmbeddings(encoder_hidden_dim, max_interlocutors_count,\n",
    "                                                    max_dialogue_length, dropout_p)\n",
    "\n",
    "        decoder_ff_inner_dim = encoder_hidden_dim * dim_feedforward_mult\n",
    "        layer = nn.TransformerEncoderLayer(d_model=encoder_hidden_dim,\n",
    "                                           nhead=decoder_n_head,\n",
    "                                           dim_feedforward=decoder_ff_inner_dim,\n",
    "                                           activation=F.gelu,  # using gelu instead of default relu\n",
    "                                           dropout=dropout_p,\n",
    "                                           batch_first=True)  # using encoder layers due to not a seq2seq setup\n",
    "        self.model = nn.TransformerEncoder(layer, decoder_n_layers)\n",
    "\n",
    "        # self.lstm_model = nn.LSTM(input_size=encoder_hidden_dim,\n",
    "        #                           hidden_size=decoder_ff_inner_dim,\n",
    "        #                           num_layers=decoder_n_layers,\n",
    "        #                           bidirectional=False,\n",
    "        #                           dropout=dropout_p,\n",
    "        #                           batch_first=True)\n",
    "\n",
    "        # self.logits_projector = DialogOutput(encoder_hidden_dim, dim_feedforward_mult, dropout_p=dropout_p)\n",
    "        self.logits_projector = nn.Linear(in_features=encoder_hidden_dim, out_features=encoder_hidden_dim, bias=True)\n",
    "\n",
    "        self.interlocutors_projector = nn.Linear(in_features=encoder_hidden_dim, out_features=max_interlocutors_count+1, bias=True)\n",
    "\n",
    "    def forward(self, encodings: torch.FloatTensor,\n",
    "                interlocutors_ids: torch.LongTensor,\n",
    "                position_ids: torch.LongTensor = None,\n",
    "                attention_mask: torch.BoolTensor = None,\n",
    "                return_loss = True):\n",
    "        \"\"\"\n",
    "        :param encodings: Pooled hiddens from sentence-transformer in shape [bs, lines_count, hidden_dim]\n",
    "        :param labels: Labels for dialog lines\n",
    "        :param interlocutors_ids: shape [bs, seq_len], interlocutors for each line (from one)\n",
    "        :param position_ids: shape [bs, seq_len], position of line in dialogue (from one)\n",
    "        :param attention_mask: shape [bs, seq_len], attention mask for padding where 1 is disabled and 0 is enabled\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = encodings.shape[0]\n",
    "\n",
    "        representation = torch.cat([\n",
    "            # self.bos_vector.repeat([batch_size, 1, 1]),\n",
    "            encodings,\n",
    "            self.eos_vector.repeat([batch_size, 1, 1])\n",
    "        ], dim=1)  # insert bos and eos vector\n",
    "\n",
    "        x = self.input_norm(representation)\n",
    "        x = self.input_projection(x)\n",
    "        x = x + self.dialogue_embeddings(interlocutors_ids=interlocutors_ids, position_ids=position_ids)\n",
    "\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.zeros([batch_size, x.shape[1]]).bool().to(x.device)\n",
    "\n",
    "        causal_mask = torch.triu(torch.ones(x.shape[1], x.shape[1]), diagonal=1).bool().to(x.device)  # only attend to past (not necessary, but logical...)\n",
    "        x = self.model.forward(src=x,\n",
    "                               mask=causal_mask,\n",
    "                               src_key_padding_mask=attention_mask)\n",
    "\n",
    "        # x, (ht, ct) = self.lstm_model(x)\n",
    "\n",
    "        predicted_logits = self.logits_projector(x)\n",
    "        predicted_interlocutors = self.interlocutors_projector(x)\n",
    "\n",
    "        if return_loss:\n",
    "            copied_interlocutors = interlocutors_ids.clone() - 1\n",
    "            copied_interlocutors[copied_interlocutors < 0] = -100\n",
    "\n",
    "            interlocutors_loss = compute_clm_loss(predicted_interlocutors, copied_interlocutors)\n",
    "            logits_loss = compute_shifted_mse_loss(predicted_logits, representation, padding_mask=attention_mask)\n",
    "\n",
    "            total_loss = logits_loss + interlocutors_loss\n",
    "\n",
    "            return total_loss, interlocutors_loss, predicted_logits, predicted_interlocutors\n",
    "\n",
    "        return predicted_logits, predicted_interlocutors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[False,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n        [False, False,  True,  True,  True,  True,  True,  True,  True,  True],\n        [False, False, False,  True,  True,  True,  True,  True,  True,  True],\n        [False, False, False, False,  True,  True,  True,  True,  True,  True],\n        [False, False, False, False, False,  True,  True,  True,  True,  True],\n        [False, False, False, False, False, False,  True,  True,  True,  True],\n        [False, False, False, False, False, False, False,  True,  True,  True],\n        [False, False, False, False, False, False, False, False,  True,  True],\n        [False, False, False, False, False, False, False, False, False,  True],\n        [False, False, False, False, False, False, False, False, False, False]])"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.triu(torch.ones(10, 10), diagonal=1).bool()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "DialogTransformer(\n  (input_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (input_projection): Linear(in_features=768, out_features=768, bias=True)\n  (dialogue_embeddings): DialogEmbeddings(\n    (position_embeddings): Embedding(51, 768, padding_idx=0)\n    (interlocutors_embeddings): Embedding(4, 768, padding_idx=0)\n    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    (dropout): Dropout(p=0.15, inplace=False)\n  )\n  (model): TransformerEncoder(\n    (layers): ModuleList(\n      (0-3): 4 x TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n        (dropout): Dropout(p=0.15, inplace=False)\n        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.15, inplace=False)\n        (dropout2): Dropout(p=0.15, inplace=False)\n      )\n    )\n  )\n  (logits_projector): Linear(in_features=768, out_features=768, bias=True)\n  (interlocutors_projector): Linear(in_features=768, out_features=3, bias=True)\n)"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialo_transformer = DialogTransformer(encoder_hidden_dim=768,\n",
    "                                      max_dialogue_length=50,\n",
    "                                      max_interlocutors_count=2,\n",
    "                                      decoder_n_layers=4,\n",
    "                                      decoder_n_head=6,\n",
    "                                      dim_feedforward_mult=4,\n",
    "                                      dropout_p=0.15\n",
    "                                      ).to(sent_transformer.device).eval()\n",
    "dialo_transformer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "==========================================================================================\nLayer (type:depth-idx)                                            Param #\n==========================================================================================\nDialogTransformer                                                 768\nLayerNorm: 1-1                                                  1,536\nLinear: 1-2                                                     590,592\nDialogEmbeddings: 1-3                                           --\n    Embedding: 2-1                                             39,168\n    Embedding: 2-2                                             3,072\n    LayerNorm: 2-3                                             1,536\n    Dropout: 2-4                                               --\nTransformerEncoder: 1-4                                         --\n    ModuleList: 2-5                                            --\n        TransformerEncoderLayer: 3-1                          7,087,872\n        TransformerEncoderLayer: 3-2                          7,087,872\n        TransformerEncoderLayer: 3-3                          7,087,872\n        TransformerEncoderLayer: 3-4                          7,087,872\nLinear: 1-5                                                     590,592\nLinear: 1-6                                                     2,307\n==========================================================================================\nTotal params: 29,581,059\nTrainable params: 29,581,059\nNon-trainable params: 0\n=========================================================================================="
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(dialo_transformer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dialog Tokenizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "class DialogTokenizer:\n",
    "    \"\"\"\n",
    "    Accepts dicts with keys: 'dialog' - required, 'interlocutors' and 'labels'\n",
    "    Must return dict with 'encoder_hidden', 'interlocutors_ids' and 'labels'\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lines_encoder: SentenceTransformer,\n",
    "                 all_interlocutors: list = None):\n",
    "\n",
    "        self.lines_encoder = lines_encoder\n",
    "        for p in self.lines_encoder[0].parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        self.padding_idx = 0\n",
    "        # self.bos_idx = 1\n",
    "        self.eos_idx = 1\n",
    "\n",
    "        if all_interlocutors:\n",
    "            all_interlocutors = set(all_interlocutors)\n",
    "            self.id2interlocutors = dict(zip(range(2, len(all_interlocutors) + 2), all_interlocutors))\n",
    "            self.interlocutor2id = {v: k for k, v in self.id2interlocutors.items()}\n",
    "\n",
    "    def encode(self, dialog: List[str],\n",
    "               interlocutors: List[int] = None,\n",
    "               lines_batch_size: int = 50,\n",
    "               unsqueeze: bool = True,\n",
    "               return_loss: bool = True,\n",
    "               device: str = None,\n",
    "               **kwargs):\n",
    "\n",
    "        encodings = self.lines_encoder.encode(sentences=dialog,\n",
    "                                              batch_size=lines_batch_size,\n",
    "                                              normalize_embeddings=False,  # better not to normalize\n",
    "                                              show_progress_bar=False,\n",
    "                                              convert_to_tensor=True)\n",
    "        # encodings = encodings.cpu()\n",
    "        # encodings.requires_grad = False\n",
    "\n",
    "        if interlocutors is None:\n",
    "            interlocutors = [(i % 2) + 2 for i in range(len(dialog))]\n",
    "        elif hasattr(self, 'interlocutor2id'):\n",
    "            interlocutors = list(map(self.interlocutor2id.get, interlocutors))\n",
    "\n",
    "        # print(interlocutors)\n",
    "        interlocutors = interlocutors + [self.eos_idx]\n",
    "\n",
    "        save_device = device if device is not None else encodings.device\n",
    "\n",
    "        result = {\n",
    "            'encodings': encodings.unsqueeze(0).to(save_device) if unsqueeze else encodings.to(save_device),\n",
    "            'interlocutors_ids': torch.LongTensor([interlocutors] if unsqueeze else interlocutors).to(save_device),\n",
    "            'return_loss': return_loss\n",
    "        }\n",
    "\n",
    "        return result\n",
    "\n",
    "    def encode_batch(self, dialog: List[List[str]],\n",
    "                     interlocutors: List[List[int]] = None,\n",
    "                     lines_batch_size: int = 50,\n",
    "                     return_loss: bool = True,\n",
    "                     **kwargs):\n",
    "\n",
    "        if interlocutors is None:\n",
    "            interlocutors = [None] * len(dialog)\n",
    "        unsqueeze = [False] * len(dialog)\n",
    "\n",
    "        assert len(dialog) == len(interlocutors)\n",
    "\n",
    "        zipped = zip(dialog, interlocutors, [lines_batch_size] * len(dialog), unsqueeze)\n",
    "        encoded_batch = list(map(lambda x: self.encode(*x), zipped))\n",
    "\n",
    "        encodings = pad_sequence([encode_dict['encodings'] for encode_dict in encoded_batch],\n",
    "                                 batch_first=True,\n",
    "                                 padding_value=self.padding_idx)\n",
    "        encodings.requires_grad = False\n",
    "\n",
    "        interlocutors_ids = pad_sequence([encode_dict['interlocutors_ids'] for encode_dict in encoded_batch],\n",
    "                                         batch_first=True,\n",
    "                                         padding_value=self.padding_idx)\n",
    "\n",
    "        lengths = [len(dial) + 1 for dial in dialog]  # keep in mind bos and eos\n",
    "        masks = list(map(lambda x: torch.zeros(size=[x]), lengths))\n",
    "        attention_masks = pad_sequence(masks, batch_first=True, padding_value=1).bool().to(encodings.device)\n",
    "\n",
    "        result = {\n",
    "            'encodings': encodings,\n",
    "            'interlocutors_ids': interlocutors_ids,\n",
    "            'attention_mask': attention_masks,\n",
    "            'return_loss': return_loss\n",
    "        }\n",
    "\n",
    "        return result\n",
    "\n",
    "    def encode_cached_batch(self,\n",
    "                            dialog: List[List[str]],\n",
    "                            encodings: List[List[float]],\n",
    "                            interlocutors_ids: List[List[int]],\n",
    "                            return_loss: List[bool],\n",
    "                            **kwargs):\n",
    "\n",
    "        encodings = pad_sequence(list(map(torch.FloatTensor, encodings)),\n",
    "                                 batch_first=True,\n",
    "                                 padding_value=self.padding_idx)\n",
    "        encodings.requires_grad = False\n",
    "\n",
    "        interlocutors_ids = pad_sequence(list(map(torch.LongTensor, interlocutors_ids)),\n",
    "                                         batch_first=True,\n",
    "                                         padding_value=self.padding_idx)\n",
    "\n",
    "        lengths = [len(dial) + 1 for dial in dialog]  # keep in mind bos and eos\n",
    "        masks = list(map(lambda x: torch.zeros(size=[x]), lengths))\n",
    "        attention_masks = pad_sequence(masks, batch_first=True, padding_value=1).bool()\n",
    "\n",
    "        result = {\n",
    "            'encodings': encodings.to(self.lines_encoder.device),\n",
    "            'interlocutors_ids': interlocutors_ids.to(self.lines_encoder.device),\n",
    "            'attention_mask': attention_masks.to(self.lines_encoder.device),\n",
    "            'return_loss': return_loss\n",
    "        }\n",
    "\n",
    "        return result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "dialo_tokenizer = DialogTokenizer(sent_transformer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "{'encodings': tensor([[[-0.0063,  0.0209, -0.0350,  ...,  0.1227,  0.0884, -0.1384],\n          [-0.1029,  0.0217, -0.0217,  ...,  0.1145,  0.1352, -0.0226],\n          [-0.1593, -0.1368, -0.0621,  ..., -0.0499,  0.1226, -0.0770]]],\n        device='cuda:0'),\n 'interlocutors_ids': tensor([[2, 3, 2, 1]], device='cuda:0'),\n 'return_loss': True}"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialo_encoded = dialo_tokenizer.encode(['Hello man', 'Goodbye', 'Thanks'])\n",
    "dialo_encoded"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hivaze/.local/lib/python3.10/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched src_key_padding_mask and src_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "/home/hivaze/.local/lib/python3.10/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "(tensor(2.0595, device='cuda:0', grad_fn=<AddBackward0>),\n tensor(1.4151, device='cuda:0', grad_fn=<NllLossBackward0>),\n tensor([[[-0.0365,  0.9141, -0.0103,  ..., -0.6281, -0.1102,  0.0251],\n          [ 0.2504,  0.4592,  0.2971,  ...,  0.0166,  0.2883, -0.4671],\n          [-0.3220,  0.7605,  0.3304,  ..., -0.6320,  0.2128, -0.6550],\n          [-0.5230,  0.7253,  1.0248,  ...,  0.1678,  0.2667, -0.5948]]],\n        device='cuda:0', grad_fn=<ViewBackward0>),\n tensor([[[-0.3381,  0.5763, -0.1889],\n          [-0.6859,  0.3923,  0.8071],\n          [-0.7884,  0.2733,  0.0021],\n          [-0.5124,  1.3932,  0.7823]]], device='cuda:0',\n        grad_fn=<ViewBackward0>))"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialo_transformer.forward(**dialo_encoded)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "{'encodings': tensor([[[-0.1229,  0.0763, -0.0674,  ...,  0.1157, -0.0237,  0.1064],\n          [-0.0535,  0.0219, -0.0494,  ..., -0.0082, -0.0195,  0.0140],\n          [-0.0737,  0.1139,  0.0222,  ...,  0.1433,  0.0218,  0.0222],\n          ...,\n          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n \n         [[ 0.0299,  0.0150, -0.0858,  ...,  0.0880, -0.1263, -0.0495],\n          [-0.0020,  0.0831,  0.0922,  ..., -0.0483, -0.0163, -0.0296],\n          [ 0.2385,  0.0478, -0.1042,  ...,  0.0440, -0.0205, -0.0307],\n          ...,\n          [ 0.0335,  0.0992, -0.1176,  ...,  0.0282, -0.1591,  0.0083],\n          [ 0.0709,  0.1372, -0.0287,  ...,  0.0309,  0.0206, -0.0188],\n          [ 0.0385,  0.1623, -0.0323,  ..., -0.0121, -0.0094, -0.0142]],\n \n         [[ 0.0238,  0.2390, -0.0990,  ...,  0.1283, -0.0921,  0.0132],\n          [-0.0614,  0.3116, -0.0427,  ...,  0.1978, -0.0355,  0.0038],\n          [ 0.0239,  0.3987, -0.0931,  ...,  0.2244,  0.0240, -0.0188],\n          ...,\n          [-0.1606,  0.1241, -0.1204,  ...,  0.1776, -0.1485, -0.1455],\n          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n        device='cuda:0'),\n 'interlocutors_ids': tensor([[2, 3, 2, 3, 2, 3, 1, 0, 0, 0, 0, 0],\n         [2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 1],\n         [2, 3, 2, 3, 2, 3, 2, 3, 2, 1, 0, 0]], device='cuda:0'),\n 'attention_mask': tensor([[False, False, False, False, False, False, False,  True,  True,  True,\n           True,  True],\n         [False, False, False, False, False, False, False, False, False, False,\n          False, False],\n         [False, False, False, False, False, False, False, False, False, False,\n           True,  True]], device='cuda:0'),\n 'return_loss': True}"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialogs_encoded = dialo_tokenizer.encode_batch(**soda_dataset['train'][:3])\n",
    "dialogs_encoded"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor(1.6745, device='cuda:0', grad_fn=<AddBackward0>),\n tensor(1.2943, device='cuda:0', grad_fn=<NllLossBackward0>),\n tensor([[[ 0.2114,  0.7446, -0.3154,  ..., -0.9015,  0.2804, -0.1730],\n          [ 0.3370, -0.3017, -0.0107,  ..., -0.1362,  0.5804, -0.5768],\n          [-0.2352,  0.5094,  0.0805,  ..., -0.5421,  0.7610, -1.0535],\n          ...,\n          [ 0.1780,  0.1422,  0.8171,  ..., -0.0669,  0.8547, -0.4866],\n          [ 0.1780,  0.1422,  0.8171,  ..., -0.0669,  0.8547, -0.4866],\n          [ 0.0032,  0.3904,  0.6067,  ...,  0.2095,  0.2037,  0.4184]],\n \n         [[ 0.4034,  0.7077, -0.3238,  ..., -0.7807,  0.2862, -0.0821],\n          [ 0.5176,  0.0389,  0.3609,  ...,  0.0106,  0.5444, -0.4430],\n          [-0.2124,  0.1175,  0.0956,  ..., -0.8237,  0.7618, -0.9133],\n          ...,\n          [ 1.0151, -0.2149, -0.1663,  ...,  0.5097,  0.1578, -0.5449],\n          [ 0.5007,  0.5310,  0.5060,  ...,  0.0545,  0.5350, -0.9780],\n          [-0.1006,  0.1896,  0.7994,  ..., -0.2294,  0.4360,  0.2797]],\n \n         [[ 0.1471,  0.7193,  0.6009,  ..., -0.7465,  0.1309, -0.2169],\n          [ 0.4721, -0.1323,  0.7073,  ..., -0.1768,  0.5507, -0.3177],\n          [-0.2608,  0.4752,  0.4069,  ..., -0.9059,  0.4544, -0.8250],\n          ...,\n          [ 0.6980,  0.3263,  0.2028,  ..., -0.2855,  0.1375, -0.7897],\n          [-0.4177,  0.2012,  0.7391,  ..., -0.5065,  0.6496, -0.1556],\n          [-0.4635,  0.2972,  0.6196,  ..., -0.0646,  0.1928,  0.6346]]],\n        device='cuda:0', grad_fn=<ViewBackward0>),\n tensor([[[-3.6525e-02,  5.2779e-01,  6.2693e-02],\n          [-2.0173e-01, -3.6189e-04,  5.3420e-01],\n          [-3.7065e-03,  3.6556e-01, -3.7563e-01],\n          [-4.8481e-01,  8.6778e-01,  5.4407e-01],\n          [ 3.5407e-01,  3.6824e-01, -7.3563e-01],\n          [-4.2470e-01,  5.1154e-01,  5.4431e-01],\n          [-2.3497e-01,  3.7753e-01, -2.3207e-02],\n          [ 8.5529e-02,  6.3216e-01, -3.5835e-01],\n          [ 8.5529e-02,  6.3216e-01, -3.5835e-01],\n          [ 8.5529e-02,  6.3216e-01, -3.5835e-01],\n          [ 8.5529e-02,  6.3216e-01, -3.5835e-01],\n          [ 2.6857e-01,  1.7473e+00, -2.1083e-01]],\n \n         [[-1.9740e-01,  2.4421e-01,  6.3048e-02],\n          [-7.1620e-01, -8.3001e-02,  1.0584e+00],\n          [-3.0000e-01,  3.6090e-01,  1.2452e-01],\n          [-5.4670e-01,  5.3002e-01,  1.7742e+00],\n          [-2.3535e-01,  2.4794e-01, -2.1742e-01],\n          [-4.1534e-01,  4.0553e-01,  1.2296e+00],\n          [-5.3316e-01,  3.8934e-01,  1.1145e-01],\n          [-4.1376e-01,  3.7862e-01,  7.8412e-01],\n          [-1.4356e-02,  6.3303e-01, -6.5179e-01],\n          [-7.3399e-01,  1.1046e-01,  2.1102e-01],\n          [-9.7185e-01, -2.6029e-01, -1.1955e-01],\n          [ 6.6238e-02,  1.2658e+00, -1.3149e-02]],\n \n         [[-3.3815e-01,  4.0807e-01, -2.3567e-01],\n          [-6.8415e-01, -8.5359e-02,  6.4256e-01],\n          [-4.1840e-01,  9.7421e-02,  1.3224e-01],\n          [-9.4937e-01,  7.7632e-01,  1.2369e+00],\n          [-2.0233e-02,  5.0784e-01, -4.5067e-01],\n          [-7.2933e-01,  6.7615e-01,  8.9873e-01],\n          [-6.9667e-01,  6.9785e-01, -4.7974e-03],\n          [-7.1260e-01,  5.0490e-01,  6.0771e-01],\n          [-3.2349e-01,  4.2788e-01, -2.2714e-01],\n          [-3.4967e-01,  1.4331e-01,  1.2533e-01],\n          [-6.5569e-01,  7.8777e-01,  3.9800e-01],\n          [-1.3914e-01,  1.7680e+00,  3.3266e-01]]], device='cuda:0',\n        grad_fn=<ViewBackward0>))"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialo_transformer.forward(**dialogs_encoded)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pretokenize all dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['dialog', 'interlocutors', 'encodings', 'interlocutors_ids', 'return_loss'],\n        num_rows: 1186423\n    })\n    test: Dataset({\n        features: ['dialog', 'interlocutors', 'encodings', 'interlocutors_ids', 'return_loss'],\n        num_rows: 148294\n    })\n    validation: Dataset({\n        features: ['dialog', 'interlocutors', 'encodings', 'interlocutors_ids', 'return_loss'],\n        num_rows: 145660\n    })\n})"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soda_dataset = datasets.load_from_disk('datasets/soda_pmp_vectorized')\n",
    "soda_dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "Map (num_proc=2):   0%|          | 0/1186423 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f6247a27a3434eac9292e453d1d30ee0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRemoteTraceback\u001B[0m                           Traceback (most recent call last)",
      "\u001B[0;31mRemoteTraceback\u001B[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/hivaze/.local/lib/python3.10/site-packages/multiprocess/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"/home/hivaze/.local/lib/python3.10/site-packages/datasets/utils/py_utils.py\", line 1349, in _write_generator_to_queue\n    for i, result in enumerate(func(**kwargs)):\n  File \"/home/hivaze/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 3307, in _map_single\n    example = apply_function_on_filtered_inputs(example, i, offset=offset)\n  File \"/home/hivaze/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 3210, in apply_function_on_filtered_inputs\n    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\n  File \"/tmp/ipykernel_1848/4207648494.py\", line 1, in <lambda>\n    soda_dataset = soda_dataset.map(lambda row: dialo_tokenizer.encode(**row, unsqueeze=False, device='cpu'), num_proc=2)\n  File \"/tmp/ipykernel_1848/370328997.py\", line 31, in encode\n    encodings = self.lines_encoder.encode(sentences=dialog,\n  File \"/home/hivaze/.local/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py\", line 153, in encode\n    self.to(device)\n  File \"/home/hivaze/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1145, in to\n    return self._apply(convert)\n  File \"/home/hivaze/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 797, in _apply\n    module._apply(fn)\n  File \"/home/hivaze/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 797, in _apply\n    module._apply(fn)\n  File \"/home/hivaze/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 797, in _apply\n    module._apply(fn)\n  [Previous line repeated 1 more time]\n  File \"/home/hivaze/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 820, in _apply\n    param_applied = fn(param)\n  File \"/home/hivaze/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1143, in convert\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n  File \"/home/hivaze/.local/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 235, in _lazy_init\n    raise RuntimeError(\nRuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[44], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m soda_dataset \u001B[38;5;241m=\u001B[39m \u001B[43msoda_dataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrow\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mdialo_tokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mrow\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43munsqueeze\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcpu\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_proc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m soda_dataset\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/datasets/dataset_dict.py:852\u001B[0m, in \u001B[0;36mDatasetDict.map\u001B[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001B[0m\n\u001B[1;32m    849\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m cache_file_names \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    850\u001B[0m     cache_file_names \u001B[38;5;241m=\u001B[39m {k: \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m}\n\u001B[1;32m    851\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DatasetDict(\n\u001B[0;32m--> 852\u001B[0m     {\n\u001B[1;32m    853\u001B[0m         k: dataset\u001B[38;5;241m.\u001B[39mmap(\n\u001B[1;32m    854\u001B[0m             function\u001B[38;5;241m=\u001B[39mfunction,\n\u001B[1;32m    855\u001B[0m             with_indices\u001B[38;5;241m=\u001B[39mwith_indices,\n\u001B[1;32m    856\u001B[0m             with_rank\u001B[38;5;241m=\u001B[39mwith_rank,\n\u001B[1;32m    857\u001B[0m             input_columns\u001B[38;5;241m=\u001B[39minput_columns,\n\u001B[1;32m    858\u001B[0m             batched\u001B[38;5;241m=\u001B[39mbatched,\n\u001B[1;32m    859\u001B[0m             batch_size\u001B[38;5;241m=\u001B[39mbatch_size,\n\u001B[1;32m    860\u001B[0m             drop_last_batch\u001B[38;5;241m=\u001B[39mdrop_last_batch,\n\u001B[1;32m    861\u001B[0m             remove_columns\u001B[38;5;241m=\u001B[39mremove_columns,\n\u001B[1;32m    862\u001B[0m             keep_in_memory\u001B[38;5;241m=\u001B[39mkeep_in_memory,\n\u001B[1;32m    863\u001B[0m             load_from_cache_file\u001B[38;5;241m=\u001B[39mload_from_cache_file,\n\u001B[1;32m    864\u001B[0m             cache_file_name\u001B[38;5;241m=\u001B[39mcache_file_names[k],\n\u001B[1;32m    865\u001B[0m             writer_batch_size\u001B[38;5;241m=\u001B[39mwriter_batch_size,\n\u001B[1;32m    866\u001B[0m             features\u001B[38;5;241m=\u001B[39mfeatures,\n\u001B[1;32m    867\u001B[0m             disable_nullable\u001B[38;5;241m=\u001B[39mdisable_nullable,\n\u001B[1;32m    868\u001B[0m             fn_kwargs\u001B[38;5;241m=\u001B[39mfn_kwargs,\n\u001B[1;32m    869\u001B[0m             num_proc\u001B[38;5;241m=\u001B[39mnum_proc,\n\u001B[1;32m    870\u001B[0m             desc\u001B[38;5;241m=\u001B[39mdesc,\n\u001B[1;32m    871\u001B[0m         )\n\u001B[1;32m    872\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m k, dataset \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mitems()\n\u001B[1;32m    873\u001B[0m     }\n\u001B[1;32m    874\u001B[0m )\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/datasets/dataset_dict.py:853\u001B[0m, in \u001B[0;36m<dictcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    849\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m cache_file_names \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    850\u001B[0m     cache_file_names \u001B[38;5;241m=\u001B[39m {k: \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m}\n\u001B[1;32m    851\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DatasetDict(\n\u001B[1;32m    852\u001B[0m     {\n\u001B[0;32m--> 853\u001B[0m         k: \u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    854\u001B[0m \u001B[43m            \u001B[49m\u001B[43mfunction\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfunction\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    855\u001B[0m \u001B[43m            \u001B[49m\u001B[43mwith_indices\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwith_indices\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    856\u001B[0m \u001B[43m            \u001B[49m\u001B[43mwith_rank\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwith_rank\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    857\u001B[0m \u001B[43m            \u001B[49m\u001B[43minput_columns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_columns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    858\u001B[0m \u001B[43m            \u001B[49m\u001B[43mbatched\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatched\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    859\u001B[0m \u001B[43m            \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    860\u001B[0m \u001B[43m            \u001B[49m\u001B[43mdrop_last_batch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdrop_last_batch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    861\u001B[0m \u001B[43m            \u001B[49m\u001B[43mremove_columns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mremove_columns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    862\u001B[0m \u001B[43m            \u001B[49m\u001B[43mkeep_in_memory\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkeep_in_memory\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    863\u001B[0m \u001B[43m            \u001B[49m\u001B[43mload_from_cache_file\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mload_from_cache_file\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    864\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcache_file_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_file_names\u001B[49m\u001B[43m[\u001B[49m\u001B[43mk\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    865\u001B[0m \u001B[43m            \u001B[49m\u001B[43mwriter_batch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwriter_batch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    866\u001B[0m \u001B[43m            \u001B[49m\u001B[43mfeatures\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfeatures\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    867\u001B[0m \u001B[43m            \u001B[49m\u001B[43mdisable_nullable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdisable_nullable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    868\u001B[0m \u001B[43m            \u001B[49m\u001B[43mfn_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfn_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    869\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnum_proc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_proc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    870\u001B[0m \u001B[43m            \u001B[49m\u001B[43mdesc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdesc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    871\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    872\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m k, dataset \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mitems()\n\u001B[1;32m    873\u001B[0m     }\n\u001B[1;32m    874\u001B[0m )\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py:563\u001B[0m, in \u001B[0;36mtransmit_tasks.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    561\u001B[0m     \u001B[38;5;28mself\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mself\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    562\u001B[0m \u001B[38;5;66;03m# apply actual function\u001B[39;00m\n\u001B[0;32m--> 563\u001B[0m out: Union[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDatasetDict\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    564\u001B[0m datasets: List[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(out\u001B[38;5;241m.\u001B[39mvalues()) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(out, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m [out]\n\u001B[1;32m    565\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m dataset \u001B[38;5;129;01min\u001B[39;00m datasets:\n\u001B[1;32m    566\u001B[0m     \u001B[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py:528\u001B[0m, in \u001B[0;36mtransmit_format.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    521\u001B[0m self_format \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    522\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtype\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_type,\n\u001B[1;32m    523\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mformat_kwargs\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_kwargs,\n\u001B[1;32m    524\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcolumns\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_columns,\n\u001B[1;32m    525\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutput_all_columns\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_output_all_columns,\n\u001B[1;32m    526\u001B[0m }\n\u001B[1;32m    527\u001B[0m \u001B[38;5;66;03m# apply actual function\u001B[39;00m\n\u001B[0;32m--> 528\u001B[0m out: Union[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDatasetDict\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    529\u001B[0m datasets: List[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(out\u001B[38;5;241m.\u001B[39mvalues()) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(out, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m [out]\n\u001B[1;32m    530\u001B[0m \u001B[38;5;66;03m# re-apply format to the output\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py:3046\u001B[0m, in \u001B[0;36mDataset.map\u001B[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001B[0m\n\u001B[1;32m   3038\u001B[0m logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSpawning \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_proc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m processes\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   3039\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m logging\u001B[38;5;241m.\u001B[39mtqdm(\n\u001B[1;32m   3040\u001B[0m     disable\u001B[38;5;241m=\u001B[39m\u001B[38;5;129;01mnot\u001B[39;00m logging\u001B[38;5;241m.\u001B[39mis_progress_bar_enabled(),\n\u001B[1;32m   3041\u001B[0m     unit\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m examples\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3044\u001B[0m     desc\u001B[38;5;241m=\u001B[39m(desc \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMap\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m+\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m (num_proc=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_proc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   3045\u001B[0m ) \u001B[38;5;28;01mas\u001B[39;00m pbar:\n\u001B[0;32m-> 3046\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m rank, done, content \u001B[38;5;129;01min\u001B[39;00m iflatmap_unordered(\n\u001B[1;32m   3047\u001B[0m         pool, Dataset\u001B[38;5;241m.\u001B[39m_map_single, kwargs_iterable\u001B[38;5;241m=\u001B[39mkwargs_per_job\n\u001B[1;32m   3048\u001B[0m     ):\n\u001B[1;32m   3049\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m done:\n\u001B[1;32m   3050\u001B[0m             shards_done \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/datasets/utils/py_utils.py:1373\u001B[0m, in \u001B[0;36miflatmap_unordered\u001B[0;34m(pool, func, kwargs_iterable)\u001B[0m\n\u001B[1;32m   1371\u001B[0m             \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m   1372\u001B[0m \u001B[38;5;66;03m# we get the result in case there's an error to raise\u001B[39;00m\n\u001B[0;32m-> 1373\u001B[0m [async_result\u001B[38;5;241m.\u001B[39mget() \u001B[38;5;28;01mfor\u001B[39;00m async_result \u001B[38;5;129;01min\u001B[39;00m async_results]\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/datasets/utils/py_utils.py:1373\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m   1371\u001B[0m             \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m   1372\u001B[0m \u001B[38;5;66;03m# we get the result in case there's an error to raise\u001B[39;00m\n\u001B[0;32m-> 1373\u001B[0m [\u001B[43masync_result\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m async_result \u001B[38;5;129;01min\u001B[39;00m async_results]\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/multiprocess/pool.py:774\u001B[0m, in \u001B[0;36mApplyResult.get\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    772\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_value\n\u001B[1;32m    773\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 774\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_value\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method"
     ]
    }
   ],
   "source": [
    "soda_dataset = soda_dataset.map(lambda row: dialo_tokenizer.encode(**row, unsqueeze=False, device='cpu'), num_proc=2)\n",
    "soda_dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "soda_dataset.save_to_disk('datasets/soda_pmp_vectorized')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "# Accepts list of dialog dicts per batch\n",
    "def collate_batch(batch: list):\n",
    "    v = {k: [dic[k] for dic in batch] for k in batch[0].keys()}  # list of dicts to dict of lists\n",
    "    return v"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(soda_dataset['train'],\n",
    "                              collate_fn=collate_batch,\n",
    "                              num_workers=12,\n",
    "                              shuffle=True, batch_size=512)\n",
    "eval_dataloader = DataLoader(soda_dataset['validation'],\n",
    "                             collate_fn=collate_batch,\n",
    "                             num_workers=12,\n",
    "                             shuffle=True, batch_size=512)\n",
    "test_dataloader = DataLoader(soda_dataset['test'],\n",
    "                             collate_fn=collate_batch,\n",
    "                             num_workers=12,\n",
    "                             shuffle=True, batch_size=512)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from transformers import get_scheduler"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "optimizer = AdamW(dialo_transformer.parameters(), lr=5e-5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "num_epochs = 40\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(name=\"cosine_with_restarts\", optimizer=optimizer, num_warmup_steps=10,\n",
    "                             num_training_steps=num_training_steps)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "def eval_loop(model: DialogTransformer, tokenizer: DialogTokenizer, data_loader):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for batch in tqdm(data_loader):\n",
    "        with torch.inference_mode():\n",
    "            tokenized_input = tokenizer.encode_cached_batch(**batch)\n",
    "            loss, i1, i2, i3 = dialo_transformer.forward(**tokenized_input)\n",
    "            del i1, i2, i3\n",
    "        losses.append(loss.detach().item())\n",
    "    return losses"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/285 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b5e3e0a6955f4c8aab30197036277b3a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "1.6606605278818232"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(eval_loop(dialo_transformer, dialo_tokenizer, eval_dataloader)).mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "def create_experiment_info(best_epoch_n, best_train_loss, best_eval_loss, losses_history = None, file_name='experiment_info.json'):\n",
    "    result_dict = {\n",
    "        'best_epoch': {\n",
    "            'number': best_epoch_n,\n",
    "            'train_loss': best_train_loss,\n",
    "            'eval_loss': best_eval_loss\n",
    "        }\n",
    "    }\n",
    "    if losses_history is not None:\n",
    "        result_dict['history'] = losses_history\n",
    "    with open(file_name, 'w') as outfile:\n",
    "        json.dump(result_dict, outfile, indent=4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "def train_loop(model: DialogTransformer, tokenizer: DialogTokenizer, checkpoints_dir):\n",
    "    Path(checkpoints_dir).mkdir(parents=True, exist_ok=True)\n",
    "    losses_history = {\n",
    "        'train': [],\n",
    "        'eval': []\n",
    "    }\n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "    min_eval_loss = 999999999.9\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        print(f'Starting epoch {epoch}...')\n",
    "        train_losses = []\n",
    "\n",
    "        for batch in train_dataloader:\n",
    "            tokenized_input = tokenizer.encode_cached_batch(**batch)\n",
    "            loss, i1, i2, i3 = dialo_transformer.forward(**tokenized_input)\n",
    "            loss.backward()\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "\n",
    "            del i2, i3\n",
    "\n",
    "        train_loss = np.array(train_losses).mean()\n",
    "        eval_loss = np.array(eval_loop(dialo_transformer, dialo_tokenizer, eval_dataloader)).mean()\n",
    "\n",
    "        losses_history['train'].append(train_loss)\n",
    "        losses_history['eval'].append(eval_loss)\n",
    "\n",
    "        print(f'[TRAIN] Mean epoch loss: {train_loss}')\n",
    "        print(f'[EVAL] Mean epoch loss: {eval_loss}')\n",
    "\n",
    "        if eval_loss < min_eval_loss:\n",
    "            save_path = checkpoints_dir + 'best_model.pth'\n",
    "            print(f'Current best on eval, saving model to {save_path}...')\n",
    "            torch.save(model, save_path)\n",
    "            create_experiment_info(best_epoch_n=epoch, best_train_loss=train_loss, best_eval_loss=eval_loss,\n",
    "                                   losses_history=losses_history, file_name=checkpoints_dir + 'experiment_info.json')\n",
    "            min_eval_loss = eval_loss\n",
    "\n",
    "    return losses_history"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/92720 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4d6c0b9948914d9da712bb6136731dfe"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/285 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "02d78922d834414cb59403bc9f6d4e05"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Mean epoch loss: 0.25275447216395985\n",
      "[EVAL] Mean epoch loss: 0.21841756670098555\n",
      "Current best on eval, saving model to ./experiments/soda_pmpn_eos_shift_tr_4l6h_MSE_d0.15_30M/best_model.pth...\n",
      "Starting epoch 1...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/285 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c139a091eb7b43f7ad4f53c58f805104"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Mean epoch loss: 0.21853831592976172\n",
      "[EVAL] Mean epoch loss: 0.2096842467262034\n",
      "Current best on eval, saving model to ./experiments/soda_pmpn_eos_shift_tr_4l6h_MSE_d0.15_30M/best_model.pth...\n",
      "Starting epoch 2...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/285 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "02b4682424d54342ae0c456657be118d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Mean epoch loss: 0.20768277957148343\n",
      "[EVAL] Mean epoch loss: 0.199619771880016\n",
      "Current best on eval, saving model to ./experiments/soda_pmpn_eos_shift_tr_4l6h_MSE_d0.15_30M/best_model.pth...\n",
      "Starting epoch 3...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/285 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "824b608fdd8f47fba51cad99dbc37013"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Mean epoch loss: 0.20078131146375428\n",
      "[EVAL] Mean epoch loss: 0.1965400532149432\n",
      "Current best on eval, saving model to ./experiments/soda_pmpn_eos_shift_tr_4l6h_MSE_d0.15_30M/best_model.pth...\n",
      "Starting epoch 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fd1d0fe0550>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hivaze/.conda/envs/ort/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1466, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/hivaze/.conda/envs/ort/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1430, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/hivaze/.conda/envs/ort/lib/python3.10/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/home/hivaze/.conda/envs/ort/lib/python3.10/multiprocessing/popen_fork.py\", line 40, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "  File \"/home/hivaze/.conda/envs/ort/lib/python3.10/multiprocessing/connection.py\", line 936, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/home/hivaze/.conda/envs/ort/lib/python3.10/selectors.py\", line 416, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt: \n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "losses_history = train_loop(dialo_transformer, dialo_tokenizer, './experiments/soda_pmpn_eos_shift_tr_4l6h_MSE_d0.15_30M/')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dialo_transformer = torch.load('./experiments/pmpn_1l_4h_COS&MSE_d0.1/best_model.pth').eval()\n",
    "dialo_transformer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "evaluate(dialo_transformer, dialo_tokenizer, test_dataloader)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tests"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dd_dataset['train']['dialog'][14]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_encoded = dialo_tokenizer.encode(dd_dataset['train']['dialog'][14])\n",
    "test_encoded"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    test_output = dialo_transformer.forward(**test_encoded)\n",
    "test_output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "F.mse_loss(test_encoded['encodings'][0][1:].detach(),\n",
    "           test_encoded['encodings'][0][:-1].detach(), reduction='none').mean(-1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "F.mse_loss(test_output[1][0].detach(),\n",
    "           test_encoded['encodings'][0].detach(), reduction='none').mean(-1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.cdist(test_encoded['encodings'][0].detach(), test_encoded['encodings'][0].detach())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cosine_similarity(test_encoded['encodings'][0].cpu(), test_encoded['encodings'][0].cpu())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.cdist(test_output[2][0], test_output[2][0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cosine_similarity(test_output[2][0].cpu(), test_output[2][0].cpu())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.cdist(test_output[2][0], test_encoded['encodings'][0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cosine_similarity(test_output[2][0][:-1].cpu(), test_encoded['encodings'][0].cpu())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.cdist(test_output[1][0][1:], test_encoded['encodings'][0][:-1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "F.mse_loss(test_output[1][0][1:].detach(),\n",
    "           test_output[1][0][:-1].detach(), reduction='none').mean(-1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_output[3].softmax(-1).argmax(-1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
