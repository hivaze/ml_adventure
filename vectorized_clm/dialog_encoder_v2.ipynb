{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 196,
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from typing import List\n",
    "\n",
    "import datasets\n",
    "import json\n",
    "from pathlib import Path\n",
    "import evaluate\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary\n",
    "from tqdm.notebook import tqdm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Util functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "outputs": [],
   "source": [
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output.last_hidden_state  #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "outputs": [],
   "source": [
    "def compute_clm_loss(logits, labels):\n",
    "    # Classical Language modeling task (nope)\n",
    "    # Next token prediction task in causual setup\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "    shift_labels = labels[..., 1:].contiguous()\n",
    "    loss = F.cross_entropy(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1), ignore_index=-100)\n",
    "    return loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "outputs": [],
   "source": [
    "def compute_shifted_cosine_mse_loss(input_logits, output_logits, padding_mask):\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    batch_size = padding_mask.shape[0]\n",
    "    for batch_idx in range(batch_size):\n",
    "        preds = input_logits[batch_idx, :][~padding_mask[batch_idx, :]]\n",
    "        targets = output_logits[batch_idx, :][~padding_mask[batch_idx, :]]\n",
    "\n",
    "        shift_input = preds[:-1, :].contiguous()\n",
    "        shift_output = targets[1:, :].contiguous()\n",
    "\n",
    "        loss = (1 - F.cosine_similarity(shift_input, shift_output, -1)).mean() + F.mse_loss(shift_input, shift_output)\n",
    "        losses.append(loss)\n",
    "\n",
    "    return torch.stack(losses).contiguous().mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "outputs": [],
   "source": [
    "def compute_shifted_cross_l2_loss(input_logits, output_logits, padding_mask):\n",
    "    losses = []\n",
    "\n",
    "    batch_size = padding_mask.shape[0]\n",
    "    for batch_idx in range(batch_size):\n",
    "\n",
    "        preds = input_logits[batch_idx, :][~padding_mask[batch_idx, :]]\n",
    "        targets = output_logits[batch_idx, :][~padding_mask[batch_idx, :]]\n",
    "\n",
    "        orig_distances = torch.cdist(targets[1:], targets[1:]).detach()\n",
    "        pred_distances = torch.cdist(preds[:-1], preds[:-1])\n",
    "\n",
    "        idx = torch.triu_indices(*orig_distances.shape)\n",
    "        loss = F.mse_loss(pred_distances[idx[0], idx[1]].view(-1), orig_distances[idx[0], idx[1]].view(-1))\n",
    "        losses.append(loss)\n",
    "\n",
    "    return torch.stack(losses).contiguous().mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "outputs": [],
   "source": [
    "def compute_cosine_mse_loss(input_logits, output_logits, padding_mask):\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    batch_size = padding_mask.shape[0]\n",
    "    for batch_idx in range(batch_size):\n",
    "        preds = input_logits[batch_idx, :][~padding_mask[batch_idx, :]]\n",
    "        targets = output_logits[batch_idx, :][~padding_mask[batch_idx, :]]\n",
    "        # print(preds.shape, targets.shape)\n",
    "\n",
    "        loss = (1 - F.cosine_similarity(preds, targets, -1)).mean() + F.mse_loss(preds, targets)\n",
    "        losses.append(loss)\n",
    "\n",
    "    return torch.stack(losses).contiguous().mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Losses Playground"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "outputs": [],
   "source": [
    "example_output = torch.rand([5, 8, 100], dtype=torch.float)\n",
    "example_input = example_output.clone()\n",
    "example_input[:, :-1] = example_output[:, 1:]\n",
    "# compute_shifted_mse_loss(example_input, example_output, torch.zeros(5, 8).bool())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(0.)"
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.mse_loss(example_input[0][:-1], example_output[0][1:])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(0.)"
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.mse_loss(torch.cdist(example_output[0][1:], example_output[0][1:]).view(-1),\n",
    "           torch.cdist(example_input[0][:-1], example_input[0][:-1]).view(-1))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.0000, 3.9967, 4.1140, 4.5747, 4.1753, 3.7062, 3.9707],\n        [3.9967, 0.0000, 4.4865, 4.6694, 4.3787, 3.6571, 3.9915],\n        [4.1140, 4.4865, 0.0000, 4.2886, 4.0478, 3.6570, 3.7794],\n        [4.5747, 4.6694, 4.2886, 0.0000, 4.4998, 4.3774, 3.9894],\n        [4.1753, 4.3787, 4.0478, 4.4998, 0.0000, 4.1431, 3.9067],\n        [3.7062, 3.6571, 3.6570, 4.3774, 4.1431, 0.0000, 3.7294],\n        [3.9707, 3.9915, 3.7794, 3.9894, 3.9067, 3.7294, 0.0000]])"
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cdist(example_input[0][:-1], example_input[0][:-1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.0000, 3.9967, 4.1140, 4.5747, 4.1753, 3.7062, 3.9707],\n        [3.9967, 0.0000, 4.4865, 4.6694, 4.3787, 3.6571, 3.9915],\n        [4.1140, 4.4865, 0.0000, 4.2886, 4.0478, 3.6570, 3.7794],\n        [4.5747, 4.6694, 4.2886, 0.0000, 4.4998, 4.3774, 3.9894],\n        [4.1753, 4.3787, 4.0478, 4.4998, 0.0000, 4.1431, 3.9067],\n        [3.7062, 3.6571, 3.6570, 4.3774, 4.1431, 0.0000, 3.7294],\n        [3.9707, 3.9915, 3.7794, 3.9894, 3.9067, 3.7294, 0.0000]])"
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cdist(example_output[0][1:], example_output[0][1:])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.0000, 3.9967, 4.1140, 4.5747, 4.1753, 3.7062, 3.9707],\n        [3.9967, 0.0000, 4.4865, 4.6694, 4.3787, 3.6571, 3.9915],\n        [4.1140, 4.4865, 0.0000, 4.2886, 4.0478, 3.6570, 3.7794],\n        [4.5747, 4.6694, 4.2886, 0.0000, 4.4998, 4.3774, 3.9894],\n        [4.1753, 4.3787, 4.0478, 4.4998, 0.0000, 4.1431, 3.9067],\n        [3.7062, 3.6571, 3.6570, 4.3774, 4.1431, 0.0000, 3.7294],\n        [3.9707, 3.9915, 3.7794, 3.9894, 3.9067, 3.7294, 0.0000]])"
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cdist(example_input[0][:-1], example_output[0][1:])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using daily_dialog dataset\n",
    "This dataset contains emotions and acts, each dialog is a separate list of lines"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset daily_dialog (/home/hivaze/.cache/huggingface/datasets/daily_dialog/default/1.0.0/1d0a58c7f2a4dab5ed9d01dbde8e55e0058e589ab81fce5c2df929ea810eabcd)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2cb10dafa6424d869d90f4b5cadbe02e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['dialog', 'act', 'emotion'],\n        num_rows: 11118\n    })\n    validation: Dataset({\n        features: ['dialog', 'act', 'emotion'],\n        num_rows: 1000\n    })\n    test: Dataset({\n        features: ['dialog', 'act', 'emotion'],\n        num_rows: 1000\n    })\n})"
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd_dataset = datasets.load_dataset('daily_dialog')\n",
    "dd_dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "outputs": [
    {
     "data": {
      "text/plain": "{'dialog': [['Say , Jim , how about going for a few beers after dinner ? ',\n   ' You know that is tempting but is really not good for our fitness . ',\n   ' What do you mean ? It will help us to relax . ',\n   \" Do you really think so ? I don't . It will just make us fat and act silly . Remember last time ? \",\n   \" I guess you are right.But what shall we do ? I don't feel like sitting at home . \",\n   ' I suggest a walk over to the gym where we can play singsong and meet some of our friends . ',\n   \" That's a good idea . I hear Mary and Sally often go there to play pingpong.Perhaps we can make a foursome with them . \",\n   ' Sounds great to me ! If they are willing , we could ask them to go dancing with us.That is excellent exercise and fun , too . ',\n   \" Good.Let ' s go now . \",\n   ' All right . '],\n  ['Can you do push-ups ? ',\n   \" Of course I can . It's a piece of cake ! Believe it or not , I can do 30 push-ups a minute . \",\n   \" Really ? I think that's impossible ! \",\n   ' You mean 30 push-ups ? ',\n   ' Yeah ! ',\n   \" It's easy . If you do exercise everyday , you can make it , too . \"],\n  ['Can you study with the radio on ? ',\n   ' No , I listen to background music . ',\n   ' What is the difference ? ',\n   ' The radio has too many comerials . ',\n   \" That's true , but then you have to buy a record player . \"]],\n 'act': [[3, 4, 2, 2, 2, 3, 4, 1, 3, 4], [2, 1, 2, 2, 1, 1], [2, 1, 2, 1, 1]],\n 'emotion': [[0, 0, 0, 0, 0, 0, 4, 4, 4, 4],\n  [0, 0, 6, 0, 0, 0],\n  [0, 0, 0, 0, 0]]}"
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd_dataset['train'][:3]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "outputs": [
    {
     "data": {
      "text/plain": "{0, 1, 2, 3, 4, 5, 6}"
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce(lambda x, y: set(x) | set(y), dd_dataset['train']['emotion'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "outputs": [
    {
     "data": {
      "text/plain": "{1, 2, 3, 4}"
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce(lambda x, y: set(x) | set(y), dd_dataset['train']['act'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "outputs": [
    {
     "data": {
      "text/plain": "(array([3.791e+03, 3.057e+03, 2.119e+03, 1.669e+03, 3.110e+02, 1.090e+02,\n        4.000e+01, 1.100e+01, 8.000e+00, 3.000e+00]),\n array([ 2. ,  5.3,  8.6, 11.9, 15.2, 18.5, 21.8, 25.1, 28.4, 31.7, 35. ]),\n <BarContainer object of 10 artists>)"
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dialogs_lengths = [len(x) for x in dd_dataset['train']['dialog']]\n",
    "plt.hist(dialogs_lengths)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Phrase encoder model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "outputs": [
    {
     "data": {
      "text/plain": "SentenceTransformer(\n  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: MPNetModel \n  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n)"
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# phrase_model = 'roberta-base'\n",
    "# phrase_model = 'microsoft/deberta-v3-base'\n",
    "# phrase_model = 'sentence-transformers/all-MiniLM-L12-v2'\n",
    "# phrase_model = 'sentence-transformers/bert-base-nli-mean-tokens'\n",
    "# phrase_model = 'intfloat/e5-base'\n",
    "# phrase_model = 'cardiffnlp/twitter-xlm-roberta-base-sentiment'\n",
    "phrase_model = 'sentence-transformers/paraphrase-mpnet-base-v2'\n",
    "# phrase_model = 'sentence-transformers/sentence-t5-base'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(phrase_model)\n",
    "# model = AutoModel.from_pretrained(phrase_model).to(device)\n",
    "# model\n",
    "sent_transformer = SentenceTransformer(model_name_or_path=phrase_model, device=device)\n",
    "sent_transformer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "outputs": [
    {
     "data": {
      "text/plain": "SentenceTransformer(\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: MPNetModel \n  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n)"
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_transformer.max_seq_length = 256\n",
    "sent_transformer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([5, 768])"
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_phrases = ['Some day i will go to school',\n",
    "                'To make maximum progress on addressing these pressing problems',\n",
    "                'I will nether go to school',\n",
    "                'I like to visit school',\n",
    "                'The day will come when i will go to school']\n",
    "\n",
    "phrases_encodings = sent_transformer.encode(test_phrases, convert_to_tensor=True, normalize_embeddings=False)\n",
    "phrases_encodings.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0.0710, 0.8443, 0.6406, 0.9213], device='cuda:0')"
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cosine_similarity(phrases_encodings[0], phrases_encodings[1:])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.0000, 4.4946, 1.8611, 2.8869, 1.3281],\n        [4.4946, 0.0000, 4.5073, 4.7039, 4.3278],\n        [1.8611, 4.5073, 0.0000, 3.1569, 2.2718],\n        [2.8869, 4.7039, 3.1569, 0.0000, 3.1362],\n        [1.3281, 4.3278, 2.2718, 3.1362, 0.0000]], device='cuda:0')"
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cdist(phrases_encodings, phrases_encodings)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dialog encoder model\n",
    "\n",
    "Концептуально тут нужно:\n",
    "- модель-кодировщик фраз (замоороженная) -> готовые эмбединги текста\n",
    "- эмбединги участников диалога\n",
    "- эмбединги позиции текста в диалоге\n",
    "- кастомный токенизер c BOS и EOS\n",
    "- causual lm crossentropy loss\n",
    "- causual маска атеншена\n",
    "- финальный классификатор в условный словарь"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "outputs": [],
   "source": [
    "class DialogEmbeddings(nn.Module):\n",
    "    def __init__(self, encoder_hidden_dim: int,\n",
    "                 max_interlocutors_count: int,\n",
    "                 max_dialogue_length: int,\n",
    "                 dropout_p: float):\n",
    "        super(DialogEmbeddings, self).__init__()\n",
    "\n",
    "        self.padding_idx = 0  # special index for padding (must be in tokenizer)\n",
    "\n",
    "        self.position_embeddings = nn.Embedding(max_dialogue_length + 1,  # padding\n",
    "                                                encoder_hidden_dim, padding_idx=self.padding_idx)\n",
    "        self.interlocutors_embeddings = nn.Embedding(max_interlocutors_count + 2,  # padding, eos, bos\n",
    "                                                     encoder_hidden_dim, padding_idx=self.padding_idx)\n",
    "        self.norm = nn.LayerNorm(encoder_hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, interlocutors_ids: torch.LongTensor, position_ids: torch.LongTensor = None):\n",
    "        if position_ids is None:\n",
    "            position_ids = self.create_position_ids_from_input_ids(interlocutors_ids)\n",
    "\n",
    "        interlocutors_embeds = self.interlocutors_embeddings(interlocutors_ids)\n",
    "        position_embeds = self.position_embeddings(position_ids)\n",
    "\n",
    "        embeddings = interlocutors_embeds + position_embeds\n",
    "        embeddings = self.norm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    def create_position_ids_from_input_ids(self, input_ids):\n",
    "        \"\"\"\n",
    "        Replace non-padding symbols with their position numbers. Position numbers begin at padding_idx+1. Padding symbols\n",
    "        are ignored. This is modified from fairseq's `utils.make_positions`. :param torch.Tensor x: :return torch.Tensor:\n",
    "        \"\"\"\n",
    "        # The series of casts and type-conversions here are carefully balanced to both work with ONNX export and XLA.\n",
    "        mask = input_ids.ne(self.padding_idx).int()\n",
    "        incremental_indices = torch.cumsum(mask, dim=1).type_as(mask) * mask\n",
    "        return incremental_indices.long() + self.padding_idx"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "outputs": [],
   "source": [
    "class DialogOutput(nn.Module):\n",
    "    def __init__(self,\n",
    "                 encoder_hidden_dim: int,\n",
    "                 dim_feedforward_mult: int = 3,\n",
    "                 dropout_p: float = 0.1):\n",
    "        super(DialogOutput, self).__init__()\n",
    "\n",
    "        self.inner_proj = nn.Linear(encoder_hidden_dim, dim_feedforward_mult * encoder_hidden_dim)\n",
    "        self.norm = nn.LayerNorm(dim_feedforward_mult * encoder_hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.out_proj = nn.Linear(dim_feedforward_mult * encoder_hidden_dim, encoder_hidden_dim)\n",
    "        # self.norm2 = nn.LayerNorm(encoder_hidden_dim)\n",
    "\n",
    "    def forward(self, inp: torch.FloatTensor):\n",
    "        x = F.gelu(self.inner_proj(inp))\n",
    "        x = self.norm(self.dropout(x))\n",
    "        x = self.out_proj(x)\n",
    "        x = x + inp\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "outputs": [],
   "source": [
    "class DialogTransformer(nn.Module):\n",
    "    def __init__(self, encoder_hidden_dim: int,\n",
    "                 max_dialogue_length: int,\n",
    "                 max_interlocutors_count: int,\n",
    "                 decoder_n_layers: int = 2,\n",
    "                 decoder_n_head: int = 4,\n",
    "                 dim_feedforward_mult: int = 3,\n",
    "                 dropout_p: float = 0.1):\n",
    "        super(DialogTransformer, self).__init__()\n",
    "\n",
    "        # self.bos_vector = nn.Parameter(torch.randn([encoder_hidden_dim]), requires_grad=True)\n",
    "        self.eos_vector = nn.Parameter(torch.randn([encoder_hidden_dim]), requires_grad=True)\n",
    "\n",
    "        self.input_norm = nn.LayerNorm(encoder_hidden_dim)\n",
    "        self.dialogue_embeddings = DialogEmbeddings(encoder_hidden_dim, max_interlocutors_count,\n",
    "                                                    max_dialogue_length, dropout_p)\n",
    "\n",
    "        decoder_ff_inner_dim = encoder_hidden_dim * dim_feedforward_mult\n",
    "        layer = nn.TransformerEncoderLayer(d_model=encoder_hidden_dim,\n",
    "                                           nhead=decoder_n_head,\n",
    "                                           dim_feedforward=decoder_ff_inner_dim,\n",
    "                                           activation=F.gelu,  # using gelu instead of default relu\n",
    "                                           dropout=dropout_p,\n",
    "                                           batch_first=True)  # using encoder layers due to not a seq2seq setup\n",
    "        self.model = nn.TransformerEncoder(layer, decoder_n_layers)\n",
    "\n",
    "        # self.lstm_model = nn.LSTM(input_size=encoder_hidden_dim,\n",
    "        #                           hidden_size=decoder_ff_inner_dim,\n",
    "        #                           num_layers=decoder_n_layers,\n",
    "        #                           bidirectional=False,\n",
    "        #                           dropout=dropout_p,\n",
    "        #                           batch_first=True)\n",
    "\n",
    "        # self.logits_projector = DialogOutput(encoder_hidden_dim, dim_feedforward_mult, dropout_p=dropout_p)\n",
    "        self.logits_projector = nn.Linear(in_features=encoder_hidden_dim, out_features=encoder_hidden_dim, bias=True)\n",
    "\n",
    "        self.interlocutors_projector = nn.Linear(in_features=encoder_hidden_dim, out_features=max_interlocutors_count+1, bias=True)\n",
    "\n",
    "    def forward(self, encodings: torch.FloatTensor,\n",
    "                interlocutors_ids: torch.LongTensor,\n",
    "                position_ids: torch.LongTensor = None,\n",
    "                attention_mask: torch.BoolTensor = None,\n",
    "                return_loss = True):\n",
    "        \"\"\"\n",
    "        :param encodings: Pooled hiddens from sentence-transformer in shape [bs, lines_count, hidden_dim]\n",
    "        :param labels: Labels for dialog lines\n",
    "        :param interlocutors_ids: shape [bs, seq_len], interlocutors for each line (from one)\n",
    "        :param position_ids: shape [bs, seq_len], position of line in dialogue (from one)\n",
    "        :param attention_mask: shape [bs, seq_len], attention mask for padding where 1 is disabled and 0 is enabled\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = encodings.shape[0]\n",
    "\n",
    "        representation = torch.cat([\n",
    "            # self.bos_vector.repeat([batch_size, 1, 1]),\n",
    "            encodings,\n",
    "            self.eos_vector.repeat([batch_size, 1, 1])\n",
    "        ], dim=1)  # insert bos and eos vector\n",
    "\n",
    "        x = self.input_norm(representation)\n",
    "        x = x + self.dialogue_embeddings(interlocutors_ids=interlocutors_ids, position_ids=position_ids)\n",
    "\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.zeros([batch_size, x.shape[1]]).bool().to(x.device)\n",
    "\n",
    "        causal_mask = torch.triu(torch.ones(x.shape[1], x.shape[1]), diagonal=1).bool().to(x.device)  # only attend to past (not necessary, but logical...)\n",
    "        x = self.model.forward(src=x,\n",
    "                               mask=causal_mask,\n",
    "                               src_key_padding_mask=attention_mask)\n",
    "\n",
    "        # x, (ht, ct) = self.lstm_model(x)\n",
    "\n",
    "        predicted_logits = self.logits_projector(x)\n",
    "        predicted_interlocutors = self.interlocutors_projector(x)\n",
    "\n",
    "        if return_loss:\n",
    "            copied_interlocutors = interlocutors_ids.clone() - 1\n",
    "            copied_interlocutors[copied_interlocutors < 0] = -100\n",
    "\n",
    "            interlocutors_loss = compute_clm_loss(predicted_interlocutors, copied_interlocutors)\n",
    "            logits_loss = compute_shifted_cosine_mse_loss(predicted_logits, representation, padding_mask=attention_mask)\n",
    "\n",
    "            total_loss = logits_loss + interlocutors_loss\n",
    "\n",
    "            return total_loss, interlocutors_loss, predicted_logits, predicted_interlocutors\n",
    "\n",
    "        return predicted_logits, predicted_interlocutors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[False,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n        [False, False,  True,  True,  True,  True,  True,  True,  True,  True],\n        [False, False, False,  True,  True,  True,  True,  True,  True,  True],\n        [False, False, False, False,  True,  True,  True,  True,  True,  True],\n        [False, False, False, False, False,  True,  True,  True,  True,  True],\n        [False, False, False, False, False, False,  True,  True,  True,  True],\n        [False, False, False, False, False, False, False,  True,  True,  True],\n        [False, False, False, False, False, False, False, False,  True,  True],\n        [False, False, False, False, False, False, False, False, False,  True],\n        [False, False, False, False, False, False, False, False, False, False]])"
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.triu(torch.ones(10, 10), diagonal=1).bool()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "outputs": [
    {
     "data": {
      "text/plain": "DialogTransformer(\n  (input_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (dialogue_embeddings): DialogEmbeddings(\n    (position_embeddings): Embedding(51, 768, padding_idx=0)\n    (interlocutors_embeddings): Embedding(4, 768, padding_idx=0)\n    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    (dropout): Dropout(p=0.2, inplace=False)\n  )\n  (model): TransformerEncoder(\n    (layers): ModuleList(\n      (0): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n        (dropout): Dropout(p=0.2, inplace=False)\n        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.2, inplace=False)\n        (dropout2): Dropout(p=0.2, inplace=False)\n      )\n      (1): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n        (dropout): Dropout(p=0.2, inplace=False)\n        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.2, inplace=False)\n        (dropout2): Dropout(p=0.2, inplace=False)\n      )\n    )\n  )\n  (logits_projector): Linear(in_features=768, out_features=768, bias=True)\n  (interlocutors_projector): Linear(in_features=768, out_features=3, bias=True)\n)"
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialo_transformer = DialogTransformer(encoder_hidden_dim=768,\n",
    "                                      max_dialogue_length=50,\n",
    "                                      max_interlocutors_count=2,\n",
    "                                      decoder_n_layers=2,\n",
    "                                      decoder_n_head=4,\n",
    "                                      dim_feedforward_mult=4,\n",
    "                                      dropout_p=0.2\n",
    "                                      ).to(sent_transformer.device).eval()\n",
    "dialo_transformer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "outputs": [
    {
     "data": {
      "text/plain": "==========================================================================================\nLayer (type:depth-idx)                                            Param #\n==========================================================================================\nDialogTransformer                                                 768\n├─LayerNorm: 1-1                                                  1,536\n├─DialogEmbeddings: 1-2                                           --\n│    └─Embedding: 2-1                                             39,168\n│    └─Embedding: 2-2                                             3,072\n│    └─LayerNorm: 2-3                                             1,536\n│    └─Dropout: 2-4                                               --\n├─TransformerEncoder: 1-3                                         --\n│    └─ModuleList: 2-5                                            --\n│    │    └─TransformerEncoderLayer: 3-1                          7,087,872\n│    │    └─TransformerEncoderLayer: 3-2                          7,087,872\n├─Linear: 1-4                                                     590,592\n├─Linear: 1-5                                                     2,307\n==========================================================================================\nTotal params: 14,814,723\nTrainable params: 14,814,723\nNon-trainable params: 0\n=========================================================================================="
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(dialo_transformer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "outputs": [],
   "source": [
    "# from torch.nn import MultiheadAttention\n",
    "# MultiheadAttention??"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "outputs": [],
   "source": [
    "class DialogTokenizer:\n",
    "    \"\"\"\n",
    "    Accepts dicts with keys: 'dialog' - required, 'interlocutors' and 'labels'\n",
    "    Must return dict with 'encoder_hidden', 'interlocutors_ids' and 'labels'\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lines_encoder: SentenceTransformer,\n",
    "                 all_interlocutors: list = None):\n",
    "\n",
    "        self.lines_encoder = lines_encoder\n",
    "        for p in self.lines_encoder[0].parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        self.padding_idx = 0\n",
    "        # self.bos_idx = 1\n",
    "        self.eos_idx = 1\n",
    "\n",
    "        if all_interlocutors:\n",
    "            all_interlocutors = set(all_interlocutors)\n",
    "            self.id2interlocutors = dict(zip(range(2, len(all_interlocutors) + 2), all_interlocutors))\n",
    "            self.interlocutor2id = {v: k for k, v in self.id2interlocutors.items()}\n",
    "\n",
    "    def encode(self, dialog: List[str],\n",
    "               interlocutors: List[int] = None,\n",
    "               lines_batch_size: int = 50,\n",
    "               unsqueeze: bool = True,\n",
    "               return_loss: bool = True):\n",
    "\n",
    "        encodings = self.lines_encoder.encode(sentences=dialog,\n",
    "                                              batch_size=lines_batch_size,\n",
    "                                              normalize_embeddings=False,  # better not to normalize\n",
    "                                              show_progress_bar=False,\n",
    "                                              convert_to_tensor=True)\n",
    "        # encodings.requires_grad = False\n",
    "\n",
    "        if interlocutors is None:\n",
    "            interlocutors = [(i % 2) + 2 for i in range(len(dialog))]\n",
    "        elif hasattr(self, 'interlocutor2id'):\n",
    "            interlocutors = list(map(self.interlocutor2id.get, interlocutors))\n",
    "\n",
    "        interlocutors = interlocutors + [self.eos_idx]\n",
    "\n",
    "        result = {\n",
    "            'encodings': encodings.unsqueeze(0) if unsqueeze else encodings,\n",
    "            'interlocutors_ids': torch.LongTensor([interlocutors] if unsqueeze else interlocutors).to(encodings.device),\n",
    "            'return_loss': return_loss\n",
    "        }\n",
    "\n",
    "        return result\n",
    "\n",
    "    def encode_batch(self, dialog: List[List[str]],\n",
    "                     interlocutors: List[List[int]] = None,\n",
    "                     lines_batch_size: int = 50,\n",
    "                     return_loss: bool = True):\n",
    "\n",
    "        if interlocutors is None:\n",
    "            interlocutors = [None] * len(dialog)\n",
    "        unsqueeze = [False] * len(dialog)\n",
    "\n",
    "        assert len(dialog) == len(interlocutors)\n",
    "\n",
    "        zipped = zip(dialog, interlocutors, [lines_batch_size] * len(dialog), unsqueeze)\n",
    "        encoded_batch = list(map(lambda x: self.encode(*x), zipped))\n",
    "\n",
    "        encodings = pad_sequence([encode_dict['encodings'] for encode_dict in encoded_batch],\n",
    "                                 batch_first=True,\n",
    "                                 padding_value=self.padding_idx)\n",
    "        encodings.requires_grad = False\n",
    "\n",
    "        interlocutors_ids = pad_sequence([encode_dict['interlocutors_ids'] for encode_dict in encoded_batch],\n",
    "                                         batch_first=True,\n",
    "                                         padding_value=self.padding_idx)\n",
    "\n",
    "        lengths = [len(dial) + 1 for dial in dialog]  # keep in mind bos and eos\n",
    "        masks = list(map(lambda x: torch.zeros(size=[x]), lengths))\n",
    "        attention_masks = pad_sequence(masks, batch_first=True, padding_value=1).bool().to(encodings.device)\n",
    "\n",
    "        result = {\n",
    "            'encodings': encodings,\n",
    "            'interlocutors_ids': interlocutors_ids,\n",
    "            'attention_mask': attention_masks,\n",
    "            'return_loss': return_loss\n",
    "        }\n",
    "\n",
    "        return result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "outputs": [],
   "source": [
    "dialo_tokenizer = DialogTokenizer(sent_transformer,)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "outputs": [
    {
     "data": {
      "text/plain": "{'encodings': tensor([[[-0.0060,  0.0213, -0.0350,  ...,  0.1227,  0.0883, -0.1385],\n          [-0.1030,  0.0213, -0.0218,  ...,  0.1145,  0.1353, -0.0227],\n          [-0.1590, -0.1364, -0.0621,  ..., -0.0499,  0.1225, -0.0771]]],\n        device='cuda:0'),\n 'interlocutors_ids': tensor([[2, 3, 2, 1]], device='cuda:0'),\n 'return_loss': True}"
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialo_encoded = dialo_tokenizer.encode(['Hello man', 'Goodbye', 'Thanks'])\n",
    "dialo_encoded"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor(2.8566, device='cuda:0', grad_fn=<AddBackward0>),\n tensor(1.1870, device='cuda:0', grad_fn=<NllLossBackward0>),\n tensor([[[ 0.8403, -1.0246,  0.6459,  ..., -0.0582,  0.2867, -0.9311],\n          [ 0.4788, -1.2432,  0.2149,  ..., -0.1269, -0.2462,  0.3600],\n          [ 0.2637, -0.8267, -0.0999,  ...,  0.0399,  0.8430, -0.4850],\n          [-0.2287,  0.6210,  0.2625,  ...,  0.2015,  0.5411,  0.3481]]],\n        device='cuda:0', grad_fn=<ViewBackward0>),\n tensor([[[-0.4627, -0.3020, -0.9830],\n          [-0.8576,  0.2013, -0.2270],\n          [-0.3021,  0.3460, -0.4432],\n          [ 0.0061, -0.5514, -0.5011]]], device='cuda:0',\n        grad_fn=<ViewBackward0>))"
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialo_transformer.forward(**dialo_encoded)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "outputs": [
    {
     "data": {
      "text/plain": "{'encodings': tensor([[[-0.1000,  0.0136, -0.0275,  ...,  0.1817, -0.0039, -0.1345],\n          [-0.0769, -0.2066, -0.0541,  ...,  0.1537,  0.0137, -0.0936],\n          [-0.1030,  0.0213, -0.0218,  ...,  0.1145,  0.1353, -0.0227],\n          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n \n         [[-0.0664, -0.0605,  0.0674,  ...,  0.0133,  0.0375, -0.0828],\n          [-0.2907,  0.0527, -0.0544,  ...,  0.1449,  0.0805,  0.0240],\n          [-0.0519,  0.1118,  0.0433,  ...,  0.1584,  0.0710,  0.0479],\n          [-0.1146,  0.1408, -0.0545,  ..., -0.0027,  0.1091, -0.0393]]],\n        device='cuda:0'),\n 'interlocutors_ids': tensor([[2, 3, 2, 1, 0],\n         [2, 3, 2, 3, 1]], device='cuda:0'),\n 'attention_mask': tensor([[False, False, False, False,  True],\n         [False, False, False, False, False]], device='cuda:0'),\n 'return_loss': True}"
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialogs_encoded = dialo_tokenizer.encode_batch(dialog=[\n",
    "    ['Hello', 'Hi there', 'Goodbye'], ['My name is', 'Stop', 'Go away', 'Please']\n",
    "])\n",
    "dialogs_encoded"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor(2.7416, device='cuda:0', grad_fn=<AddBackward0>),\n tensor(1.2946, device='cuda:0', grad_fn=<NllLossBackward0>),\n tensor([[[ 0.9142, -0.8341,  0.3065,  ...,  0.1835,  0.6315, -0.4177],\n          [ 0.6381, -1.0293,  0.1844,  ..., -0.3488,  0.0772,  0.4306],\n          [ 0.1184, -0.8992, -0.3219,  ...,  0.2437,  0.7100, -0.2873],\n          [-1.0385,  0.2369, -0.6214,  ...,  0.4448,  0.8912,  0.5903],\n          [ 0.3089,  0.7537,  0.8462,  ...,  0.3665,  0.1596, -0.4438]],\n \n         [[ 0.9228, -1.2609,  0.0737,  ...,  0.3191,  1.0034, -0.7359],\n          [ 0.4104, -0.6578, -0.1195,  ..., -0.0142,  0.2446,  0.3401],\n          [-0.2627, -0.6756, -0.0721,  ...,  0.5098,  1.0123, -0.6795],\n          [ 0.1883, -0.5296, -0.3300,  ...,  0.2569,  0.7108,  0.5082],\n          [ 0.3537,  0.5046,  0.1305,  ...,  0.3377,  0.1120, -0.6320]]],\n        device='cuda:0', grad_fn=<ViewBackward0>),\n tensor([[[-0.0873, -0.2479, -0.6059],\n          [-0.9294, -0.1228, -0.3811],\n          [-0.5265,  0.5296, -0.2970],\n          [-0.1919, -0.5905, -0.2953],\n          [ 0.5048, -0.2040, -0.6505]],\n \n         [[-0.0375, -0.5512, -0.7255],\n          [-0.6493, -0.4267, -0.2347],\n          [-0.5693,  0.7626,  0.2451],\n          [-0.3598,  0.0287,  0.4343],\n          [-0.1788, -0.2491, -0.1696]]], device='cuda:0',\n        grad_fn=<ViewBackward0>))"
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialo_transformer.forward(**dialogs_encoded)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "outputs": [],
   "source": [
    "# Accepts list of dialog dicts per batch\n",
    "def collate_batch(batch: list):\n",
    "    v = {k: [dic[k] for dic in batch] for k in batch[0].keys()}  # list of dicts to dict of lists\n",
    "    return v"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Column name act not in the dataset. Current columns in the dataset: ['dialog']",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[339], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m dd_dataset[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mdd_dataset\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mremove_columns\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mact\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43memotion\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m dd_dataset[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvalidation\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m dd_dataset[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvalidation\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mremove_columns([\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mact\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124memotion\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m      3\u001B[0m dd_dataset[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtest\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m dd_dataset[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtest\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mremove_columns([\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mact\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124memotion\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "File \u001B[0;32m~/.conda/envs/ort/lib/python3.10/site-packages/datasets/arrow_dataset.py:563\u001B[0m, in \u001B[0;36mtransmit_tasks.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    561\u001B[0m     \u001B[38;5;28mself\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mself\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    562\u001B[0m \u001B[38;5;66;03m# apply actual function\u001B[39;00m\n\u001B[0;32m--> 563\u001B[0m out: Union[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDatasetDict\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    564\u001B[0m datasets: List[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(out\u001B[38;5;241m.\u001B[39mvalues()) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(out, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m [out]\n\u001B[1;32m    565\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m dataset \u001B[38;5;129;01min\u001B[39;00m datasets:\n\u001B[1;32m    566\u001B[0m     \u001B[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001B[39;00m\n",
      "File \u001B[0;32m~/.conda/envs/ort/lib/python3.10/site-packages/datasets/arrow_dataset.py:528\u001B[0m, in \u001B[0;36mtransmit_format.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    521\u001B[0m self_format \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    522\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtype\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_type,\n\u001B[1;32m    523\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mformat_kwargs\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_kwargs,\n\u001B[1;32m    524\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcolumns\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_columns,\n\u001B[1;32m    525\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutput_all_columns\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_output_all_columns,\n\u001B[1;32m    526\u001B[0m }\n\u001B[1;32m    527\u001B[0m \u001B[38;5;66;03m# apply actual function\u001B[39;00m\n\u001B[0;32m--> 528\u001B[0m out: Union[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDatasetDict\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    529\u001B[0m datasets: List[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(out\u001B[38;5;241m.\u001B[39mvalues()) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(out, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m [out]\n\u001B[1;32m    530\u001B[0m \u001B[38;5;66;03m# re-apply format to the output\u001B[39;00m\n",
      "File \u001B[0;32m~/.conda/envs/ort/lib/python3.10/site-packages/datasets/fingerprint.py:511\u001B[0m, in \u001B[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    507\u001B[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001B[1;32m    509\u001B[0m \u001B[38;5;66;03m# Call actual function\u001B[39;00m\n\u001B[0;32m--> 511\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    513\u001B[0m \u001B[38;5;66;03m# Update fingerprint of in-place transforms + update in-place history of transforms\u001B[39;00m\n\u001B[1;32m    515\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m inplace:  \u001B[38;5;66;03m# update after calling func so that the fingerprint doesn't change if the function fails\u001B[39;00m\n",
      "File \u001B[0;32m~/.conda/envs/ort/lib/python3.10/site-packages/datasets/arrow_dataset.py:2013\u001B[0m, in \u001B[0;36mDataset.remove_columns\u001B[0;34m(self, column_names, new_fingerprint)\u001B[0m\n\u001B[1;32m   2011\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m column_name \u001B[38;5;129;01min\u001B[39;00m column_names:\n\u001B[1;32m   2012\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m column_name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m dataset\u001B[38;5;241m.\u001B[39m_data\u001B[38;5;241m.\u001B[39mcolumn_names:\n\u001B[0;32m-> 2013\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   2014\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumn name \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcolumn_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m not in the dataset. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2015\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCurrent columns in the dataset: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdataset\u001B[38;5;241m.\u001B[39m_data\u001B[38;5;241m.\u001B[39mcolumn_names\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2016\u001B[0m         )\n\u001B[1;32m   2018\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m column_name \u001B[38;5;129;01min\u001B[39;00m column_names:\n\u001B[1;32m   2019\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m dataset\u001B[38;5;241m.\u001B[39m_info\u001B[38;5;241m.\u001B[39mfeatures[column_name]\n",
      "\u001B[0;31mValueError\u001B[0m: Column name act not in the dataset. Current columns in the dataset: ['dialog']"
     ]
    }
   ],
   "source": [
    "dd_dataset['train'] = dd_dataset['train'].remove_columns(['act', 'emotion'])\n",
    "dd_dataset['validation'] = dd_dataset['validation'].remove_columns(['act', 'emotion'])\n",
    "dd_dataset['test'] = dd_dataset['test'].remove_columns(['act', 'emotion'])\n",
    "dd_dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dd_dataset['train'], collate_fn=collate_batch, shuffle=True, batch_size=128)\n",
    "eval_dataloader = DataLoader(dd_dataset['validation'], collate_fn=collate_batch, shuffle=True, batch_size=128)\n",
    "test_dataloader = DataLoader(dd_dataset['test'], collate_fn=collate_batch, shuffle=True, batch_size=128)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "outputs": [],
   "source": [
    "optimizer = AdamW(dialo_transformer.parameters(), lr=5e-4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "outputs": [],
   "source": [
    "num_epochs = 15\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(name=\"cosine_with_restarts\", optimizer=optimizer, num_warmup_steps=10,\n",
    "                             num_training_steps=num_training_steps)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "outputs": [],
   "source": [
    "def evaluate(model: DialogTransformer, tokenizer: DialogTokenizer, data_loader):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for batch in tqdm(data_loader):\n",
    "        with torch.inference_mode():\n",
    "            tokenized_input = tokenizer.encode_batch(**batch)\n",
    "            loss, i1, i2, i3 = dialo_transformer.forward(**tokenized_input)\n",
    "            del i1, i2, i3\n",
    "        losses.append(loss.detach().item())\n",
    "    return losses"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/8 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "45ac6b2bbf674b9193336e0c28581fa7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[2.496969223022461,\n 2.495931386947632,\n 2.5014333724975586,\n 2.491014003753662,\n 2.4800643920898438,\n 2.485868215560913,\n 2.5064473152160645,\n 2.4883999824523926]"
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(dialo_transformer, dialo_tokenizer, eval_dataloader)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "outputs": [],
   "source": [
    "def create_experiment_info(best_epoch_n, best_train_loss, best_eval_loss, losses_history = None, file_name='experiment_info.json'):\n",
    "    result_dict = {\n",
    "        'best_epoch': {\n",
    "            'number': best_epoch_n,\n",
    "            'train_loss': best_train_loss,\n",
    "            'eval_loss': best_eval_loss\n",
    "        }\n",
    "    }\n",
    "    if losses_history is not None:\n",
    "        result_dict['history'] = losses_history\n",
    "    with open(file_name, 'w') as outfile:\n",
    "        json.dump(result_dict, outfile, indent=4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "outputs": [],
   "source": [
    "def train(model: DialogTransformer, tokenizer: DialogTokenizer, checkpoints_dir):\n",
    "    Path(checkpoints_dir).mkdir(parents=True, exist_ok=True)\n",
    "    losses_history = {\n",
    "        'train': [],\n",
    "        'eval': []\n",
    "    }\n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "    min_eval_loss = 999999999.9\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        print(f'Starting epoch {epoch}...')\n",
    "        train_losses = []\n",
    "\n",
    "        for batch in train_dataloader:\n",
    "            tokenized_input = tokenizer.encode_batch(**batch)\n",
    "            loss, i1, i2, i3 = dialo_transformer.forward(**tokenized_input)\n",
    "            loss.backward()\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "\n",
    "            del i2, i3\n",
    "\n",
    "        train_loss = np.array(train_losses).mean()\n",
    "        eval_loss = np.array(evaluate(dialo_transformer, dialo_tokenizer, eval_dataloader)).mean()\n",
    "\n",
    "        losses_history['train'].append(train_loss)\n",
    "        losses_history['eval'].append(eval_loss)\n",
    "\n",
    "        print(f'[TRAIN] Mean epoch loss: {train_loss}')\n",
    "        print(f'[EVAL] Mean epoch loss: {eval_loss}')\n",
    "\n",
    "        if eval_loss < min_eval_loss:\n",
    "            save_path = checkpoints_dir + 'best_model.pth'\n",
    "            print(f'Current best on eval, saving model to {save_path}...')\n",
    "            torch.save(model, save_path)\n",
    "            create_experiment_info(best_epoch_n=epoch, best_train_loss=train_loss, best_eval_loss=eval_loss,\n",
    "                                   losses_history=losses_history, file_name=checkpoints_dir + 'experiment_info.json')\n",
    "            min_eval_loss = eval_loss\n",
    "\n",
    "    return losses_history"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/1305 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6ebb9d9d0bdb4878b08c14ea5033e621"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/8 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a6a1b6ab26804835b862ae5b63d1b465"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Mean epoch loss: 1.2173487098737694\n",
      "[EVAL] Mean epoch loss: 0.9656231626868248\n",
      "Current best on eval, saving model to ./experiments/pmpn_eos_shift_tr_2l4h_COS&MSE_d0.2_15M/best_model.pth...\n",
      "Starting epoch 1...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/8 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e864b1fc9e204078ba91fe147ef58c06"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Mean epoch loss: 1.005477902532994\n",
      "[EVAL] Mean epoch loss: 0.9043259471654892\n",
      "Current best on eval, saving model to ./experiments/pmpn_eos_shift_tr_2l4h_COS&MSE_d0.2_15M/best_model.pth...\n",
      "Starting epoch 2...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/8 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "89a41b24bdaf4192bacfd67b79065d99"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Mean epoch loss: 0.9399584888041704\n",
      "[EVAL] Mean epoch loss: 0.9002432003617287\n",
      "Current best on eval, saving model to ./experiments/pmpn_eos_shift_tr_2l4h_COS&MSE_d0.2_15M/best_model.pth...\n",
      "Starting epoch 3...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/8 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5fb208147b984c9591dd327df2c91947"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Mean epoch loss: 0.9051854706358635\n",
      "[EVAL] Mean epoch loss: 0.8669495806097984\n",
      "Current best on eval, saving model to ./experiments/pmpn_eos_shift_tr_2l4h_COS&MSE_d0.2_15M/best_model.pth...\n",
      "Starting epoch 4...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/8 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ba18988c1fe24131ad8aa91f909791a1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Mean epoch loss: 0.8800069861028387\n",
      "[EVAL] Mean epoch loss: 0.8603663817048073\n",
      "Current best on eval, saving model to ./experiments/pmpn_eos_shift_tr_2l4h_COS&MSE_d0.2_15M/best_model.pth...\n",
      "Starting epoch 5...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/8 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "85e5fa4ff2ce4cb68a6e3f47cf23a89e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Mean epoch loss: 0.8528773346166502\n",
      "[EVAL] Mean epoch loss: 0.8520498275756836\n",
      "Current best on eval, saving model to ./experiments/pmpn_eos_shift_tr_2l4h_COS&MSE_d0.2_15M/best_model.pth...\n",
      "Starting epoch 6...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/8 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c8f126c007604ef4953b34163121818f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Mean epoch loss: 0.8272491693496704\n",
      "[EVAL] Mean epoch loss: 0.8429921194911003\n",
      "Current best on eval, saving model to ./experiments/pmpn_eos_shift_tr_2l4h_COS&MSE_d0.2_15M/best_model.pth...\n",
      "Starting epoch 7...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/8 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3fe7c50cdc714c24905027631c8738ee"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Mean epoch loss: 0.8036789510441923\n",
      "[EVAL] Mean epoch loss: 0.8549497872591019\n",
      "Starting epoch 8...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/8 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "691e32cfddbe463cb4bbe832e337e803"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Mean epoch loss: 0.7782610956279711\n",
      "[EVAL] Mean epoch loss: 0.850077211856842\n",
      "Starting epoch 9...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/8 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "848fc6ed3e3749b29ff6b54d14f2eeac"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Mean epoch loss: 0.7561562218885313\n",
      "[EVAL] Mean epoch loss: 0.8705039396882057\n",
      "Starting epoch 10...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/8 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "97f0886cd8f14dd3b4f77d693cf649b8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Mean epoch loss: 0.7369650341998571\n",
      "[EVAL] Mean epoch loss: 0.8759426102042198\n",
      "Starting epoch 11...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/8 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2b6bdfa5e8e14cf4a31ccf10a053b5b8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Mean epoch loss: 0.7193578604994149\n",
      "[EVAL] Mean epoch loss: 0.8850851804018021\n",
      "Starting epoch 12...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/8 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7167f0a08f9c4126a62d593bbec66560"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Mean epoch loss: 0.7088298043985476\n",
      "[EVAL] Mean epoch loss: 0.8874960690736771\n",
      "Starting epoch 13...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/8 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "56201994fe564054a0a701fb2c84b343"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Mean epoch loss: 0.7015573937317421\n",
      "[EVAL] Mean epoch loss: 0.8962971344590187\n",
      "Starting epoch 14...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/8 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d94e34ffe2024228815fd7dfc016f456"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Mean epoch loss: 0.6980681344010364\n",
      "[EVAL] Mean epoch loss: 0.8935330137610435\n"
     ]
    }
   ],
   "source": [
    "losses_history = train(dialo_transformer, dialo_tokenizer, './experiments/pmpn_eos_shift_tr_2l4h_COS&MSE_d0.2_15M/')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dialo_transformer = torch.load('./experiments/pmpn_1l_4h_COS&MSE_d0.1/best_model.pth').eval()\n",
    "dialo_transformer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/8 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fcdff5e7b5fc452c83234a94f252f2d1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[0.9237592816352844,\n 0.9070261716842651,\n 0.950858473777771,\n 0.8814290761947632,\n 0.8824372887611389,\n 0.9190113544464111,\n 0.9275416135787964,\n 0.8859769105911255]"
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(dialo_transformer, dialo_tokenizer, test_dataloader)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tests"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "outputs": [
    {
     "data": {
      "text/plain": "['Ah , ah , ah ... ',\n \" All right , Bill.Here ' s your daily exercise schedule . You are to jog before breakfast . \",\n ' Jog ? ',\n ' Then , you are to walk to work . ',\n ' Walk ? ',\n ' Thirty minutes in gym at lunch time . ',\n ' Oh no . ',\n ' Use the stairs , never the elevator . ',\n ' Oh , dear . ',\n ' And three times a week , you can either swim , play racketball , or hand ball . ',\n ' Oh no . ',\n \" OK , you can stop now.It ' s time for the dance class . \",\n \" Dance class ! I don't know how . \",\n ' You will . ',\n ' Oh ... ']"
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd_dataset['train']['dialog'][14]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "outputs": [
    {
     "data": {
      "text/plain": "{'encodings': tensor([[[-0.0103,  0.1684, -0.0691,  ...,  0.1169, -0.0007, -0.1451],\n          [-0.0941,  0.0757, -0.0304,  ...,  0.0930, -0.2517,  0.0010],\n          [-0.0930, -0.0974, -0.0908,  ...,  0.0892, -0.0962,  0.0261],\n          ...,\n          [ 0.0306, -0.0410, -0.0153,  ...,  0.1048,  0.1243,  0.0121],\n          [-0.0457,  0.4035, -0.0132,  ...,  0.1135,  0.0317, -0.0892],\n          [ 0.0376,  0.1684, -0.1105,  ...,  0.0079, -0.1181, -0.2239]]],\n        device='cuda:0'),\n 'interlocutors_ids': tensor([[2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 1]], device='cuda:0'),\n 'return_loss': True}"
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_encoded = dialo_tokenizer.encode(dd_dataset['train']['dialog'][14])\n",
    "test_encoded"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor(0.6839, device='cuda:0'),\n tensor(0.0247, device='cuda:0'),\n tensor([[[-0.0054,  0.2533, -0.0621,  ...,  0.1817, -0.0858, -0.1744],\n          [-0.0472,  0.2055, -0.0639,  ...,  0.1937, -0.2100, -0.0826],\n          [-0.0577,  0.1250, -0.0506,  ...,  0.1641, -0.1539, -0.0822],\n          ...,\n          [-0.0987,  0.2450, -0.0656,  ...,  0.1326, -0.0793, -0.1174],\n          [-0.0833,  0.3241, -0.1029,  ...,  0.1071, -0.1333, -0.1488],\n          [ 0.0911, -0.0708, -0.2612,  ...,  0.3676, -0.3488, -0.0472]]],\n        device='cuda:0'),\n tensor([[[-1.6289, -3.6384,  4.8216],\n          [-0.6677,  5.9705, -3.8075],\n          [-0.9473, -2.9548,  3.4717],\n          [ 0.1636,  4.8853, -3.8278],\n          [-1.4058, -3.8429,  4.7044],\n          [-0.0464,  5.7073, -4.0799],\n          [-1.2514, -4.0976,  4.9222],\n          [ 0.3383,  5.2997, -4.1880],\n          [-0.0505, -4.4959,  3.9795],\n          [-0.2033,  5.1663, -3.4534],\n          [ 0.4945, -5.1131,  4.1055],\n          [ 1.4640,  4.6333, -5.0039],\n          [-0.8511, -5.0847,  5.3615],\n          [ 2.3942,  3.7247, -5.1322],\n          [ 4.6767, -4.4057, -1.5020],\n          [ 7.6733, -2.1399, -7.5773]]], device='cuda:0'))"
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    test_output = dialo_transformer.forward(**test_encoded)\n",
    "test_output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0.0079, 0.0218, 0.0166, 0.0125, 0.0190, 0.0195, 0.0222, 0.0111],\n       device='cuda:0')"
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.mse_loss(test_encoded['encodings'][0][1:].detach(),\n",
    "           test_encoded['encodings'][0][:-1].detach(), reduction='none').mean(-1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_50041/2758299011.py:1: UserWarning: Using a target size (torch.Size([9, 768])) that is different to the input size (torch.Size([10, 768])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  F.mse_loss(test_output[1][0].detach(),\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (10) must match the size of tensor b (9) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[253], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmse_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtest_output\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdetach\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[43m           \u001B[49m\u001B[43mtest_encoded\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mencodings\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdetach\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreduction\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mnone\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mmean(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m~/.conda/envs/ort/lib/python3.10/site-packages/torch/nn/functional.py:3291\u001B[0m, in \u001B[0;36mmse_loss\u001B[0;34m(input, target, size_average, reduce, reduction)\u001B[0m\n\u001B[1;32m   3288\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m size_average \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m reduce \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   3289\u001B[0m     reduction \u001B[38;5;241m=\u001B[39m _Reduction\u001B[38;5;241m.\u001B[39mlegacy_get_string(size_average, reduce)\n\u001B[0;32m-> 3291\u001B[0m expanded_input, expanded_target \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbroadcast_tensors\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3292\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_nn\u001B[38;5;241m.\u001B[39mmse_loss(expanded_input, expanded_target, _Reduction\u001B[38;5;241m.\u001B[39mget_enum(reduction))\n",
      "File \u001B[0;32m~/.conda/envs/ort/lib/python3.10/site-packages/torch/functional.py:74\u001B[0m, in \u001B[0;36mbroadcast_tensors\u001B[0;34m(*tensors)\u001B[0m\n\u001B[1;32m     72\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function(tensors):\n\u001B[1;32m     73\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(broadcast_tensors, tensors, \u001B[38;5;241m*\u001B[39mtensors)\n\u001B[0;32m---> 74\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_VF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbroadcast_tensors\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: The size of tensor a (10) must match the size of tensor b (9) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "F.mse_loss(test_output[1][0].detach(),\n",
    "           test_encoded['encodings'][0].detach(), reduction='none').mean(-1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.0000, 4.1069, 4.3854, 4.2318, 4.2074, 4.3881, 3.7951, 4.1927, 3.2442,\n         4.3049, 3.7951, 3.7929, 4.2565, 3.9215, 2.3609],\n        [4.1069, 0.0000, 2.9001, 3.5279, 3.6740, 3.1960, 4.4397, 4.0415, 4.2673,\n         3.7079, 4.4397, 3.6758, 4.2172, 3.9979, 4.1196],\n        [4.3854, 2.9001, 0.0000, 4.0644, 3.3790, 3.3917, 4.4951, 4.2465, 4.5239,\n         4.0350, 4.4951, 4.0545, 4.1159, 4.4436, 4.4061],\n        [4.2318, 3.5279, 4.0644, 0.0000, 3.0401, 4.2472, 4.4114, 3.9599, 4.2616,\n         4.1153, 4.4114, 4.1545, 4.4685, 3.8570, 4.2276],\n        [4.2074, 3.6740, 3.3790, 3.0401, 0.0000, 4.2362, 4.3700, 3.9190, 4.2597,\n         4.1915, 4.3700, 3.9400, 4.0513, 4.1252, 4.2156],\n        [4.3881, 3.1960, 3.3917, 4.2472, 4.2362, 0.0000, 4.5586, 4.1508, 4.5284,\n         3.4303, 4.5586, 3.6911, 3.8418, 4.6737, 4.4486],\n        [3.7951, 4.4397, 4.4951, 4.4114, 4.3700, 4.5586, 0.0000, 3.7496, 3.2890,\n         4.5234, 0.0000, 3.9934, 4.4479, 4.2542, 3.4534],\n        [4.1927, 4.0415, 4.2465, 3.9599, 3.9190, 4.1508, 3.7496, 0.0000, 4.2738,\n         4.3166, 3.7496, 4.1499, 4.2228, 4.4662, 4.3719],\n        [3.2442, 4.2673, 4.5239, 4.2616, 4.2597, 4.5284, 3.2890, 4.2738, 0.0000,\n         4.3989, 3.2890, 3.8694, 4.4225, 3.6331, 3.1255],\n        [4.3049, 3.7079, 4.0350, 4.1153, 4.1915, 3.4303, 4.5234, 4.3166, 4.3989,\n         0.0000, 4.5234, 3.8794, 4.0940, 4.2162, 4.2732],\n        [3.7951, 4.4397, 4.4951, 4.4114, 4.3700, 4.5586, 0.0000, 3.7496, 3.2890,\n         4.5234, 0.0000, 3.9934, 4.4479, 4.2542, 3.4534],\n        [3.7929, 3.6758, 4.0545, 4.1545, 3.9400, 3.6911, 3.9934, 4.1499, 3.8694,\n         3.8794, 3.9934, 0.0000, 2.9314, 3.8055, 3.8680],\n        [4.2565, 4.2172, 4.1159, 4.4685, 4.0513, 3.8418, 4.4479, 4.2228, 4.4225,\n         4.0940, 4.4479, 2.9314, 0.0000, 4.7948, 4.4201],\n        [3.9215, 3.9979, 4.4436, 3.8570, 4.1252, 4.6737, 4.2542, 4.4662, 3.6331,\n         4.2162, 4.2542, 3.8055, 4.7948, 0.0000, 3.9207],\n        [2.3609, 4.1196, 4.4061, 4.2276, 4.2156, 4.4486, 3.4534, 4.3719, 3.1255,\n         4.2732, 3.4534, 3.8680, 4.4201, 3.9207, 0.0000]], device='cuda:0')"
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cdist(test_encoded['encodings'][0].detach(), test_encoded['encodings'][0].detach())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 1.        ,  0.15790567,  0.09273249,  0.11650714,  0.1552225 ,\n         0.09158262,  0.2808028 ,  0.114875  ,  0.48645896,  0.07288989,\n         0.2808028 ,  0.27564257,  0.12037336,  0.24630192,  0.7235265 ],\n       [ 0.15790567,  1.0000002 ,  0.61616194,  0.40563208,  0.37585837,\n         0.533515  ,  0.04750321,  0.20445731,  0.13855869,  0.33457285,\n         0.04750321,  0.3418637 ,  0.16368568,  0.24134715,  0.18388458],\n       [ 0.09273249,  0.61616194,  0.9999999 ,  0.2527678 ,  0.49875173,\n         0.5003265 ,  0.07558285,  0.16905624,  0.08226644,  0.25432837,\n         0.07558285,  0.2426284 ,  0.24468377,  0.11178365,  0.11592379],\n       [ 0.11650714,  0.40563208,  0.2527678 ,  0.9999999 ,  0.5773941 ,\n         0.18393034,  0.07033902,  0.24510682,  0.15042621,  0.18970096,\n         0.07033902,  0.16897836,  0.07142068,  0.3017862 ,  0.15029277],\n       [ 0.1552225 ,  0.37585837,  0.49875173,  0.5773941 ,  1.        ,\n         0.21211526,  0.11638689,  0.2842696 ,  0.17729124,  0.18592969,\n         0.11638689,  0.2765031 ,  0.26004803,  0.22595014,  0.18158752],\n       [ 0.09158262,  0.533515  ,  0.5003265 ,  0.18393034,  0.21211526,\n         1.        ,  0.04920483,  0.20614433,  0.08041088,  0.4614752 ,\n         0.04920483,  0.37260073,  0.34195948,  0.01732425,  0.09874126],\n       [ 0.2808028 ,  0.04750321,  0.07558285,  0.07033902,  0.11638689,\n         0.04920483,  0.9999999 ,  0.31503117,  0.4882003 ,  0.00937704,\n         0.9999999 ,  0.2230278 ,  0.06939309,  0.14071451,  0.42632505],\n       [ 0.114875  ,  0.20445731,  0.16905624,  0.24510682,  0.2842696 ,\n         0.20614433,  0.31503117,  0.99999976,  0.1290031 ,  0.09071637,\n         0.31503117,  0.15422006,  0.15482599,  0.04555161,  0.07333785],\n       [ 0.48645896,  0.13855869,  0.08226644,  0.15042621,  0.17729124,\n         0.08041088,  0.4882003 ,  0.1290031 ,  1.        ,  0.08289346,\n         0.4882003 ,  0.28607482,  0.09878434,  0.3862356 ,  0.539989  ],\n       [ 0.07288989,  0.33457285,  0.25432837,  0.18970096,  0.18592969,\n         0.4614752 ,  0.00937704,  0.09071637,  0.08289346,  1.        ,\n         0.00937704,  0.26555672,  0.21041308,  0.15466583,  0.12021631],\n       [ 0.2808028 ,  0.04750321,  0.07558285,  0.07033902,  0.11638689,\n         0.04920483,  0.9999999 ,  0.31503117,  0.4882003 ,  0.00937704,\n         0.9999999 ,  0.2230278 ,  0.06939309,  0.14071451,  0.42632505],\n       [ 0.27564257,  0.3418637 ,  0.2426284 ,  0.16897836,  0.2765031 ,\n         0.37260073,  0.2230278 ,  0.15422006,  0.28607482,  0.26555672,\n         0.2230278 ,  1.        ,  0.59298825,  0.30711073,  0.27462727],\n       [ 0.12037336,  0.16368568,  0.24468377,  0.07142068,  0.26004803,\n         0.34195948,  0.06939309,  0.15482599,  0.09878434,  0.21041308,\n         0.06939309,  0.59298825,  1.        , -0.06281612,  0.08529513],\n       [ 0.24630192,  0.24134715,  0.11178365,  0.3017862 ,  0.22595014,\n         0.01732425,  0.14071451,  0.04555161,  0.3862356 ,  0.15466583,\n         0.14071451,  0.30711073, -0.06281612,  1.        ,  0.27362   ],\n       [ 0.7235265 ,  0.18388458,  0.11592379,  0.15029277,  0.18158752,\n         0.09874126,  0.42632505,  0.07333785,  0.539989  ,  0.12021631,\n         0.42632505,  0.27462727,  0.08529513,  0.27362   ,  0.99999994]],\n      dtype=float32)"
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(test_encoded['encodings'][0].cpu(), test_encoded['encodings'][0].cpu())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.0000, 2.2734, 2.0998, 2.7346, 2.7159, 2.9733, 1.5934, 2.1232, 1.7050,\n         3.1253, 1.6709, 2.5840, 3.2646, 2.0620, 1.6717, 6.3625],\n        [2.2734, 0.0000, 1.4396, 1.6791, 1.8385, 1.3594, 1.6308, 1.6408, 1.7323,\n         1.8945, 1.6733, 2.0502, 2.5075, 1.8503, 2.4434, 6.1954],\n        [2.0998, 1.4396, 0.0000, 2.0194, 1.6432, 1.7038, 1.5826, 1.8199, 1.8213,\n         2.3624, 1.5903, 2.2806, 2.5148, 1.8953, 2.2275, 6.2978],\n        [2.7346, 1.6791, 2.0194, 0.0000, 1.6356, 1.9076, 1.9482, 1.7777, 2.0726,\n         2.3458, 1.9900, 2.5027, 2.8806, 2.0212, 2.7001, 6.6032],\n        [2.7159, 1.8385, 1.6432, 1.6356, 0.0000, 1.8432, 1.9687, 1.8321, 2.1143,\n         2.3798, 1.9909, 2.4990, 2.5622, 2.2019, 2.6260, 6.4537],\n        [2.9733, 1.3594, 1.7038, 1.9076, 1.8432, 0.0000, 2.1214, 2.1086, 2.2048,\n         1.7798, 2.0978, 2.1779, 2.3158, 2.3044, 2.9055, 6.3156],\n        [1.5934, 1.6308, 1.5826, 1.9482, 1.9687, 2.1214, 0.0000, 1.6770, 0.9452,\n         2.4744, 0.4019, 2.3578, 2.8023, 1.4944, 1.7441, 6.3021],\n        [2.1232, 1.6408, 1.8199, 1.7777, 1.8321, 2.1086, 1.6770, 0.0000, 1.8425,\n         2.3271, 1.6807, 2.2195, 2.7645, 1.9915, 2.4716, 6.3016],\n        [1.7050, 1.7323, 1.8213, 2.0726, 2.1143, 2.2048, 0.9452, 1.8425, 0.0000,\n         2.4941, 0.9922, 2.3089, 2.8546, 1.5236, 1.8153, 6.3823],\n        [3.1253, 1.8945, 2.3624, 2.3458, 2.3798, 1.7798, 2.4744, 2.3271, 2.4941,\n         0.0000, 2.3881, 2.2821, 2.6873, 2.4538, 3.2074, 6.3185],\n        [1.6709, 1.6733, 1.5903, 1.9900, 1.9909, 2.0978, 0.4019, 1.6807, 0.9922,\n         2.3881, 0.0000, 2.2577, 2.7592, 1.5137, 1.7559, 6.2608],\n        [2.5840, 2.0502, 2.2806, 2.5027, 2.4990, 2.1779, 2.3578, 2.2195, 2.3089,\n         2.2821, 2.2577, 0.0000, 2.0557, 2.1783, 2.7992, 6.1946],\n        [3.2646, 2.5075, 2.5148, 2.8806, 2.5622, 2.3158, 2.8023, 2.7645, 2.8546,\n         2.6873, 2.7592, 2.0557, 0.0000, 3.0631, 3.3045, 6.7285],\n        [2.0620, 1.8503, 1.8953, 2.0212, 2.2019, 2.3044, 1.4944, 1.9915, 1.5236,\n         2.4538, 1.5137, 2.1783, 3.0631, 0.0000, 2.1758, 6.1530],\n        [1.6717, 2.4434, 2.2275, 2.7001, 2.6260, 2.9055, 1.7441, 2.4716, 1.8153,\n         3.2074, 1.7559, 2.7992, 3.3045, 2.1758, 0.0000, 6.4373],\n        [6.3625, 6.1954, 6.2978, 6.6032, 6.4537, 6.3156, 6.3021, 6.3016, 6.3823,\n         6.3185, 6.2608, 6.1946, 6.7285, 6.1530, 6.4373, 0.0000]],\n       device='cuda:0')"
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cdist(test_output[2][0], test_output[2][0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1.        , 0.8088258 , 0.7897816 , 0.719144  , 0.704582  ,\n        0.69052804, 0.882852  , 0.8322343 , 0.87681556, 0.741297  ,\n        0.87235296, 0.8042887 , 0.71701777, 0.802703  , 0.8782024 ,\n        0.4413397 ],\n       [0.8088258 , 1.0000001 , 0.9370428 , 0.90465057, 0.88142323,\n        0.9416651 , 0.9084791 , 0.90830594, 0.89291596, 0.9179991 ,\n        0.90199155, 0.881487  , 0.8449291 , 0.87772524, 0.7798788 ,\n        0.48842746],\n       [0.7897816 , 0.9370428 , 1.        , 0.852502  , 0.8963144 ,\n        0.9199401 , 0.874556  , 0.8821808 , 0.8541694 , 0.8914889 ,\n        0.8756376 , 0.8648577 , 0.86965835, 0.8188772 , 0.77020246,\n        0.45904002],\n       [0.719144  , 0.90465057, 0.852502  , 0.99999976, 0.906376  ,\n        0.8839818 , 0.8625478 , 0.8923702 , 0.84478676, 0.86452353,\n        0.85571444, 0.8197846 , 0.7891209 , 0.8508991 , 0.72971463,\n        0.39265084],\n       [0.704582  , 0.88142323, 0.8963144 , 0.906376  , 1.        ,\n        0.88993466, 0.84495044, 0.88108313, 0.8273524 , 0.8624563 ,\n        0.8417534 , 0.818232  , 0.8388283 , 0.80353725, 0.7283248 ,\n        0.4237517 ],\n       [0.69052804, 0.9416651 , 0.9199401 , 0.8839818 , 0.88993466,\n        1.0000001 , 0.852685  , 0.85731685, 0.8381766 , 0.92582434,\n        0.85579884, 0.86733747, 0.8692201 , 0.82159066, 0.7081288 ,\n        0.46536368],\n       [0.882852  , 0.9084791 , 0.874556  , 0.8625478 , 0.84495044,\n        0.852685  , 1.        , 0.9001957 , 0.96382284, 0.86459684,\n        0.9923751 , 0.8463322 , 0.81081855, 0.89158714, 0.8636365 ,\n        0.4572326 ],\n       [0.8322343 , 0.90830594, 0.8821808 , 0.8923702 , 0.88108313,\n        0.85731685, 0.9001957 , 1.0000002 , 0.8766899 , 0.8672885 ,\n        0.89900565, 0.85967815, 0.8072299 , 0.85323465, 0.7721143 ,\n        0.46351397],\n       [0.87681556, 0.89291596, 0.8541694 , 0.84478676, 0.8273524 ,\n        0.8381766 , 0.96382284, 0.8766899 , 0.9999999 , 0.8500078 ,\n        0.9593282 , 0.84792364, 0.7941841 , 0.9002738 , 0.86253864,\n        0.43805778],\n       [0.741297  , 0.9179991 , 0.8914889 , 0.86452353, 0.8624563 ,\n        0.92582434, 0.86459684, 0.8672885 , 0.8500078 , 1.0000002 ,\n        0.87621933, 0.87354386, 0.8343549 , 0.868654  , 0.7246829 ,\n        0.48441184],\n       [0.87235296, 0.90199155, 0.8756376 , 0.85571444, 0.8417534 ,\n        0.85579884, 0.9923751 , 0.89900565, 0.9593282 , 0.87621933,\n        0.99999994, 0.8613403 , 0.8171015 , 0.89036703, 0.8628691 ,\n        0.46847165],\n       [0.8042887 , 0.881487  , 0.8648577 , 0.8197846 , 0.818232  ,\n        0.86733747, 0.8463322 , 0.85967815, 0.84792364, 0.87354386,\n        0.8613403 , 0.9999998 , 0.8989589 , 0.8760016 , 0.76476324,\n        0.498787  ],\n       [0.71701777, 0.8449291 , 0.86965835, 0.7891209 , 0.8388283 ,\n        0.8692201 , 0.81081855, 0.8072299 , 0.7941841 , 0.8343549 ,\n        0.8171015 , 0.8989589 , 1.0000001 , 0.76029396, 0.7089428 ,\n        0.4049119 ],\n       [0.802703  , 0.87772524, 0.8188772 , 0.8508991 , 0.80353725,\n        0.82159066, 0.89158714, 0.85323465, 0.9002738 , 0.868654  ,\n        0.89036703, 0.8760016 , 0.76029396, 0.9999999 , 0.7861352 ,\n        0.49817854],\n       [0.8782024 , 0.7798788 , 0.77020246, 0.72971463, 0.7283248 ,\n        0.7081288 , 0.8636365 , 0.7721143 , 0.86253864, 0.7246829 ,\n        0.8628691 , 0.76476324, 0.7089428 , 0.7861352 , 1.        ,\n        0.422373  ],\n       [0.4413397 , 0.48842746, 0.45904002, 0.39265084, 0.4237517 ,\n        0.46536368, 0.4572326 , 0.46351397, 0.43805778, 0.48441184,\n        0.46847165, 0.498787  , 0.4049119 , 0.49817854, 0.422373  ,\n        1.0000002 ]], dtype=float32)"
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(test_output[2][0].cpu(), test_output[2][0].cpu())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[2.8510, 3.6089, 4.2648, 3.8091, 3.8867, 4.2695, 3.5289, 4.1104, 2.7641,\n         3.8951, 3.5289, 3.3130, 4.2320, 3.2051, 2.8737],\n        [3.8565, 2.8241, 3.8928, 3.8030, 3.9234, 3.6076, 4.2010, 4.4479, 3.5562,\n         3.7087, 4.2010, 3.4794, 4.3094, 3.7405, 3.5147],\n        [3.4233, 2.6156, 3.1798, 3.2939, 3.4030, 3.1691, 3.7193, 3.8341, 3.2288,\n         3.1485, 3.7193, 2.9924, 3.8031, 3.1931, 3.2125],\n        [4.0874, 3.2980, 4.0922, 3.1800, 3.8168, 3.8462, 4.1764, 4.4409, 3.6749,\n         3.9560, 4.1764, 3.7993, 4.3695, 3.9264, 3.6859],\n        [4.0343, 3.0715, 3.7143, 3.2754, 3.2186, 3.5657, 4.1791, 4.0101, 3.7357,\n         3.5709, 4.1791, 3.5518, 4.1522, 3.7547, 3.7564],\n        [4.2393, 3.0261, 3.8623, 3.8334, 4.0260, 3.1063, 4.4775, 4.5144, 3.9729,\n         3.4988, 4.4775, 3.5510, 4.3251, 4.0624, 3.9496],\n        [3.3792, 3.1575, 3.9880, 3.3661, 3.6529, 3.7825, 3.4217, 3.9212, 2.8901,\n         3.5045, 3.4217, 3.1687, 4.1832, 3.1710, 3.0804],\n        [3.7243, 3.5450, 4.1167, 3.8144, 3.7629, 3.9784, 3.7192, 4.0254, 3.2452,\n         4.0079, 3.7192, 3.4931, 4.2340, 3.7356, 3.3624],\n        [3.5368, 3.3412, 4.1666, 3.5932, 3.8453, 3.9523, 3.7358, 4.1717, 2.7615,\n         3.6963, 3.7358, 3.3411, 4.3825, 3.2790, 3.3321],\n        [4.6688, 3.9088, 4.5971, 4.5596, 4.6453, 4.1660, 4.8672, 5.1748, 4.2432,\n         3.5745, 4.8672, 4.1332, 4.8526, 4.3053, 4.2716],\n        [3.4113, 3.2236, 4.0000, 3.4545, 3.6935, 3.7473, 3.4351, 3.9674, 2.9232,\n         3.4634, 3.4351, 3.1090, 4.1651, 3.1898, 3.0987],\n        [4.1443, 3.7945, 4.4173, 4.3583, 4.3403, 4.0850, 4.4561, 4.8194, 3.8120,\n         4.1711, 4.4561, 3.0378, 4.1725, 3.8462, 3.9745],\n        [4.6895, 3.7880, 4.4454, 4.2518, 4.3095, 4.1726, 5.0381, 4.8485, 4.5441,\n         4.1944, 5.0381, 3.4264, 4.0900, 4.4198, 4.5575],\n        [3.5112, 3.2871, 4.0225, 3.4970, 3.7434, 3.8772, 3.7225, 4.1655, 3.0262,\n         3.6149, 3.7225, 3.2203, 4.1767, 2.7618, 3.2931],\n        [3.2434, 3.5019, 4.1813, 3.6583, 3.9260, 4.1162, 3.6407, 4.0850, 3.0787,\n         3.7906, 3.6407, 3.4019, 4.3665, 3.2483, 2.9681],\n        [6.9773, 6.9118, 7.2536, 7.5905, 7.2497, 7.0803, 6.8741, 7.5217, 6.8565,\n         6.9569, 6.8741, 6.7343, 7.3282, 6.9302, 6.8363]], device='cuda:0')"
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cdist(test_output[2][0], test_encoded['encodings'][0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.61240834, 0.39606062, 0.19933596, 0.33439296, 0.32772204,\n        0.1975444 , 0.42242968, 0.21023484, 0.6527048 , 0.29509702,\n        0.42242968, 0.48731837, 0.19031198, 0.5315366 , 0.6189385 ],\n       [0.4008254 , 0.693552  , 0.42703247, 0.43596533, 0.41285384,\n        0.50899744, 0.30240712, 0.21114874, 0.5117265 , 0.4592294 ,\n        0.30240712, 0.5240295 , 0.28052935, 0.45718893, 0.5180512 ],\n       [0.38921118, 0.65606827, 0.5213343 , 0.4607897 , 0.4441473 ,\n        0.5245586 , 0.30345193, 0.25354064, 0.4873229 , 0.5002415 ,\n        0.30345193, 0.54551655, 0.2924835 , 0.49676663, 0.48331466],\n       [0.32421252, 0.5763266 , 0.3660668 , 0.6098894 , 0.44489267,\n        0.44087878, 0.3108157 , 0.21377544, 0.47780806, 0.3825348 ,\n        0.3108157 , 0.42918497, 0.2600621 , 0.4006325 , 0.46851206],\n       [0.3021291 , 0.60992146, 0.44999203, 0.5588709 , 0.5842282 ,\n        0.4933554 , 0.2701199 , 0.3243491 , 0.4282776 , 0.4688541 ,\n        0.2701199 , 0.47208145, 0.29570097, 0.4208351 , 0.41425234],\n       [0.31871042, 0.6733695 , 0.47063175, 0.4640681 , 0.41904935,\n        0.66293514, 0.25398594, 0.23678917, 0.4262066 , 0.55418503,\n        0.25398594, 0.53837067, 0.3189192 , 0.39773718, 0.42707253],\n       [0.429711  , 0.51805687, 0.27247238, 0.45856765, 0.38265425,\n        0.34564024, 0.43385923, 0.25062227, 0.6047319 , 0.4051904 ,\n        0.43385923, 0.5106184 , 0.17662078, 0.52249485, 0.543396  ],\n       [0.43699825, 0.5029914 , 0.35268396, 0.4271099 , 0.45572394,\n        0.39589405, 0.45137814, 0.35093063, 0.5913302 , 0.35973793,\n        0.45137814, 0.5152146 , 0.2997041 , 0.45348573, 0.5556704 ],\n       [0.4320563 , 0.5074359 , 0.26902825, 0.43540257, 0.371113  ,\n        0.34229288, 0.38321966, 0.22455783, 0.6700694 , 0.39540806,\n        0.38321966, 0.5040924 , 0.17057025, 0.5326871 , 0.5120044 ],\n       [0.32946515, 0.55872226, 0.38117594, 0.37832433, 0.36319888,\n        0.5008026 , 0.27774143, 0.1704716 , 0.47242674, 0.6421828 ,\n        0.27774143, 0.4968865 , 0.2935217 , 0.45423025, 0.4606596 ],\n       [0.42695278, 0.5043384 , 0.27706364, 0.4371547 , 0.37664774,\n        0.3656101 , 0.4369949 , 0.24310952, 0.6007457 , 0.42677101,\n        0.4369949 , 0.5352774 , 0.19417697, 0.5229947 , 0.5440481 ],\n       [0.42027062, 0.5298419 , 0.36824417, 0.3696106 , 0.38694113,\n        0.4641449 , 0.33433115, 0.20949747, 0.5296696 , 0.4218815 ,\n        0.33433115, 0.7147428 , 0.4306664 , 0.5197201 , 0.48100328],\n       [0.33014923, 0.59516656, 0.4305814 , 0.47392273, 0.46528202,\n        0.504538  , 0.2288831 , 0.28798473, 0.39242494, 0.4871313 ,\n        0.2288831 , 0.6817299 , 0.5209365 , 0.42773756, 0.38345852],\n       [0.3818287 , 0.47576302, 0.25726834, 0.4135228 , 0.34941408,\n        0.3100267 , 0.32749248, 0.15118483, 0.56509686, 0.36476013,\n        0.32749248, 0.492674  , 0.17624287, 0.63656396, 0.4762711 ],\n       [0.5102724 , 0.4447251 , 0.24687682, 0.40011618, 0.32896686,\n        0.27009898, 0.3995486 , 0.23802271, 0.5790212 , 0.34796613,\n        0.3995486 , 0.4722708 , 0.15707004, 0.52986795, 0.603185  ]],\n      dtype=float32)"
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(test_output[2][0][:-1].cpu(), test_encoded['encodings'][0].cpu())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[2.5671, 2.7597, 3.3063, 2.9722, 2.8253, 3.3752, 3.6660, 2.9233],\n        [3.2390, 3.1299, 2.5967, 2.7111, 2.7886, 3.4478, 3.6980, 3.2700],\n        [3.0660, 3.2342, 3.3027, 2.5255, 2.8296, 3.4488, 3.8317, 3.0841],\n        [3.4916, 3.5352, 3.4793, 3.1186, 2.6600, 3.8441, 4.1601, 3.2892],\n        [3.0600, 3.1149, 3.1777, 2.8568, 2.8587, 2.7464, 3.3875, 2.6347],\n        [3.2911, 3.6312, 3.7015, 3.3788, 3.3048, 3.4174, 3.4775, 3.1840],\n        [3.4213, 3.6574, 3.7432, 3.3750, 3.2135, 3.4877, 4.0184, 2.3332],\n        [3.7509, 3.8965, 4.0852, 3.8383, 3.8490, 3.7951, 3.8789, 2.9148],\n        [4.4709, 4.6435, 4.9981, 4.7081, 4.4533, 4.7905, 5.1068, 4.5436]],\n       device='cuda:0')"
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cdist(test_output[1][0][1:], test_encoded['encodings'][0][:-1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0.0022, 0.0049, 0.0040, 0.0033, 0.0057, 0.0036, 0.0052, 0.0104, 0.0269],\n       device='cuda:0')"
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.mse_loss(test_output[1][0][1:].detach(),\n",
    "           test_output[1][0][:-1].detach(), reduction='none').mean(-1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 0]], device='cuda:0')"
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output[3].softmax(-1).argmax(-1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
