{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from typing import List\n",
    "\n",
    "import datasets\n",
    "import evaluate\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary\n",
    "from tqdm.notebook import tqdm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Util functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output.last_hidden_state  #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def compute_clm_loss(logits, labels):\n",
    "    # Classical Language modeling task (nope)\n",
    "    # Next token prediction task in causual setup\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "    shift_labels = labels[..., 1:].contiguous()\n",
    "    loss = F.cross_entropy(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1), ignore_index=-100)\n",
    "    return loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def compute_shifted_cossim_loss(input_logits, output_logits):\n",
    "    # Classical Language modeling task (nope)\n",
    "    # Next token prediction task in causual setup\n",
    "    shift_input = input_logits[..., :-1, :].contiguous()\n",
    "    shift_output = output_logits[..., 1:, :].contiguous()\n",
    "    loss = (1 - F.cosine_similarity(shift_input, shift_output, -1)).mean()\n",
    "    return loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using daily_dialog dataset\n",
    "This dataset contains emotions and acts, each dialog is a separate list of lines"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset daily_dialog (/home/hivaze/.cache/huggingface/datasets/daily_dialog/default/1.0.0/1d0a58c7f2a4dab5ed9d01dbde8e55e0058e589ab81fce5c2df929ea810eabcd)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4f13b3ad1df348838285d83b1d1161eb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['dialog', 'act', 'emotion'],\n        num_rows: 11118\n    })\n    validation: Dataset({\n        features: ['dialog', 'act', 'emotion'],\n        num_rows: 1000\n    })\n    test: Dataset({\n        features: ['dialog', 'act', 'emotion'],\n        num_rows: 1000\n    })\n})"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd_dataset = datasets.load_dataset('daily_dialog')\n",
    "dd_dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "{'dialog': [['Say , Jim , how about going for a few beers after dinner ? ',\n   ' You know that is tempting but is really not good for our fitness . ',\n   ' What do you mean ? It will help us to relax . ',\n   \" Do you really think so ? I don't . It will just make us fat and act silly . Remember last time ? \",\n   \" I guess you are right.But what shall we do ? I don't feel like sitting at home . \",\n   ' I suggest a walk over to the gym where we can play singsong and meet some of our friends . ',\n   \" That's a good idea . I hear Mary and Sally often go there to play pingpong.Perhaps we can make a foursome with them . \",\n   ' Sounds great to me ! If they are willing , we could ask them to go dancing with us.That is excellent exercise and fun , too . ',\n   \" Good.Let ' s go now . \",\n   ' All right . '],\n  ['Can you do push-ups ? ',\n   \" Of course I can . It's a piece of cake ! Believe it or not , I can do 30 push-ups a minute . \",\n   \" Really ? I think that's impossible ! \",\n   ' You mean 30 push-ups ? ',\n   ' Yeah ! ',\n   \" It's easy . If you do exercise everyday , you can make it , too . \"],\n  ['Can you study with the radio on ? ',\n   ' No , I listen to background music . ',\n   ' What is the difference ? ',\n   ' The radio has too many comerials . ',\n   \" That's true , but then you have to buy a record player . \"]],\n 'act': [[3, 4, 2, 2, 2, 3, 4, 1, 3, 4], [2, 1, 2, 2, 1, 1], [2, 1, 2, 1, 1]],\n 'emotion': [[0, 0, 0, 0, 0, 0, 4, 4, 4, 4],\n  [0, 0, 6, 0, 0, 0],\n  [0, 0, 0, 0, 0]]}"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd_dataset['train'][:3]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "{0, 1, 2, 3, 4, 5, 6}"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce(lambda x, y: set(x) | set(y), dd_dataset['train']['emotion'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "{1, 2, 3, 4}"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce(lambda x, y: set(x) | set(y), dd_dataset['train']['act'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "(array([3.791e+03, 3.057e+03, 2.119e+03, 1.669e+03, 3.110e+02, 1.090e+02,\n        4.000e+01, 1.100e+01, 8.000e+00, 3.000e+00]),\n array([ 2. ,  5.3,  8.6, 11.9, 15.2, 18.5, 21.8, 25.1, 28.4, 31.7, 35. ]),\n <BarContainer object of 10 artists>)"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dialogs_lengths = [len(x) for x in dd_dataset['train']['dialog']]\n",
    "plt.hist(dialogs_lengths)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Phrase encoder model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /home/hivaze/.cache/torch/sentence_transformers/intfloat_e5-base. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "data": {
      "text/plain": "SentenceTransformer(\n  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: BertModel \n  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# phrase_model = 'roberta-base'\n",
    "# phrase_model = 'microsoft/deberta-v3-base'\n",
    "# phrase_model = 'sentence-transformers/all-MiniLM-L12-v2'\n",
    "# phrase_model = 'sentence-transformers/bert-base-nli-mean-tokens'\n",
    "phrase_model = 'intfloat/e5-base'\n",
    "# phrase_model = 'cardiffnlp/twitter-xlm-roberta-base-sentiment'\n",
    "# phrase_model = 'sentence-transformers/paraphrase-mpnet-base-v2'\n",
    "# phrase_model = 'sentence-transformers/sentence-t5-base'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(phrase_model)\n",
    "# model = AutoModel.from_pretrained(phrase_model).to(device)\n",
    "# model\n",
    "sent_transformer = SentenceTransformer(model_name_or_path=phrase_model, device=device)\n",
    "sent_transformer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "SentenceTransformer(\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_transformer.max_seq_length = 256\n",
    "sent_transformer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([5, 768])"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_phrases = ['Some day i will go to school',\n",
    "                'To make maximum progress on addressing these pressing problems',\n",
    "                'I will nether go to school',\n",
    "                'I like to visit school',\n",
    "                'THe day will come when i will go to school']\n",
    "\n",
    "phrases_encodings = sent_transformer.encode(test_phrases, convert_to_tensor=True, normalize_embeddings=True)\n",
    "phrases_encodings.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0.7232, 0.8850, 0.8622, 0.9491], device='cuda:0')"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cosine_similarity(phrases_encodings[0], phrases_encodings[1:])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.0000, 0.7441, 0.4796, 0.5250, 0.3190],\n        [0.7441, 0.0000, 0.7644, 0.7989, 0.7910],\n        [0.4796, 0.7644, 0.0000, 0.5644, 0.4903],\n        [0.5250, 0.7989, 0.5644, 0.0000, 0.5372],\n        [0.3190, 0.7910, 0.4903, 0.5372, 0.0000]], device='cuda:0')"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cdist(phrases_encodings, phrases_encodings)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dialog encoder model\n",
    "\n",
    "Концептуально тут нужно:\n",
    "- модель-кодировщик фраз (замоороженная) -> готовые эмбединги текста\n",
    "- эмбединги участников диалога\n",
    "- эмбединги позиции текста в диалоге\n",
    "- кастомный токенизер c BOS и EOS\n",
    "- causual lm crossentropy loss\n",
    "- causual маска атеншена\n",
    "- финальный классификатор в условный словарь"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "class DialogEmbeddings(nn.Module):\n",
    "    def __init__(self, encoder_hidden_dim: int,\n",
    "                 max_interlocutors_count: int,\n",
    "                 max_dialogue_length: int,\n",
    "                 dropout_p: float):\n",
    "        super(DialogEmbeddings, self).__init__()\n",
    "\n",
    "        self.padding_idx = 0  # special index for padding (must be in tokenizer)\n",
    "\n",
    "        self.position_embeddings = nn.Embedding(max_dialogue_length + 1,  # padding\n",
    "                                                encoder_hidden_dim, padding_idx=self.padding_idx)\n",
    "        self.interlocutors_embeddings = nn.Embedding(max_interlocutors_count + 2,  # padding + bos\n",
    "                                                     encoder_hidden_dim, padding_idx=self.padding_idx)\n",
    "        self.norm = nn.LayerNorm(encoder_hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, interlocutors_ids: torch.LongTensor, position_ids: torch.LongTensor = None):\n",
    "        if position_ids is None:\n",
    "            position_ids = self.create_position_ids_from_input_ids(interlocutors_ids)\n",
    "\n",
    "        interlocutors_embeds = self.interlocutors_embeddings(interlocutors_ids)\n",
    "        position_embeds = self.position_embeddings(position_ids)\n",
    "\n",
    "        embeddings = interlocutors_embeds + position_embeds\n",
    "        embeddings = self.norm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    def create_position_ids_from_input_ids(self, input_ids):\n",
    "        \"\"\"\n",
    "        Replace non-padding symbols with their position numbers. Position numbers begin at padding_idx+1. Padding symbols\n",
    "        are ignored. This is modified from fairseq's `utils.make_positions`. :param torch.Tensor x: :return torch.Tensor:\n",
    "        \"\"\"\n",
    "        # The series of casts and type-conversions here are carefully balanced to both work with ONNX export and XLA.\n",
    "        mask = input_ids.ne(self.padding_idx).int()\n",
    "        incremental_indices = torch.cumsum(mask, dim=1).type_as(mask) * mask\n",
    "        return incremental_indices.long() + self.padding_idx"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "class DialogTransformer(nn.Module):\n",
    "    def __init__(self, encoder_hidden_dim: int,\n",
    "                 out_vocab_size: int,\n",
    "                 max_dialogue_length: int,\n",
    "                 max_interlocutors_count: int,\n",
    "                 decoder_n_layers: int = 1,\n",
    "                 decoder_n_head: int = 4,\n",
    "                 dim_feedforward_mult: int = 3,\n",
    "                 dropout_p: float = 0.1):\n",
    "        super(DialogTransformer, self).__init__()\n",
    "\n",
    "        self.norm = nn.LayerNorm(encoder_hidden_dim)\n",
    "        self.eos_vector = nn.Parameter(torch.randn([encoder_hidden_dim]), requires_grad=True)\n",
    "        self.dialogue_embeddings = DialogEmbeddings(encoder_hidden_dim, max_interlocutors_count,\n",
    "                                                    max_dialogue_length, dropout_p)\n",
    "\n",
    "        decoder_ff_inner_dim = encoder_hidden_dim * dim_feedforward_mult\n",
    "        layer = nn.TransformerEncoderLayer(d_model=encoder_hidden_dim,\n",
    "                                           nhead=decoder_n_head,\n",
    "                                           dim_feedforward=decoder_ff_inner_dim,\n",
    "                                           activation=F.gelu,  # using gelu instead of default relu\n",
    "                                           dropout=dropout_p,\n",
    "                                           batch_first=True)  # using encoder layers due to not a seq2seq setup\n",
    "        self.model = nn.TransformerEncoder(layer, decoder_n_layers)\n",
    "\n",
    "        self.labels_projector = nn.Linear(in_features=encoder_hidden_dim, out_features=out_vocab_size, bias=True)\n",
    "        self.interlocutors_projector = nn.Linear(in_features=encoder_hidden_dim, out_features=max_interlocutors_count, bias=True)\n",
    "\n",
    "    def forward(self, encodings: torch.FloatTensor,\n",
    "                interlocutors_ids: torch.LongTensor,\n",
    "                position_ids: torch.LongTensor = None,\n",
    "                attention_mask: torch.BoolTensor = None,\n",
    "                labels: torch.LongTensor = None):\n",
    "        \"\"\"\n",
    "        :param encodings: Pooled hiddens from sentence-transformer in shape [bs, lines_count, hidden_dim]\n",
    "        :param labels: Labels for dialog lines\n",
    "        :param interlocutors_ids: shape [bs, seq_len], interlocutors for each line (from one)\n",
    "        :param position_ids: shape [bs, seq_len], position of line in dialogue (from one)\n",
    "        :param attention_mask: shape [bs, seq_len], attention mask for padding where 1 is disabled and 0 is enabled\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = encodings.shape[0]\n",
    "\n",
    "        x = self.norm(encodings)\n",
    "\n",
    "        representation = torch.cat([\n",
    "            x,\n",
    "            self.eos_vector.repeat([batch_size, 1, 1]),\n",
    "        ], dim=1)  # insert bos vector\n",
    "\n",
    "        x = representation + self.dialogue_embeddings(interlocutors_ids=interlocutors_ids, position_ids=position_ids)\n",
    "\n",
    "        causal_mask = torch.triu(torch.ones(x.shape[1], x.shape[1]), diagonal=1).bool().to(\n",
    "            x.device)  # only attend to past (not necessary, but logical...)\n",
    "        x = self.model.forward(src=x, mask=causal_mask, src_key_padding_mask=attention_mask)\n",
    "\n",
    "        predicted_labels = self.labels_projector(x)\n",
    "        predicted_interlocutors = self.interlocutors_projector(x)\n",
    "\n",
    "        if labels is not None:\n",
    "            copied_interlocutors = interlocutors_ids.clone() - 2\n",
    "            copied_interlocutors[copied_interlocutors < 0] = -100\n",
    "\n",
    "            interlocutors_loss = compute_clm_loss(predicted_interlocutors, copied_interlocutors)\n",
    "            logits_loss = compute_shifted_cossim_loss(x, representation)\n",
    "\n",
    "            total_loss = logits_loss + interlocutors_loss\n",
    "\n",
    "            return total_loss, predicted_labels, predicted_interlocutors\n",
    "\n",
    "        # if labels is not None:\n",
    "        #     labels_loss = compute_clm_loss(predicted_labels, labels)\n",
    "        #\n",
    "        #     copied_interlocutors = interlocutors_ids.clone() - 2\n",
    "        #     copied_interlocutors[copied_interlocutors < 0] = -100\n",
    "        #     interlocutors_loss = compute_clm_loss(predicted_interlocutors, copied_interlocutors)\n",
    "        #\n",
    "        #     total_loss = labels_loss + interlocutors_loss\n",
    "        #\n",
    "        #     return total_loss, predicted_labels, predicted_interlocutors\n",
    "\n",
    "        return predicted_labels, predicted_interlocutors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[False,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n        [False, False,  True,  True,  True,  True,  True,  True,  True,  True],\n        [False, False, False,  True,  True,  True,  True,  True,  True,  True],\n        [False, False, False, False,  True,  True,  True,  True,  True,  True],\n        [False, False, False, False, False,  True,  True,  True,  True,  True],\n        [False, False, False, False, False, False,  True,  True,  True,  True],\n        [False, False, False, False, False, False, False,  True,  True,  True],\n        [False, False, False, False, False, False, False, False,  True,  True],\n        [False, False, False, False, False, False, False, False, False,  True],\n        [False, False, False, False, False, False, False, False, False, False]])"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.triu(torch.ones(10, 10), diagonal=1).bool()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "DialogTransformer(\n  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (dialogue_embeddings): DialogEmbeddings(\n    (position_embeddings): Embedding(51, 768, padding_idx=0)\n    (interlocutors_embeddings): Embedding(4, 768, padding_idx=0)\n    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (model): TransformerEncoder(\n    (layers): ModuleList(\n      (0): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n      (1): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n      (2): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n      (3): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n      (4): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n      (5): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n    )\n  )\n  (labels_projector): Linear(in_features=768, out_features=7, bias=True)\n  (interlocutors_projector): Linear(in_features=768, out_features=2, bias=True)\n)"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialo_transformer = DialogTransformer(encoder_hidden_dim=768,\n",
    "                                      out_vocab_size=7,\n",
    "                                      max_dialogue_length=50,\n",
    "                                      max_interlocutors_count=2,\n",
    "                                      decoder_n_layers=6,\n",
    "                                      decoder_n_head=8,\n",
    "                                      dim_feedforward_mult=4,\n",
    "                                      dropout_p=0.1\n",
    "                                      ).to(sent_transformer.device).eval()\n",
    "dialo_transformer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "==========================================================================================\nLayer (type:depth-idx)                                            Param #\n==========================================================================================\nDialogTransformer                                                 768\n├─LayerNorm: 1-1                                                  1,536\n├─DialogEmbeddings: 1-2                                           --\n│    └─Embedding: 2-1                                             39,168\n│    └─Embedding: 2-2                                             3,072\n│    └─LayerNorm: 2-3                                             1,536\n│    └─Dropout: 2-4                                               --\n├─TransformerEncoder: 1-3                                         --\n│    └─ModuleList: 2-5                                            --\n│    │    └─TransformerEncoderLayer: 3-1                          7,087,872\n│    │    └─TransformerEncoderLayer: 3-2                          7,087,872\n│    │    └─TransformerEncoderLayer: 3-3                          7,087,872\n│    │    └─TransformerEncoderLayer: 3-4                          7,087,872\n│    │    └─TransformerEncoderLayer: 3-5                          7,087,872\n│    │    └─TransformerEncoderLayer: 3-6                          7,087,872\n├─Linear: 1-4                                                     5,383\n├─Linear: 1-5                                                     1,538\n==========================================================================================\nTotal params: 42,580,233\nTrainable params: 42,580,233\nNon-trainable params: 0\n=========================================================================================="
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(dialo_transformer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "class DialogTokenizer:\n",
    "    \"\"\"\n",
    "    Accepts dicts with keys: 'dialog' - required, 'interlocutors' and 'labels'\n",
    "    Must return dict with 'encoder_hidden', 'interlocutors_ids' and 'labels'\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lines_encoder: SentenceTransformer,\n",
    "                 all_labels: list = None,\n",
    "                 all_interlocutors: list = None):\n",
    "\n",
    "        self.lines_encoder = lines_encoder\n",
    "\n",
    "        self.padding_idx = 0\n",
    "        self.eos_idx = 1\n",
    "\n",
    "        if all_labels:\n",
    "            all_labels = set(all_labels)\n",
    "            self.id2label = dict(zip(range(len(all_labels)), all_labels))\n",
    "            self.label2id = {v: k for k, v in self.id2label.items()}\n",
    "        if all_interlocutors:\n",
    "            all_interlocutors = set(all_interlocutors)\n",
    "            self.id2interlocutors = dict(zip(range(2, len(all_interlocutors) + 2), all_interlocutors))\n",
    "            self.interlocutor2id = {v: k for k, v in self.id2label.items()}\n",
    "\n",
    "    def encode(self, dialog: List[str],\n",
    "               interlocutors: List[int] = None,\n",
    "               labels: List[int] = None,\n",
    "               lines_batch_size=32,\n",
    "               unsqueeze=True):\n",
    "\n",
    "        encodings = self.lines_encoder.encode(sentences=dialog,\n",
    "                                              batch_size=lines_batch_size,\n",
    "                                              normalize_embeddings=True,\n",
    "                                              show_progress_bar=False,\n",
    "                                              convert_to_tensor=True)\n",
    "        encodings.requires_grad = False\n",
    "\n",
    "        if interlocutors is None:\n",
    "            interlocutors = [(i % 2) + 2 for i in range(len(dialog))]\n",
    "        elif hasattr(self, 'interlocutor2id'):\n",
    "            interlocutors = list(map(self.interlocutor2id.get, interlocutors))\n",
    "        if labels is not None and hasattr(self, 'label2id'):\n",
    "            labels = list(map(self.label2id.get, labels))\n",
    "\n",
    "        interlocutors = interlocutors + [self.eos_idx]\n",
    "        if labels is not None:\n",
    "            labels = labels + [-100]\n",
    "\n",
    "        result = {\n",
    "            'encodings': encodings.unsqueeze(0) if unsqueeze else encodings,\n",
    "            'interlocutors_ids': torch.LongTensor([interlocutors] if unsqueeze else interlocutors).to(encodings.device)\n",
    "        }\n",
    "        if labels is not None:\n",
    "            result['labels'] = torch.LongTensor([labels] if unsqueeze else labels).to(encodings.device)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def encode_batch(self, dialog: List[List[str]],\n",
    "                     interlocutors: List[List[int]] = None,\n",
    "                     labels: List[List[int]] = None,\n",
    "                     lines_batch_size=50):\n",
    "\n",
    "        if interlocutors is None:\n",
    "            interlocutors = [None] * len(dialog)\n",
    "        if labels is None:\n",
    "            labels = [None] * len(dialog)\n",
    "        unsqueeze = [False] * len(dialog)\n",
    "\n",
    "        assert len(dialog) == len(interlocutors) and len(interlocutors) == len(labels)\n",
    "\n",
    "        zipped = zip(dialog, interlocutors, labels, [lines_batch_size] * len(dialog), unsqueeze)\n",
    "        encoded_batch = list(map(lambda x: self.encode(*x), zipped))\n",
    "\n",
    "        encodings = pad_sequence([encode_dict['encodings'] for encode_dict in encoded_batch],\n",
    "                                 batch_first=True,\n",
    "                                 padding_value=self.padding_idx)\n",
    "        encodings.requires_grad = False\n",
    "\n",
    "        interlocutors_ids = pad_sequence([encode_dict['interlocutors_ids'] for encode_dict in encoded_batch],\n",
    "                                         batch_first=True,\n",
    "                                         padding_value=self.padding_idx)\n",
    "\n",
    "        lengths = [len(dial) + 1 for dial in dialog]  # keep in mind bos vector\n",
    "        masks = list(map(lambda x: torch.zeros(size=[x]), lengths))\n",
    "        attention_masks = pad_sequence(masks, batch_first=True, padding_value=1).bool().to(encodings.device)\n",
    "\n",
    "        result = {\n",
    "            'encodings': encodings,\n",
    "            'interlocutors_ids': interlocutors_ids,\n",
    "            'attention_mask': attention_masks\n",
    "        }\n",
    "        if labels[0] is not None:\n",
    "            labels = pad_sequence([encode_dict['labels'] for encode_dict in encoded_batch],\n",
    "                                  batch_first=True,\n",
    "                                  padding_value=-100)\n",
    "            result['labels'] = labels\n",
    "\n",
    "        return result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "dialo_tokenizer = DialogTokenizer(sent_transformer,\n",
    "                                  # all_labels=[1, 2, 3, 4]\n",
    "                                  )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "{'encodings': tensor([[[-0.0042,  0.0217, -0.0237,  ...,  0.0116,  0.0340,  0.0314],\n          [-0.0369,  0.0174,  0.0452,  ...,  0.0200,  0.0160,  0.0653],\n          [-0.0401,  0.0212, -0.0176,  ...,  0.0143,  0.0121,  0.0441]]],\n        device='cuda:0'),\n 'interlocutors_ids': tensor([[2, 3, 2, 1]], device='cuda:0'),\n 'labels': tensor([[   1,    3,    1, -100]], device='cuda:0')}"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialo_encoded = dialo_tokenizer.encode(['Hello man', 'Goodbye', 'Thanks'],\n",
    "                                       labels=[1, 3, 1])\n",
    "dialo_encoded"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor(1.7481, device='cuda:0', grad_fn=<AddBackward0>),\n tensor([[[-0.7042, -0.7361,  0.6979, -0.4645,  0.9864,  0.1224, -1.1395],\n          [-0.2058, -0.3343,  0.1103, -0.3265,  0.9724,  0.2944, -1.0671],\n          [-0.6186, -0.5042,  0.4554, -0.0337,  0.6771,  0.1185, -1.2657],\n          [-0.6287, -0.9157,  0.1138, -0.0457,  1.4048, -0.0072, -0.6282]]],\n        device='cuda:0', grad_fn=<ViewBackward0>),\n tensor([[[ 0.3196, -0.3407],\n          [ 0.2296, -0.1094],\n          [ 0.3050, -0.1324],\n          [ 1.1594,  0.1484]]], device='cuda:0', grad_fn=<ViewBackward0>))"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialo_transformer.forward(**dialo_encoded)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "{'encodings': tensor([[[-0.0222,  0.0347, -0.0330,  ...,  0.0192,  0.0224,  0.0413],\n          [-0.0181,  0.0499, -0.0205,  ...,  0.0387,  0.0148,  0.0444],\n          [-0.0369,  0.0174,  0.0452,  ...,  0.0200,  0.0160,  0.0653],\n          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n \n         [[-0.0155,  0.0169, -0.0177,  ...,  0.0393,  0.0083,  0.0618],\n          [-0.0271,  0.0169, -0.0272,  ...,  0.0270,  0.0196,  0.0157],\n          [-0.0341,  0.0635, -0.0151,  ...,  0.0327,  0.0089,  0.0714],\n          [-0.0260,  0.0257, -0.0143,  ...,  0.0143,  0.0018,  0.0442]]],\n        device='cuda:0'),\n 'interlocutors_ids': tensor([[2, 3, 2, 1, 0],\n         [2, 3, 2, 3, 1]], device='cuda:0'),\n 'attention_mask': tensor([[False, False, False, False,  True],\n         [False, False, False, False, False]], device='cuda:0'),\n 'labels': tensor([[   1,    1,    3, -100, -100],\n         [   2,    3,    1,    2, -100]], device='cuda:0')}"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialogs_encoded = dialo_tokenizer.encode_batch(dialog=[\n",
    "    ['Hello', 'Hi there', 'Goodbye'], ['My name is', 'Stop', 'Go away', 'Please']\n",
    "], labels=[\n",
    "    [1, 1, 3], [2, 3, 1, 2]\n",
    "])\n",
    "dialogs_encoded"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor(1.7404, device='cuda:0', grad_fn=<AddBackward0>),\n tensor([[[-0.7095, -0.6706,  0.3404, -0.5564,  0.8437,  0.2011, -1.1407],\n          [-0.0254, -0.2928, -0.1531, -0.5913,  0.8622,  0.2891, -1.2287],\n          [-0.5786, -0.5341, -0.0941, -0.2266,  0.7673,  0.1787, -0.9681],\n          [-0.3354, -0.7799,  0.1171, -0.2230,  1.3337,  0.1878, -0.7180],\n          [-0.6826, -0.8465,  0.1663, -0.0961,  1.1494,  0.2328, -0.8802]],\n \n         [[-0.2975, -0.8617,  0.4696, -0.1252,  0.9753,  0.5724, -1.3509],\n          [ 0.1282, -0.2586,  0.2773, -0.3133,  1.0429,  0.5871, -1.3603],\n          [-0.2562, -0.3986,  0.1804, -0.2978,  0.7214,  0.6200, -1.3813],\n          [-0.0725, -0.4585,  0.2548,  0.2197,  1.0482,  0.7793, -1.4150],\n          [-0.2234, -0.6633,  0.2036, -0.0663,  1.1244,  0.5459, -1.1681]]],\n        device='cuda:0', grad_fn=<ViewBackward0>),\n tensor([[[ 0.3441, -0.3382],\n          [ 0.2490, -0.0729],\n          [ 0.1897, -0.2438],\n          [ 0.7562, -0.3090],\n          [ 1.0418,  0.0490]],\n \n         [[ 0.3961, -0.1931],\n          [ 0.2746,  0.0289],\n          [ 0.0222, -0.0252],\n          [ 0.4636,  0.0268],\n          [ 1.4678,  0.6339]]], device='cuda:0', grad_fn=<ViewBackward0>))"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialo_transformer.forward(**dialogs_encoded)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "# Accepts list of dialog dicts per batch\n",
    "def collate_batch(batch: list):\n",
    "    v = {k: [dic[k] for dic in batch] for k in batch[0].keys()}  # list of dicts to dict of lists\n",
    "    return v"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Original column name emotion not in the dataset. Current columns in the dataset: ['dialog', 'labels']",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[47], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m dd_dataset[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mdd_dataset\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrename_column\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43memotion\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlabels\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mremove_columns([\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mact\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m      2\u001B[0m dd_dataset[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvalidation\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m dd_dataset[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvalidation\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mrename_column(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124memotion\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mremove_columns([\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mact\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m      3\u001B[0m dd_dataset[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtest\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m dd_dataset[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtest\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mrename_column(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124memotion\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mremove_columns([\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mact\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "File \u001B[0;32m~/.conda/envs/ort/lib/python3.10/site-packages/datasets/arrow_dataset.py:563\u001B[0m, in \u001B[0;36mtransmit_tasks.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    561\u001B[0m     \u001B[38;5;28mself\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mself\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    562\u001B[0m \u001B[38;5;66;03m# apply actual function\u001B[39;00m\n\u001B[0;32m--> 563\u001B[0m out: Union[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDatasetDict\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    564\u001B[0m datasets: List[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(out\u001B[38;5;241m.\u001B[39mvalues()) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(out, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m [out]\n\u001B[1;32m    565\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m dataset \u001B[38;5;129;01min\u001B[39;00m datasets:\n\u001B[1;32m    566\u001B[0m     \u001B[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001B[39;00m\n",
      "File \u001B[0;32m~/.conda/envs/ort/lib/python3.10/site-packages/datasets/fingerprint.py:511\u001B[0m, in \u001B[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    507\u001B[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001B[1;32m    509\u001B[0m \u001B[38;5;66;03m# Call actual function\u001B[39;00m\n\u001B[0;32m--> 511\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    513\u001B[0m \u001B[38;5;66;03m# Update fingerprint of in-place transforms + update in-place history of transforms\u001B[39;00m\n\u001B[1;32m    515\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m inplace:  \u001B[38;5;66;03m# update after calling func so that the fingerprint doesn't change if the function fails\u001B[39;00m\n",
      "File \u001B[0;32m~/.conda/envs/ort/lib/python3.10/site-packages/datasets/arrow_dataset.py:2061\u001B[0m, in \u001B[0;36mDataset.rename_column\u001B[0;34m(self, original_column_name, new_column_name, new_fingerprint)\u001B[0m\n\u001B[1;32m   2059\u001B[0m dataset \u001B[38;5;241m=\u001B[39m copy\u001B[38;5;241m.\u001B[39mdeepcopy(\u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m   2060\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m original_column_name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m dataset\u001B[38;5;241m.\u001B[39m_data\u001B[38;5;241m.\u001B[39mcolumn_names:\n\u001B[0;32m-> 2061\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   2062\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOriginal column name \u001B[39m\u001B[38;5;132;01m{\u001B[39;00moriginal_column_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m not in the dataset. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2063\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCurrent columns in the dataset: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdataset\u001B[38;5;241m.\u001B[39m_data\u001B[38;5;241m.\u001B[39mcolumn_names\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2064\u001B[0m     )\n\u001B[1;32m   2065\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m new_column_name \u001B[38;5;129;01min\u001B[39;00m dataset\u001B[38;5;241m.\u001B[39m_data\u001B[38;5;241m.\u001B[39mcolumn_names:\n\u001B[1;32m   2066\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   2067\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNew column name \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnew_column_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m already in the dataset. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2068\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease choose a column name which is not already in the dataset. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2069\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCurrent columns in the dataset: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdataset\u001B[38;5;241m.\u001B[39m_data\u001B[38;5;241m.\u001B[39mcolumn_names\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2070\u001B[0m     )\n",
      "\u001B[0;31mValueError\u001B[0m: Original column name emotion not in the dataset. Current columns in the dataset: ['dialog', 'labels']"
     ]
    }
   ],
   "source": [
    "dd_dataset['train'] = dd_dataset['train'].rename_column('emotion', 'labels').remove_columns(['act'])\n",
    "dd_dataset['validation'] = dd_dataset['validation'].rename_column('emotion', 'labels').remove_columns(['act'])\n",
    "dd_dataset['test'] = dd_dataset['test'].rename_column('emotion', 'labels').remove_columns(['act'])\n",
    "dd_dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dd_dataset['train'], collate_fn=collate_batch, shuffle=True, batch_size=128)\n",
    "eval_dataloader = DataLoader(dd_dataset['validation'], collate_fn=collate_batch, shuffle=True, batch_size=256)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "optimizer = AdamW(dialo_transformer.parameters(), lr=5e-5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "num_epochs = 6\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(name=\"cosine\", optimizer=optimizer, num_warmup_steps=10,\n",
    "                             num_training_steps=num_training_steps)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "def evaluate(model: DialogTransformer, tokenizer: DialogTokenizer):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for batch in tqdm(eval_dataloader):\n",
    "        with torch.inference_mode():\n",
    "            tokenized_input = tokenizer.encode_batch(**batch)\n",
    "            loss, l, i = dialo_transformer.forward(**tokenized_input)\n",
    "            del l, i\n",
    "        losses.append(loss.detach().item())\n",
    "    return losses"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cb6529fd94924982a08d567782b0c542"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[1.7392239570617676, 1.7405672073364258, 1.738313913345337, 1.7393399477005005]"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(dialo_transformer, dialo_tokenizer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "def train(model: DialogTransformer, tokenizer: DialogTokenizer):\n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        print(f'Starting epoch {epoch}...')\n",
    "        train_losses = []\n",
    "\n",
    "        for batch in train_dataloader:\n",
    "            tokenized_input = tokenizer.encode_batch(**batch)\n",
    "            loss, l, i = dialo_transformer.forward(**tokenized_input)\n",
    "            loss.backward()\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "\n",
    "            del l, i\n",
    "\n",
    "        eval_losses = evaluate(dialo_transformer, dialo_tokenizer)\n",
    "\n",
    "        print(f'Mean train loss: {np.array(train_losses).mean()}')\n",
    "        print(f'Mean eval loss: {np.array(eval_losses).mean()}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/522 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fefdcf86451b466d92397218b0c70f40"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "98ee6c6eb4d940fdad30a6b419daa56d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean train loss: 0.34102522247824174\n",
      "Mean eval loss: 0.12985403463244438\n",
      "Starting epoch 1...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f562f811a82445c3be01615409bc255c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean train loss: 0.16332982771698085\n",
      "Mean eval loss: 0.1217700932174921\n",
      "Starting epoch 2...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ecc76f79599c495fba239ada4ce2cd85"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean train loss: 0.1581036620612802\n",
      "Mean eval loss: 0.1365481335669756\n",
      "Starting epoch 3...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9e810fd912094a7abfedcc83bd3dd6a3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean train loss: 0.15589607387096033\n",
      "Mean eval loss: 0.11642501689493656\n",
      "Starting epoch 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train(dialo_transformer, dialo_tokenizer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tests"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_encoded = dialo_tokenizer.encode(['Congratulations , Vivian . You won the grand prize , again .',\n",
    "                                       \"Isn't it just great ! I just knew I'd win !\",\n",
    "                                       \"You did ? How ? Did you wear red underwear again this year ?\",\n",
    "                                       \"Not only that !\",\n",
    "                                       'Okey, its not interesting, go away from me'])\n",
    "test_encoded"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    test_output = dialo_transformer.forward(**test_encoded)\n",
    "test_output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_output[1].softmax(-1).argmax(-1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_output[0].softmax(-1).argmax(-1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
