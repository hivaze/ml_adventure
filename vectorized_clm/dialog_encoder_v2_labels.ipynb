{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-13T13:53:28.688005Z",
     "start_time": "2023-04-13T13:53:28.644736Z"
    }
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from functools import lru_cache\n",
    "from typing import List\n",
    "\n",
    "from cachetools import cached, TTLCache\n",
    "\n",
    "import datasets\n",
    "import json\n",
    "from pathlib import Path\n",
    "import evaluate\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-13T13:53:29.204925Z",
     "start_time": "2023-04-13T13:53:29.195811Z"
    }
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-13T13:53:31.181832Z",
     "start_time": "2023-04-13T13:53:31.179415Z"
    }
   },
   "outputs": [],
   "source": [
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output.last_hidden_state  #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-13T13:53:32.054530Z",
     "start_time": "2023-04-13T13:53:32.051350Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_clm_loss(logits, labels):\n",
    "    # Classical Language modeling task (nope)\n",
    "    # Next token prediction task in causual setup\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "    shift_labels = labels[..., 1:].contiguous()\n",
    "    loss = F.cross_entropy(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1), ignore_index=-100)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-13T13:53:32.637211Z",
     "start_time": "2023-04-13T13:53:32.629802Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_shifted_cosine_mse_loss(input_logits, output_logits, padding_mask):\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    batch_size = padding_mask.shape[0]\n",
    "    for batch_idx in range(batch_size):\n",
    "        preds = input_logits[batch_idx, :][~padding_mask[batch_idx, :]]\n",
    "        targets = output_logits[batch_idx, :][~padding_mask[batch_idx, :]]\n",
    "\n",
    "        shift_input = preds[:-1, :].contiguous()\n",
    "        shift_output = targets[1:, :].contiguous()\n",
    "\n",
    "        loss = (1 - F.cosine_similarity(shift_input, shift_output, -1)).mean() + F.mse_loss(shift_input, shift_output)\n",
    "        losses.append(loss)\n",
    "\n",
    "    return torch.stack(losses).contiguous().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-13T13:53:33.041672Z",
     "start_time": "2023-04-13T13:53:33.035509Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_shifted_cross_l2_loss(input_logits, output_logits, padding_mask):\n",
    "    losses = []\n",
    "\n",
    "    batch_size = padding_mask.shape[0]\n",
    "    for batch_idx in range(batch_size):\n",
    "\n",
    "        preds = input_logits[batch_idx, :][~padding_mask[batch_idx, :]]\n",
    "        targets = output_logits[batch_idx, :][~padding_mask[batch_idx, :]]\n",
    "\n",
    "        orig_distances = torch.cdist(targets[1:], targets[1:]).detach()\n",
    "        pred_distances = torch.cdist(preds[:-1], preds[:-1])\n",
    "\n",
    "        idx = torch.triu_indices(*orig_distances.shape)\n",
    "        loss = F.mse_loss(pred_distances[idx[0], idx[1]].view(-1), orig_distances[idx[0], idx[1]].view(-1))\n",
    "        losses.append(loss)\n",
    "\n",
    "    return torch.stack(losses).contiguous().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-13T13:53:34.258779Z",
     "start_time": "2023-04-13T13:53:34.249163Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_cosine_mse_loss(input_logits, output_logits, padding_mask):\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    batch_size = padding_mask.shape[0]\n",
    "    for batch_idx in range(batch_size):\n",
    "        preds = input_logits[batch_idx, :][~padding_mask[batch_idx, :]]\n",
    "        targets = output_logits[batch_idx, :][~padding_mask[batch_idx, :]]\n",
    "        # print(preds.shape, targets.shape)\n",
    "\n",
    "        loss = (1 - F.cosine_similarity(preds, targets, -1)).mean() + F.mse_loss(preds, targets)\n",
    "        losses.append(loss)\n",
    "\n",
    "    return torch.stack(losses).contiguous().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Losses Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_output = torch.rand([5, 8, 100], dtype=torch.float)\n",
    "example_input = example_output.clone()\n",
    "example_input[:, :-1] = example_output[:, 1:]\n",
    "# compute_shifted_mse_loss(example_input, example_output, torch.zeros(5, 8).bool())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.mse_loss(example_input[0][:-1], example_output[0][1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.mse_loss(torch.cdist(example_output[0][1:], example_output[0][1:]).view(-1),\n",
    "           torch.cdist(example_input[0][:-1], example_input[0][:-1]).view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cdist(example_input[0][:-1], example_input[0][:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cdist(example_output[0][1:], example_output[0][1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cdist(example_input[0][:-1], example_output[0][1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using allenai/soda dataset\n",
    "This dataset contains dialogs, speakers labels, each dialog is a separate list of lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soda_dataset = datasets.load_dataset('allenai/soda')\n",
    "soda_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soda_dataset = soda_dataset.remove_columns([col for col in soda_dataset['train'].column_names if col not in ['dialogue', 'speakers']])\n",
    "soda_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_interlocutors(row):\n",
    "    first_speaker = row['speakers'][0]\n",
    "    other_speaker = list(set(row['speakers']) - {first_speaker})[0]\n",
    "    mapping = {\n",
    "        first_speaker: 2,\n",
    "        other_speaker: 3\n",
    "    }\n",
    "    fixed_speakers = list(map(lambda name: mapping.get(name, 2), row['speakers']))\n",
    "    return {\n",
    "        'speakers': fixed_speakers\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'ek': 34, 'mf3': 54}.get(1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soda_dataset = soda_dataset.map(encode_interlocutors, num_proc=11)\n",
    "soda_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soda_dataset = soda_dataset.rename_columns({'dialogue': 'dialog', 'speakers': 'interlocutors'})\n",
    "soda_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soda_dataset = soda_dataset.filter(lambda row: len(row['dialog']) == len(row['interlocutors']) and len(row['dialog']) > 1)\n",
    "soda_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speakers_counts = [len(set(x)) for x in soda_dataset['validation']['interlocutors']]\n",
    "plt.hist(speakers_counts)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogs_lengths = [len(x) for x in soda_dataset['test']['dialog']]\n",
    "plt.hist(dialogs_lengths)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrase encoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phrase_model = 'roberta-base'\n",
    "# phrase_model = 'microsoft/deberta-v3-base'\n",
    "# phrase_model = 'sentence-transformers/all-MiniLM-L12-v2'\n",
    "# phrase_model = 'sentence-transformers/bert-base-nli-mean-tokens'\n",
    "# phrase_model = 'intfloat/e5-base'\n",
    "# phrase_model = 'cardiffnlp/twitter-xlm-roberta-base-sentiment'\n",
    "phrase_model = 'sentence-transformers/paraphrase-mpnet-base-v2'\n",
    "# phrase_model = 'sentence-transformers/sentence-t5-base'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(phrase_model)\n",
    "# model = AutoModel.from_pretrained(phrase_model).to(device)\n",
    "# model\n",
    "sent_transformer = SentenceTransformer(model_name_or_path=phrase_model, device=device).eval().half()\n",
    "sent_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_transformer.max_seq_length = 256\n",
    "sent_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_phrases = ['Some day i will go to school',\n",
    "                'To make maximum progress on addressing these pressing problems',\n",
    "                'I will nether go to school',\n",
    "                'I like to visit school',\n",
    "                'The day will come when i will go to school']\n",
    "\n",
    "phrases_encodings = sent_transformer.encode(test_phrases, convert_to_tensor=True, normalize_embeddings=False)\n",
    "phrases_encodings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(phrases_encodings.cpu(), phrases_encodings.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dialog encoder model\n",
    "\n",
    "Концептуально тут нужно:\n",
    "- модель-кодировщик фраз (замоороженная) -> готовые эмбединги текста\n",
    "- эмбединги участников диалога\n",
    "- эмбединги позиции текста в диалоге\n",
    "- кастомный токенизер c BOS и EOS\n",
    "- causual lm crossentropy loss\n",
    "- causual маска атеншена\n",
    "- финальный классификатор в условный словарь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DialogEmbeddings(nn.Module):\n",
    "    def __init__(self, encoder_hidden_dim: int,\n",
    "                 max_interlocutors_count: int,\n",
    "                 max_dialogue_length: int,\n",
    "                 dropout_p: float):\n",
    "        super(DialogEmbeddings, self).__init__()\n",
    "\n",
    "        self.padding_idx = 0  # special index for padding (must be in tokenizer)\n",
    "\n",
    "        self.position_embeddings = nn.Embedding(max_dialogue_length + 1,  # padding\n",
    "                                                encoder_hidden_dim, padding_idx=self.padding_idx)\n",
    "        self.interlocutors_embeddings = nn.Embedding(max_interlocutors_count + 2,  # padding, eos, bos\n",
    "                                                     encoder_hidden_dim, padding_idx=self.padding_idx)\n",
    "        self.norm = nn.LayerNorm(encoder_hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, interlocutors_ids: torch.LongTensor, position_ids: torch.LongTensor = None):\n",
    "        if position_ids is None:\n",
    "            position_ids = self.create_position_ids_from_input_ids(interlocutors_ids)\n",
    "\n",
    "        interlocutors_embeds = self.interlocutors_embeddings(interlocutors_ids)\n",
    "        position_embeds = self.position_embeddings(position_ids)\n",
    "\n",
    "        embeddings = interlocutors_embeds + position_embeds\n",
    "        embeddings = self.norm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    def create_position_ids_from_input_ids(self, input_ids):\n",
    "        \"\"\"\n",
    "        Replace non-padding symbols with their position numbers. Position numbers begin at padding_idx+1. Padding symbols\n",
    "        are ignored. This is modified from fairseq's `utils.make_positions`. :param torch.Tensor x: :return torch.Tensor:\n",
    "        \"\"\"\n",
    "        # The series of casts and type-conversions here are carefully balanced to both work with ONNX export and XLA.\n",
    "        mask = input_ids.ne(self.padding_idx).int()\n",
    "        incremental_indices = torch.cumsum(mask, dim=1).type_as(mask) * mask\n",
    "        return incremental_indices.long() + self.padding_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DialogOutput(nn.Module):\n",
    "    def __init__(self,\n",
    "                 encoder_hidden_dim: int,\n",
    "                 dim_feedforward_mult: int = 3,\n",
    "                 dropout_p: float = 0.1):\n",
    "        super(DialogOutput, self).__init__()\n",
    "\n",
    "        self.inner_proj = nn.Linear(encoder_hidden_dim, dim_feedforward_mult * encoder_hidden_dim)\n",
    "        self.norm = nn.LayerNorm(dim_feedforward_mult * encoder_hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.out_proj = nn.Linear(dim_feedforward_mult * encoder_hidden_dim, encoder_hidden_dim)\n",
    "        # self.norm2 = nn.LayerNorm(encoder_hidden_dim)\n",
    "\n",
    "    def forward(self, inp: torch.FloatTensor):\n",
    "        x = F.gelu(self.inner_proj(inp))\n",
    "        x = self.norm(self.dropout(x))\n",
    "        x = self.out_proj(x)\n",
    "        x = x + inp\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DialogTransformer(nn.Module):\n",
    "    def __init__(self, encoder_hidden_dim: int,\n",
    "                 max_dialogue_length: int,\n",
    "                 max_interlocutors_count: int,\n",
    "                 labels_count: int,\n",
    "                 decoder_n_layers: int = 2,\n",
    "                 decoder_n_head: int = 4,\n",
    "                 dim_feedforward_mult: int = 3,\n",
    "                 dropout_p: float = 0.1):\n",
    "        super(DialogTransformer, self).__init__()\n",
    "\n",
    "        # self.bos_vector = nn.Parameter(torch.randn([encoder_hidden_dim]), requires_grad=True)\n",
    "        self.eos_vector = nn.Parameter(torch.randn([encoder_hidden_dim]), requires_grad=True)\n",
    "\n",
    "        self.input_norm = nn.LayerNorm(encoder_hidden_dim)\n",
    "\n",
    "        self.labels_embeddings = nn.Embedding(labels_count + 2, encoder_hidden_dim)\n",
    "        self.dialogue_embeddings = DialogEmbeddings(encoder_hidden_dim, max_interlocutors_count,\n",
    "                                                    max_dialogue_length, dropout_p)\n",
    "\n",
    "        decoder_ff_inner_dim = encoder_hidden_dim * dim_feedforward_mult\n",
    "        layer = nn.TransformerEncoderLayer(d_model=encoder_hidden_dim,\n",
    "                                           nhead=decoder_n_head,\n",
    "                                           dim_feedforward=decoder_ff_inner_dim,\n",
    "                                           activation=F.gelu,  # using gelu instead of default relu\n",
    "                                           dropout=dropout_p,\n",
    "                                           batch_first=True)  # using encoder layers due to not a seq2seq setup\n",
    "        self.model = nn.TransformerEncoder(layer, decoder_n_layers)\n",
    "\n",
    "        # self.lstm_model = nn.LSTM(input_size=encoder_hidden_dim,\n",
    "        #                           hidden_size=decoder_ff_inner_dim,\n",
    "        #                           num_layers=decoder_n_layers,\n",
    "        #                           bidirectional=False,\n",
    "        #                           dropout=dropout_p,\n",
    "        #                           batch_first=True)\n",
    "\n",
    "        self.labels_projector = nn.Linear(in_features=encoder_hidden_dim, out_features=labels_count+1, bias=True)\n",
    "\n",
    "    def forward(self, encodings: torch.FloatTensor,\n",
    "                interlocutors_ids: torch.LongTensor,\n",
    "                labels_ids: torch.LongTensor,\n",
    "                position_ids: torch.LongTensor = None,\n",
    "                attention_mask: torch.BoolTensor = None,\n",
    "                return_loss = True):\n",
    "        \"\"\"\n",
    "        :param encodings: Pooled hiddens from sentence-transformer in shape [bs, lines_count, hidden_dim]\n",
    "        :param labels: Labels for dialog lines\n",
    "        :param interlocutors_ids: shape [bs, seq_len], interlocutors for each line (from one)\n",
    "        :param position_ids: shape [bs, seq_len], position of line in dialogue (from one)\n",
    "        :param attention_mask: shape [bs, seq_len], attention mask for padding where 1 is disabled and 0 is enabled\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = encodings.shape[0]\n",
    "\n",
    "        representation = torch.cat([\n",
    "            # self.bos_vector.repeat([batch_size, 1, 1]),\n",
    "            encodings,\n",
    "            self.eos_vector.repeat([batch_size, 1, 1])\n",
    "        ], dim=1)  # insert bos and eos vector\n",
    "\n",
    "        x = self.input_norm(representation)\n",
    "        x = x + self.labels_embeddings(labels_ids)\n",
    "        x = x + self.dialogue_embeddings(interlocutors_ids=interlocutors_ids, position_ids=position_ids)\n",
    "\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.zeros([batch_size, x.shape[1]]).bool().to(x.device)\n",
    "\n",
    "        causal_mask = torch.triu(torch.ones(x.shape[1], x.shape[1]), diagonal=1).bool().to(x.device)  # only attend to past (not necessary, but logical...)\n",
    "        x = self.model.forward(src=x,\n",
    "                               mask=causal_mask,\n",
    "                               src_key_padding_mask=attention_mask)\n",
    "\n",
    "        # x, (ht, ct) = self.lstm_model(x)\n",
    "\n",
    "        predicted_labels = self.labels_projector(x)\n",
    "\n",
    "        if return_loss:\n",
    "            copied_labels = labels_ids.clone() - 1\n",
    "            copied_labels[copied_labels < 0] = -100\n",
    "\n",
    "            labels_loss = compute_clm_loss(predicted_labels, copied_labels)\n",
    "\n",
    "            return labels_loss, predicted_labels\n",
    "\n",
    "        return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.triu(torch.ones(10, 10), diagonal=1).bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialo_transformer = DialogTransformer(encoder_hidden_dim=768,\n",
    "                                      max_dialogue_length=50,\n",
    "                                      max_interlocutors_count=2,\n",
    "                                      decoder_n_layers=3,\n",
    "                                      decoder_n_head=4,\n",
    "                                      dim_feedforward_mult=4,\n",
    "                                      dropout_p=0.15\n",
    "                                      ).to(sent_transformer.device).eval()\n",
    "dialo_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(dialo_transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dialog Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPTJForCausalLM\n",
    "\n",
    "GPTJForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DialogTokenizer:\n",
    "    \"\"\"\n",
    "    Accepts dicts with keys: 'dialog' - required, 'interlocutors' and 'labels'\n",
    "    Must return dict with 'encoder_hidden', 'interlocutors_ids' and 'labels'\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lines_encoder: SentenceTransformer,\n",
    "                 all_labels: list = None,\n",
    "                 all_interlocutors: list = None):\n",
    "\n",
    "        self.lines_encoder = lines_encoder\n",
    "        for p in self.lines_encoder[0].parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        self.padding_idx = 0\n",
    "        # self.bos_idx = 1\n",
    "        self.eos_idx = 1\n",
    "\n",
    "        if all_interlocutors:\n",
    "            all_interlocutors = set(all_interlocutors)\n",
    "            self.id2interlocutors = dict(zip(range(2, len(all_interlocutors) + 2), all_interlocutors))\n",
    "            self.interlocutor2id = {v: k for k, v in self.id2interlocutors.items()}\n",
    "\n",
    "        if all_labels:\n",
    "            all_labels = set(all_labels)\n",
    "            self.id2label = dict(zip(range(2, len(all_labels) + 2), all_labels))\n",
    "            self.label2id = {v: k for k, v in self.id2label.items()}\n",
    "\n",
    "    def encode(self, dialog: List[str],\n",
    "               labels: list,\n",
    "               interlocutors: List[int] = None,\n",
    "               lines_batch_size: int = 50,\n",
    "               unsqueeze: bool = True,\n",
    "               return_loss: bool = True,\n",
    "               device: str = None,\n",
    "               **kwargs):\n",
    "\n",
    "        encodings = self.lines_encoder.encode(sentences=dialog,\n",
    "                                              batch_size=lines_batch_size,\n",
    "                                              normalize_embeddings=False,  # better not to normalize\n",
    "                                              show_progress_bar=False,\n",
    "                                              convert_to_tensor=True)\n",
    "        # encodings = encodings.cpu()\n",
    "        # encodings.requires_grad = False\n",
    "\n",
    "        if interlocutors is None:\n",
    "            interlocutors = [(i % 2) + 2 for i in range(len(dialog))]\n",
    "        elif hasattr(self, 'interlocutor2id'):\n",
    "            interlocutors = list(map(self.interlocutor2id.get, interlocutors))\n",
    "\n",
    "        if hasattr(self, 'label2id'):\n",
    "            labels = list(map(self.label2id.get, labels))\n",
    "\n",
    "        labels = labels + [self.eos_idx]\n",
    "        interlocutors = interlocutors + [self.eos_idx]\n",
    "\n",
    "        save_device = device if device is not None else encodings.device\n",
    "\n",
    "        result = {\n",
    "            'encodings': encodings.unsqueeze(0).to(save_device) if unsqueeze else encodings.to(save_device),\n",
    "            'labels_ids': torch.LongTensor([labels] if unsqueeze else labels).to(save_device),\n",
    "            'interlocutors_ids': torch.LongTensor([interlocutors] if unsqueeze else interlocutors).to(save_device),\n",
    "            'return_loss': return_loss\n",
    "        }\n",
    "\n",
    "        return result\n",
    "\n",
    "    def encode_batch(self, dialog: List[List[str]],\n",
    "                     labels: List[list],\n",
    "                     interlocutors: List[List[int]] = None,\n",
    "                     lines_batch_size: int = 50,\n",
    "                     return_loss: bool = True,\n",
    "                     **kwargs):\n",
    "\n",
    "        if interlocutors is None:\n",
    "            interlocutors = [None] * len(dialog)\n",
    "        unsqueeze = [False] * len(dialog)\n",
    "\n",
    "        assert len(dialog) == len(interlocutors)\n",
    "\n",
    "        zipped = zip(dialog, labels, interlocutors, [lines_batch_size] * len(dialog), unsqueeze)\n",
    "        encoded_batch = list(map(lambda x: self.encode(*x), zipped))\n",
    "\n",
    "        encodings = pad_sequence([encode_dict['encodings'] for encode_dict in encoded_batch],\n",
    "                                 batch_first=True,\n",
    "                                 padding_value=self.padding_idx)\n",
    "        encodings.requires_grad = False\n",
    "\n",
    "        labels_ids = pad_sequence([encode_dict['labels_ids'] for encode_dict in encoded_batch],\n",
    "                                         batch_first=True,\n",
    "                                         padding_value=self.padding_idx)\n",
    "\n",
    "        interlocutors_ids = pad_sequence([encode_dict['interlocutors_ids'] for encode_dict in encoded_batch],\n",
    "                                         batch_first=True,\n",
    "                                         padding_value=self.padding_idx)\n",
    "\n",
    "        lengths = [len(dial) + 1 for dial in dialog]  # keep in mind bos and eos\n",
    "        masks = list(map(lambda x: torch.zeros(size=[x]), lengths))\n",
    "        attention_masks = pad_sequence(masks, batch_first=True, padding_value=1).bool()\n",
    "\n",
    "        result = {\n",
    "            'encodings': encodings.to(self.lines_encoder.device),\n",
    "            'labels_ids': labels_ids.to(self.lines_encoder.device),\n",
    "            'interlocutors_ids': interlocutors_ids.to(self.lines_encoder.device),\n",
    "            'attention_mask': attention_masks.to(self.lines_encoder.device),\n",
    "            'return_loss': return_loss\n",
    "        }\n",
    "\n",
    "        return result\n",
    "\n",
    "    def encode_cached_batch(self,\n",
    "                            dialog: List[List[str]],\n",
    "                            encodings: List[List[float]],\n",
    "                            interlocutors_ids: List[List[int]],\n",
    "                            return_loss: List[bool],\n",
    "                            **kwargs):\n",
    "\n",
    "        encodings = pad_sequence(list(map(torch.FloatTensor, encodings)),\n",
    "                                 batch_first=True,\n",
    "                                 padding_value=self.padding_idx)\n",
    "        encodings.requires_grad = False\n",
    "\n",
    "        interlocutors_ids = pad_sequence(list(map(torch.LongTensor, interlocutors_ids)),\n",
    "                                         batch_first=True,\n",
    "                                         padding_value=self.padding_idx)\n",
    "\n",
    "        lengths = [len(dial) + 1 for dial in dialog]  # keep in mind bos and eos\n",
    "        masks = list(map(lambda x: torch.zeros(size=[x]), lengths))\n",
    "        attention_masks = pad_sequence(masks, batch_first=True, padding_value=1).bool()\n",
    "\n",
    "        result = {\n",
    "            'encodings': encodings.to(self.lines_encoder.device),\n",
    "            'interlocutors_ids': interlocutors_ids.to(self.lines_encoder.device),\n",
    "            'attention_mask': attention_masks.to(self.lines_encoder.device),\n",
    "            'return_loss': return_loss\n",
    "        }\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialo_tokenizer = DialogTokenizer(sent_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialo_encoded = dialo_tokenizer.encode(['Hello man', 'Goodbye', 'Thanks'])\n",
    "dialo_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialo_transformer.forward(**dialo_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogs_encoded = dialo_tokenizer.encode_batch(**soda_dataset['train'][:3])\n",
    "dialogs_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialo_transformer.forward(**dialogs_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretokenize all dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "soda_dataset = soda_dataset.map(lambda row: dialo_tokenizer.encode(**row, unsqueeze=False, device='cpu'), num_proc=1)\n",
    "soda_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accepts list of dialog dicts per batch\n",
    "def collate_batch(batch: list):\n",
    "    v = {k: [dic[k] for dic in batch] for k in batch[0].keys()}  # list of dicts to dict of lists\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(soda_dataset['train'],\n",
    "                              collate_fn=collate_batch,\n",
    "                              shuffle=True, batch_size=256)\n",
    "eval_dataloader = DataLoader(soda_dataset['validation'],\n",
    "                             collate_fn=collate_batch,\n",
    "                             shuffle=True, batch_size=512)\n",
    "test_dataloader = DataLoader(soda_dataset['test'],\n",
    "                             collate_fn=collate_batch,\n",
    "                             shuffle=True, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(dialo_transformer.parameters(), lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(name=\"cosine_with_restarts\", optimizer=optimizer, num_warmup_steps=10,\n",
    "                             num_training_steps=num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: DialogTransformer, tokenizer: DialogTokenizer, data_loader):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for batch in tqdm(data_loader):\n",
    "        with torch.inference_mode():\n",
    "            tokenized_input = tokenizer.encode_cached_batch(**batch)\n",
    "            loss, i1, i2, i3 = dialo_transformer.forward(**tokenized_input)\n",
    "            del i1, i2, i3\n",
    "        losses.append(loss.detach().item())\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(evaluate(dialo_transformer, dialo_tokenizer, eval_dataloader)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_experiment_info(best_epoch_n, best_train_loss, best_eval_loss, losses_history = None, file_name='experiment_info.json'):\n",
    "    result_dict = {\n",
    "        'best_epoch': {\n",
    "            'number': best_epoch_n,\n",
    "            'train_loss': best_train_loss,\n",
    "            'eval_loss': best_eval_loss\n",
    "        }\n",
    "    }\n",
    "    if losses_history is not None:\n",
    "        result_dict['history'] = losses_history\n",
    "    with open(file_name, 'w') as outfile:\n",
    "        json.dump(result_dict, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: DialogTransformer, tokenizer: DialogTokenizer, checkpoints_dir):\n",
    "    Path(checkpoints_dir).mkdir(parents=True, exist_ok=True)\n",
    "    losses_history = {\n",
    "        'train': [],\n",
    "        'eval': []\n",
    "    }\n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "    min_eval_loss = 999999999.9\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        print(f'Starting epoch {epoch}...')\n",
    "        train_losses = []\n",
    "\n",
    "        for batch in train_dataloader:\n",
    "            tokenized_input = tokenizer.encode_cached_batch(**batch)\n",
    "            loss, i1, i2, i3 = dialo_transformer.forward(**tokenized_input)\n",
    "            loss.backward()\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "\n",
    "            del i2, i3\n",
    "\n",
    "        train_loss = np.array(train_losses).mean()\n",
    "        eval_loss = np.array(evaluate(dialo_transformer, dialo_tokenizer, eval_dataloader)).mean()\n",
    "\n",
    "        losses_history['train'].append(train_loss)\n",
    "        losses_history['eval'].append(eval_loss)\n",
    "\n",
    "        print(f'[TRAIN] Mean epoch loss: {train_loss}')\n",
    "        print(f'[EVAL] Mean epoch loss: {eval_loss}')\n",
    "\n",
    "        if eval_loss < min_eval_loss:\n",
    "            save_path = checkpoints_dir + 'best_model.pth'\n",
    "            print(f'Current best on eval, saving model to {save_path}...')\n",
    "            torch.save(model, save_path)\n",
    "            create_experiment_info(best_epoch_n=epoch, best_train_loss=train_loss, best_eval_loss=eval_loss,\n",
    "                                   losses_history=losses_history, file_name=checkpoints_dir + 'experiment_info.json')\n",
    "            min_eval_loss = eval_loss\n",
    "\n",
    "    return losses_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_history = train(dialo_transformer, dialo_tokenizer, './experiments/soda_pmpn_eos_shift_tr_3l4h_COS&MSE_d0.15_21M/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialo_transformer = torch.load('./experiments/pmpn_1l_4h_COS&MSE_d0.1/best_model.pth').eval()\n",
    "dialo_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(dialo_transformer, dialo_tokenizer, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_dataset['train']['dialog'][14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encoded = dialo_tokenizer.encode(dd_dataset['train']['dialog'][14])\n",
    "test_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    test_output = dialo_transformer.forward(**test_encoded)\n",
    "test_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.mse_loss(test_encoded['encodings'][0][1:].detach(),\n",
    "           test_encoded['encodings'][0][:-1].detach(), reduction='none').mean(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.mse_loss(test_output[1][0].detach(),\n",
    "           test_encoded['encodings'][0].detach(), reduction='none').mean(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cdist(test_encoded['encodings'][0].detach(), test_encoded['encodings'][0].detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(test_encoded['encodings'][0].cpu(), test_encoded['encodings'][0].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cdist(test_output[2][0], test_output[2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(test_output[2][0].cpu(), test_output[2][0].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cdist(test_output[2][0], test_encoded['encodings'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(test_output[2][0][:-1].cpu(), test_encoded['encodings'][0].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cdist(test_output[1][0][1:], test_encoded['encodings'][0][:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.mse_loss(test_output[1][0][1:].detach(),\n",
    "           test_output[1][0][:-1].detach(), reduction='none').mean(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output[3].softmax(-1).argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
