{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan\r\n",
      "Filesystem                                                                Size  Used Avail Use% Mounted on\r\n",
      "pd11-nfs.sr002.aicloud.sbercloud.tech:/vol_fg1/namespace_ai0001102-00004  200G   60G  141G  30% /home/jovyan\r\n"
     ]
    }
   ],
   "source": [
    "!pwd & df -h ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: hf_clm_training.py [-h] [--model_name_or_path MODEL_NAME_OR_PATH]\r\n",
      "                          [--model_type MODEL_TYPE]\r\n",
      "                          [--config_overrides CONFIG_OVERRIDES]\r\n",
      "                          [--config_name CONFIG_NAME]\r\n",
      "                          [--tokenizer_name TOKENIZER_NAME]\r\n",
      "                          [--cache_dir CACHE_DIR]\r\n",
      "                          [--use_fast_tokenizer [USE_FAST_TOKENIZER]]\r\n",
      "                          [--no_use_fast_tokenizer]\r\n",
      "                          [--model_revision MODEL_REVISION]\r\n",
      "                          [--use_auth_token [USE_AUTH_TOKEN]]\r\n",
      "                          [--dataset_name DATASET_NAME]\r\n",
      "                          [--dataset_config_name DATASET_CONFIG_NAME]\r\n",
      "                          [--train_file TRAIN_FILE]\r\n",
      "                          [--validation_file VALIDATION_FILE]\r\n",
      "                          [--max_train_samples MAX_TRAIN_SAMPLES]\r\n",
      "                          [--max_eval_samples MAX_EVAL_SAMPLES]\r\n",
      "                          [--block_size BLOCK_SIZE]\r\n",
      "                          [--overwrite_cache [OVERWRITE_CACHE]]\r\n",
      "                          [--validation_split_percentage VALIDATION_SPLIT_PERCENTAGE]\r\n",
      "                          [--preprocessing_num_workers PREPROCESSING_NUM_WORKERS]\r\n",
      "                          [--keep_linebreaks [KEEP_LINEBREAKS]]\r\n",
      "                          [--no_keep_linebreaks] --output_dir OUTPUT_DIR\r\n",
      "                          [--overwrite_output_dir [OVERWRITE_OUTPUT_DIR]]\r\n",
      "                          [--do_train [DO_TRAIN]] [--do_eval [DO_EVAL]]\r\n",
      "                          [--do_predict [DO_PREDICT]]\r\n",
      "                          [--evaluation_strategy {no,steps,epoch}]\r\n",
      "                          [--prediction_loss_only [PREDICTION_LOSS_ONLY]]\r\n",
      "                          [--per_device_train_batch_size PER_DEVICE_TRAIN_BATCH_SIZE]\r\n",
      "                          [--per_device_eval_batch_size PER_DEVICE_EVAL_BATCH_SIZE]\r\n",
      "                          [--per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE]\r\n",
      "                          [--per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE]\r\n",
      "                          [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\r\n",
      "                          [--eval_accumulation_steps EVAL_ACCUMULATION_STEPS]\r\n",
      "                          [--eval_delay EVAL_DELAY]\r\n",
      "                          [--learning_rate LEARNING_RATE]\r\n",
      "                          [--weight_decay WEIGHT_DECAY]\r\n",
      "                          [--adam_beta1 ADAM_BETA1] [--adam_beta2 ADAM_BETA2]\r\n",
      "                          [--adam_epsilon ADAM_EPSILON]\r\n",
      "                          [--max_grad_norm MAX_GRAD_NORM]\r\n",
      "                          [--num_train_epochs NUM_TRAIN_EPOCHS]\r\n",
      "                          [--max_steps MAX_STEPS]\r\n",
      "                          [--lr_scheduler_type {linear,cosine,cosine_with_restarts,polynomial,constant,constant_with_warmup}]\r\n",
      "                          [--warmup_ratio WARMUP_RATIO]\r\n",
      "                          [--warmup_steps WARMUP_STEPS]\r\n",
      "                          [--log_level {debug,info,warning,error,critical,passive}]\r\n",
      "                          [--log_level_replica {debug,info,warning,error,critical,passive}]\r\n",
      "                          [--log_on_each_node [LOG_ON_EACH_NODE]]\r\n",
      "                          [--no_log_on_each_node] [--logging_dir LOGGING_DIR]\r\n",
      "                          [--logging_strategy {no,steps,epoch}]\r\n",
      "                          [--logging_first_step [LOGGING_FIRST_STEP]]\r\n",
      "                          [--logging_steps LOGGING_STEPS]\r\n",
      "                          [--logging_nan_inf_filter [LOGGING_NAN_INF_FILTER]]\r\n",
      "                          [--no_logging_nan_inf_filter]\r\n",
      "                          [--save_strategy {no,steps,epoch}]\r\n",
      "                          [--save_steps SAVE_STEPS]\r\n",
      "                          [--save_total_limit SAVE_TOTAL_LIMIT]\r\n",
      "                          [--save_on_each_node [SAVE_ON_EACH_NODE]]\r\n",
      "                          [--no_cuda [NO_CUDA]]\r\n",
      "                          [--use_mps_device [USE_MPS_DEVICE]] [--seed SEED]\r\n",
      "                          [--data_seed DATA_SEED]\r\n",
      "                          [--jit_mode_eval [JIT_MODE_EVAL]]\r\n",
      "                          [--use_ipex [USE_IPEX]] [--bf16 [BF16]]\r\n",
      "                          [--fp16 [FP16]] [--fp16_opt_level FP16_OPT_LEVEL]\r\n",
      "                          [--half_precision_backend {auto,cuda_amp,apex,cpu_amp}]\r\n",
      "                          [--bf16_full_eval [BF16_FULL_EVAL]]\r\n",
      "                          [--fp16_full_eval [FP16_FULL_EVAL]] [--tf32 TF32]\r\n",
      "                          [--local_rank LOCAL_RANK] [--xpu_backend {mpi,ccl}]\r\n",
      "                          [--tpu_num_cores TPU_NUM_CORES]\r\n",
      "                          [--tpu_metrics_debug [TPU_METRICS_DEBUG]]\r\n",
      "                          [--debug DEBUG]\r\n",
      "                          [--dataloader_drop_last [DATALOADER_DROP_LAST]]\r\n",
      "                          [--eval_steps EVAL_STEPS]\r\n",
      "                          [--dataloader_num_workers DATALOADER_NUM_WORKERS]\r\n",
      "                          [--past_index PAST_INDEX] [--run_name RUN_NAME]\r\n",
      "                          [--disable_tqdm DISABLE_TQDM]\r\n",
      "                          [--remove_unused_columns [REMOVE_UNUSED_COLUMNS]]\r\n",
      "                          [--no_remove_unused_columns]\r\n",
      "                          [--label_names LABEL_NAMES [LABEL_NAMES ...]]\r\n",
      "                          [--load_best_model_at_end [LOAD_BEST_MODEL_AT_END]]\r\n",
      "                          [--metric_for_best_model METRIC_FOR_BEST_MODEL]\r\n",
      "                          [--greater_is_better GREATER_IS_BETTER]\r\n",
      "                          [--ignore_data_skip [IGNORE_DATA_SKIP]]\r\n",
      "                          [--sharded_ddp SHARDED_DDP] [--fsdp FSDP]\r\n",
      "                          [--fsdp_min_num_params FSDP_MIN_NUM_PARAMS]\r\n",
      "                          [--fsdp_transformer_layer_cls_to_wrap FSDP_TRANSFORMER_LAYER_CLS_TO_WRAP]\r\n",
      "                          [--deepspeed DEEPSPEED]\r\n",
      "                          [--label_smoothing_factor LABEL_SMOOTHING_FACTOR]\r\n",
      "                          [--optim {adamw_hf,adamw_torch,adamw_torch_xla,adamw_apex_fused,adafactor,adamw_bnb_8bit,sgd,adagrad}]\r\n",
      "                          [--adafactor [ADAFACTOR]]\r\n",
      "                          [--group_by_length [GROUP_BY_LENGTH]]\r\n",
      "                          [--length_column_name LENGTH_COLUMN_NAME]\r\n",
      "                          [--report_to REPORT_TO [REPORT_TO ...]]\r\n",
      "                          [--ddp_find_unused_parameters DDP_FIND_UNUSED_PARAMETERS]\r\n",
      "                          [--ddp_bucket_cap_mb DDP_BUCKET_CAP_MB]\r\n",
      "                          [--dataloader_pin_memory [DATALOADER_PIN_MEMORY]]\r\n",
      "                          [--no_dataloader_pin_memory]\r\n",
      "                          [--skip_memory_metrics [SKIP_MEMORY_METRICS]]\r\n",
      "                          [--no_skip_memory_metrics]\r\n",
      "                          [--use_legacy_prediction_loop [USE_LEGACY_PREDICTION_LOOP]]\r\n",
      "                          [--push_to_hub [PUSH_TO_HUB]]\r\n",
      "                          [--resume_from_checkpoint RESUME_FROM_CHECKPOINT]\r\n",
      "                          [--hub_model_id HUB_MODEL_ID]\r\n",
      "                          [--hub_strategy {end,every_save,checkpoint,all_checkpoints}]\r\n",
      "                          [--hub_token HUB_TOKEN]\r\n",
      "                          [--hub_private_repo [HUB_PRIVATE_REPO]]\r\n",
      "                          [--gradient_checkpointing [GRADIENT_CHECKPOINTING]]\r\n",
      "                          [--include_inputs_for_metrics [INCLUDE_INPUTS_FOR_METRICS]]\r\n",
      "                          [--fp16_backend {auto,cuda_amp,apex,cpu_amp}]\r\n",
      "                          [--push_to_hub_model_id PUSH_TO_HUB_MODEL_ID]\r\n",
      "                          [--push_to_hub_organization PUSH_TO_HUB_ORGANIZATION]\r\n",
      "                          [--push_to_hub_token PUSH_TO_HUB_TOKEN]\r\n",
      "                          [--mp_parameters MP_PARAMETERS]\r\n",
      "                          [--auto_find_batch_size [AUTO_FIND_BATCH_SIZE]]\r\n",
      "                          [--full_determinism [FULL_DETERMINISM]]\r\n",
      "                          [--torchdynamo {eager,nvfuser,fx2trt,fx2trt-fp16}]\r\n",
      "                          [--ray_scope RAY_SCOPE] [--ddp_timeout DDP_TIMEOUT]\r\n",
      "\r\n",
      "optional arguments:\r\n",
      "  -h, --help            show this help message and exit\r\n",
      "  --model_name_or_path MODEL_NAME_OR_PATH\r\n",
      "                        The model checkpoint for weights initialization.Don't\r\n",
      "                        set if you want to train a model from scratch.\r\n",
      "                        (default: None)\r\n",
      "  --model_type MODEL_TYPE\r\n",
      "                        If training from scratch, pass a model type from the\r\n",
      "                        list: bart, bert, bert-generation, big_bird,\r\n",
      "                        bigbird_pegasus, blenderbot, blenderbot-small, bloom,\r\n",
      "                        camembert, codegen, ctrl, data2vec-text, electra,\r\n",
      "                        ernie, gpt2, gpt_neo, gpt_neox, gpt_neox_japanese,\r\n",
      "                        gptj, marian, mbart, megatron-bert, mvp, openai-gpt,\r\n",
      "                        opt, pegasus, plbart, prophetnet, qdqbert, reformer,\r\n",
      "                        rembert, roberta, roformer, speech_to_text_2, transfo-\r\n",
      "                        xl, trocr, xglm, xlm, xlm-prophetnet, xlm-roberta,\r\n",
      "                        xlm-roberta-xl, xlnet (default: None)\r\n",
      "  --config_overrides CONFIG_OVERRIDES\r\n",
      "                        Override some existing default config settings when a\r\n",
      "                        model is trained from scratch. Example: n_embd=10,resi\r\n",
      "                        d_pdrop=0.2,scale_attn_weights=false,summary_type=cls_\r\n",
      "                        index (default: None)\r\n",
      "  --config_name CONFIG_NAME\r\n",
      "                        Pretrained config name or path if not the same as\r\n",
      "                        model_name (default: None)\r\n",
      "  --tokenizer_name TOKENIZER_NAME\r\n",
      "                        Pretrained tokenizer name or path if not the same as\r\n",
      "                        model_name (default: None)\r\n",
      "  --cache_dir CACHE_DIR\r\n",
      "                        Where do you want to store the pretrained models\r\n",
      "                        downloaded from huggingface.co (default: None)\r\n",
      "  --use_fast_tokenizer [USE_FAST_TOKENIZER]\r\n",
      "                        Whether to use one of the fast tokenizer (backed by\r\n",
      "                        the tokenizers library) or not. (default: True)\r\n",
      "  --no_use_fast_tokenizer\r\n",
      "                        Whether to use one of the fast tokenizer (backed by\r\n",
      "                        the tokenizers library) or not. (default: False)\r\n",
      "  --model_revision MODEL_REVISION\r\n",
      "                        The specific model version to use (can be a branch\r\n",
      "                        name, tag name or commit id). (default: main)\r\n",
      "  --use_auth_token [USE_AUTH_TOKEN]\r\n",
      "                        Will use the token generated when running\r\n",
      "                        `huggingface-cli login` (necessary to use this script\r\n",
      "                        with private models). (default: False)\r\n",
      "  --dataset_name DATASET_NAME\r\n",
      "                        The name of the dataset to use (via the datasets\r\n",
      "                        library). (default: None)\r\n",
      "  --dataset_config_name DATASET_CONFIG_NAME\r\n",
      "                        The configuration name of the dataset to use (via the\r\n",
      "                        datasets library). (default: None)\r\n",
      "  --train_file TRAIN_FILE\r\n",
      "                        The input training data file (a text file). (default:\r\n",
      "                        None)\r\n",
      "  --validation_file VALIDATION_FILE\r\n",
      "                        An optional input evaluation data file to evaluate the\r\n",
      "                        perplexity on (a text file). (default: None)\r\n",
      "  --max_train_samples MAX_TRAIN_SAMPLES\r\n",
      "                        For debugging purposes or quicker training, truncate\r\n",
      "                        the number of training examples to this value if set.\r\n",
      "                        (default: None)\r\n",
      "  --max_eval_samples MAX_EVAL_SAMPLES\r\n",
      "                        For debugging purposes or quicker training, truncate\r\n",
      "                        the number of evaluation examples to this value if\r\n",
      "                        set. (default: None)\r\n",
      "  --block_size BLOCK_SIZE\r\n",
      "                        Optional input sequence length after tokenization. The\r\n",
      "                        training dataset will be truncated in block of this\r\n",
      "                        size for training. Default to the model max input\r\n",
      "                        length for single sentence inputs (take into account\r\n",
      "                        special tokens). (default: None)\r\n",
      "  --overwrite_cache [OVERWRITE_CACHE]\r\n",
      "                        Overwrite the cached training and evaluation sets\r\n",
      "                        (default: False)\r\n",
      "  --validation_split_percentage VALIDATION_SPLIT_PERCENTAGE\r\n",
      "                        The percentage of the train set used as validation set\r\n",
      "                        in case there's no validation split (default: 5)\r\n",
      "  --preprocessing_num_workers PREPROCESSING_NUM_WORKERS\r\n",
      "                        The number of processes to use for the preprocessing.\r\n",
      "                        (default: None)\r\n",
      "  --keep_linebreaks [KEEP_LINEBREAKS]\r\n",
      "                        Whether to keep line breaks when using TXT files or\r\n",
      "                        not. (default: True)\r\n",
      "  --no_keep_linebreaks  Whether to keep line breaks when using TXT files or\r\n",
      "                        not. (default: False)\r\n",
      "  --output_dir OUTPUT_DIR\r\n",
      "                        The output directory where the model predictions and\r\n",
      "                        checkpoints will be written. (default: None)\r\n",
      "  --overwrite_output_dir [OVERWRITE_OUTPUT_DIR]\r\n",
      "                        Overwrite the content of the output directory. Use\r\n",
      "                        this to continue training if output_dir points to a\r\n",
      "                        checkpoint directory. (default: False)\r\n",
      "  --do_train [DO_TRAIN]\r\n",
      "                        Whether to run training. (default: False)\r\n",
      "  --do_eval [DO_EVAL]   Whether to run eval on the dev set. (default: False)\r\n",
      "  --do_predict [DO_PREDICT]\r\n",
      "                        Whether to run predictions on the test set. (default:\r\n",
      "                        False)\r\n",
      "  --evaluation_strategy {no,steps,epoch}\r\n",
      "                        The evaluation strategy to use. (default: no)\r\n",
      "  --prediction_loss_only [PREDICTION_LOSS_ONLY]\r\n",
      "                        When performing evaluation and predictions, only\r\n",
      "                        returns the loss. (default: False)\r\n",
      "  --per_device_train_batch_size PER_DEVICE_TRAIN_BATCH_SIZE\r\n",
      "                        Batch size per GPU/TPU core/CPU for training.\r\n",
      "                        (default: 8)\r\n",
      "  --per_device_eval_batch_size PER_DEVICE_EVAL_BATCH_SIZE\r\n",
      "                        Batch size per GPU/TPU core/CPU for evaluation.\r\n",
      "                        (default: 8)\r\n",
      "  --per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE\r\n",
      "                        Deprecated, the use of `--per_device_train_batch_size`\r\n",
      "                        is preferred. Batch size per GPU/TPU core/CPU for\r\n",
      "                        training. (default: None)\r\n",
      "  --per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE\r\n",
      "                        Deprecated, the use of `--per_device_eval_batch_size`\r\n",
      "                        is preferred. Batch size per GPU/TPU core/CPU for\r\n",
      "                        evaluation. (default: None)\r\n",
      "  --gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS\r\n",
      "                        Number of updates steps to accumulate before\r\n",
      "                        performing a backward/update pass. (default: 1)\r\n",
      "  --eval_accumulation_steps EVAL_ACCUMULATION_STEPS\r\n",
      "                        Number of predictions steps to accumulate before\r\n",
      "                        moving the tensors to the CPU. (default: None)\r\n",
      "  --eval_delay EVAL_DELAY\r\n",
      "                        Number of epochs or steps to wait for before the first\r\n",
      "                        evaluation can be performed, depending on the\r\n",
      "                        evaluation_strategy. (default: 0)\r\n",
      "  --learning_rate LEARNING_RATE\r\n",
      "                        The initial learning rate for AdamW. (default: 5e-05)\r\n",
      "  --weight_decay WEIGHT_DECAY\r\n",
      "                        Weight decay for AdamW if we apply some. (default:\r\n",
      "                        0.0)\r\n",
      "  --adam_beta1 ADAM_BETA1\r\n",
      "                        Beta1 for AdamW optimizer (default: 0.9)\r\n",
      "  --adam_beta2 ADAM_BETA2\r\n",
      "                        Beta2 for AdamW optimizer (default: 0.999)\r\n",
      "  --adam_epsilon ADAM_EPSILON\r\n",
      "                        Epsilon for AdamW optimizer. (default: 1e-08)\r\n",
      "  --max_grad_norm MAX_GRAD_NORM\r\n",
      "                        Max gradient norm. (default: 1.0)\r\n",
      "  --num_train_epochs NUM_TRAIN_EPOCHS\r\n",
      "                        Total number of training epochs to perform. (default:\r\n",
      "                        3.0)\r\n",
      "  --max_steps MAX_STEPS\r\n",
      "                        If > 0: set total number of training steps to perform.\r\n",
      "                        Override num_train_epochs. (default: -1)\r\n",
      "  --lr_scheduler_type {linear,cosine,cosine_with_restarts,polynomial,constant,constant_with_warmup}\r\n",
      "                        The scheduler type to use. (default: linear)\r\n",
      "  --warmup_ratio WARMUP_RATIO\r\n",
      "                        Linear warmup over warmup_ratio fraction of total\r\n",
      "                        steps. (default: 0.0)\r\n",
      "  --warmup_steps WARMUP_STEPS\r\n",
      "                        Linear warmup over warmup_steps. (default: 0)\r\n",
      "  --log_level {debug,info,warning,error,critical,passive}\r\n",
      "                        Logger log level to use on the main node. Possible\r\n",
      "                        choices are the log levels as strings: 'debug',\r\n",
      "                        'info', 'warning', 'error' and 'critical', plus a\r\n",
      "                        'passive' level which doesn't set anything and lets\r\n",
      "                        the application set the level. Defaults to 'passive'.\r\n",
      "                        (default: passive)\r\n",
      "  --log_level_replica {debug,info,warning,error,critical,passive}\r\n",
      "                        Logger log level to use on replica nodes. Same choices\r\n",
      "                        and defaults as ``log_level`` (default: passive)\r\n",
      "  --log_on_each_node [LOG_ON_EACH_NODE]\r\n",
      "                        When doing a multinode distributed training, whether\r\n",
      "                        to log once per node or just once on the main node.\r\n",
      "                        (default: True)\r\n",
      "  --no_log_on_each_node\r\n",
      "                        When doing a multinode distributed training, whether\r\n",
      "                        to log once per node or just once on the main node.\r\n",
      "                        (default: False)\r\n",
      "  --logging_dir LOGGING_DIR\r\n",
      "                        Tensorboard log dir. (default: None)\r\n",
      "  --logging_strategy {no,steps,epoch}\r\n",
      "                        The logging strategy to use. (default: steps)\r\n",
      "  --logging_first_step [LOGGING_FIRST_STEP]\r\n",
      "                        Log the first global_step (default: False)\r\n",
      "  --logging_steps LOGGING_STEPS\r\n",
      "                        Log every X updates steps. (default: 500)\r\n",
      "  --logging_nan_inf_filter [LOGGING_NAN_INF_FILTER]\r\n",
      "                        Filter nan and inf losses for logging. (default: True)\r\n",
      "  --no_logging_nan_inf_filter\r\n",
      "                        Filter nan and inf losses for logging. (default:\r\n",
      "                        False)\r\n",
      "  --save_strategy {no,steps,epoch}\r\n",
      "                        The checkpoint save strategy to use. (default: steps)\r\n",
      "  --save_steps SAVE_STEPS\r\n",
      "                        Save checkpoint every X updates steps. (default: 500)\r\n",
      "  --save_total_limit SAVE_TOTAL_LIMIT\r\n",
      "                        Limit the total amount of checkpoints. Deletes the\r\n",
      "                        older checkpoints in the output_dir. Default is\r\n",
      "                        unlimited checkpoints (default: None)\r\n",
      "  --save_on_each_node [SAVE_ON_EACH_NODE]\r\n",
      "                        When doing multi-node distributed training, whether to\r\n",
      "                        save models and checkpoints on each node, or only on\r\n",
      "                        the main one (default: False)\r\n",
      "  --no_cuda [NO_CUDA]   Do not use CUDA even when it is available (default:\r\n",
      "                        False)\r\n",
      "  --use_mps_device [USE_MPS_DEVICE]\r\n",
      "                        Whether to use Apple Silicon chip based `mps` device.\r\n",
      "                        (default: False)\r\n",
      "  --seed SEED           Random seed that will be set at the beginning of\r\n",
      "                        training. (default: 42)\r\n",
      "  --data_seed DATA_SEED\r\n",
      "                        Random seed to be used with data samplers. (default:\r\n",
      "                        None)\r\n",
      "  --jit_mode_eval [JIT_MODE_EVAL]\r\n",
      "                        Whether or not to use PyTorch jit trace for inference\r\n",
      "                        (default: False)\r\n",
      "  --use_ipex [USE_IPEX]\r\n",
      "                        Use Intel extension for PyTorch when it is available,\r\n",
      "                        installation: 'https://github.com/intel/intel-\r\n",
      "                        extension-for-pytorch' (default: False)\r\n",
      "  --bf16 [BF16]         Whether to use bf16 (mixed) precision instead of\r\n",
      "                        32-bit. Requires Ampere or higher NVIDIA architecture\r\n",
      "                        or using CPU (no_cuda). This is an experimental API\r\n",
      "                        and it may change. (default: False)\r\n",
      "  --fp16 [FP16]         Whether to use fp16 (mixed) precision instead of\r\n",
      "                        32-bit (default: False)\r\n",
      "  --fp16_opt_level FP16_OPT_LEVEL\r\n",
      "                        For fp16: Apex AMP optimization level selected in\r\n",
      "                        ['O0', 'O1', 'O2', and 'O3']. See details at\r\n",
      "                        https://nvidia.github.io/apex/amp.html (default: O1)\r\n",
      "  --half_precision_backend {auto,cuda_amp,apex,cpu_amp}\r\n",
      "                        The backend to be used for half precision. (default:\r\n",
      "                        auto)\r\n",
      "  --bf16_full_eval [BF16_FULL_EVAL]\r\n",
      "                        Whether to use full bfloat16 evaluation instead of\r\n",
      "                        32-bit. This is an experimental API and it may change.\r\n",
      "                        (default: False)\r\n",
      "  --fp16_full_eval [FP16_FULL_EVAL]\r\n",
      "                        Whether to use full float16 evaluation instead of\r\n",
      "                        32-bit (default: False)\r\n",
      "  --tf32 TF32           Whether to enable tf32 mode, available in Ampere and\r\n",
      "                        newer GPU architectures. This is an experimental API\r\n",
      "                        and it may change. (default: None)\r\n",
      "  --local_rank LOCAL_RANK\r\n",
      "                        For distributed training: local_rank (default: -1)\r\n",
      "  --xpu_backend {mpi,ccl}\r\n",
      "                        The backend to be used for distributed training on\r\n",
      "                        Intel XPU. (default: None)\r\n",
      "  --tpu_num_cores TPU_NUM_CORES\r\n",
      "                        TPU: Number of TPU cores (automatically passed by\r\n",
      "                        launcher script) (default: None)\r\n",
      "  --tpu_metrics_debug [TPU_METRICS_DEBUG]\r\n",
      "                        Deprecated, the use of `--debug tpu_metrics_debug` is\r\n",
      "                        preferred. TPU: Whether to print debug metrics\r\n",
      "                        (default: False)\r\n",
      "  --debug DEBUG         Whether or not to enable debug mode. Current options:\r\n",
      "                        `underflow_overflow` (Detect underflow and overflow in\r\n",
      "                        activations and weights), `tpu_metrics_debug` (print\r\n",
      "                        debug metrics on TPU). (default: )\r\n",
      "  --dataloader_drop_last [DATALOADER_DROP_LAST]\r\n",
      "                        Drop the last incomplete batch if it is not divisible\r\n",
      "                        by the batch size. (default: False)\r\n",
      "  --eval_steps EVAL_STEPS\r\n",
      "                        Run an evaluation every X steps. (default: None)\r\n",
      "  --dataloader_num_workers DATALOADER_NUM_WORKERS\r\n",
      "                        Number of subprocesses to use for data loading\r\n",
      "                        (PyTorch only). 0 means that the data will be loaded\r\n",
      "                        in the main process. (default: 0)\r\n",
      "  --past_index PAST_INDEX\r\n",
      "                        If >=0, uses the corresponding part of the output as\r\n",
      "                        the past state for next step. (default: -1)\r\n",
      "  --run_name RUN_NAME   An optional descriptor for the run. Notably used for\r\n",
      "                        wandb logging. (default: None)\r\n",
      "  --disable_tqdm DISABLE_TQDM\r\n",
      "                        Whether or not to disable the tqdm progress bars.\r\n",
      "                        (default: None)\r\n",
      "  --remove_unused_columns [REMOVE_UNUSED_COLUMNS]\r\n",
      "                        Remove columns not required by the model when using an\r\n",
      "                        nlp.Dataset. (default: True)\r\n",
      "  --no_remove_unused_columns\r\n",
      "                        Remove columns not required by the model when using an\r\n",
      "                        nlp.Dataset. (default: False)\r\n",
      "  --label_names LABEL_NAMES [LABEL_NAMES ...]\r\n",
      "                        The list of keys in your dictionary of inputs that\r\n",
      "                        correspond to the labels. (default: None)\r\n",
      "  --load_best_model_at_end [LOAD_BEST_MODEL_AT_END]\r\n",
      "                        Whether or not to load the best model found during\r\n",
      "                        training at the end of training. (default: False)\r\n",
      "  --metric_for_best_model METRIC_FOR_BEST_MODEL\r\n",
      "                        The metric to use to compare two different models.\r\n",
      "                        (default: None)\r\n",
      "  --greater_is_better GREATER_IS_BETTER\r\n",
      "                        Whether the `metric_for_best_model` should be\r\n",
      "                        maximized or not. (default: None)\r\n",
      "  --ignore_data_skip [IGNORE_DATA_SKIP]\r\n",
      "                        When resuming training, whether or not to skip the\r\n",
      "                        first epochs and batches to get to the same training\r\n",
      "                        data. (default: False)\r\n",
      "  --sharded_ddp SHARDED_DDP\r\n",
      "                        Whether or not to use sharded DDP training (in\r\n",
      "                        distributed training only). The base option should be\r\n",
      "                        `simple`, `zero_dp_2` or `zero_dp_3` and you can add\r\n",
      "                        CPU-offload to `zero_dp_2` or `zero_dp_3` like this:\r\n",
      "                        zero_dp_2 offload` or `zero_dp_3 offload`. You can add\r\n",
      "                        auto-wrap to `zero_dp_2` or `zero_dp_3` with the same\r\n",
      "                        syntax: zero_dp_2 auto_wrap` or `zero_dp_3 auto_wrap`.\r\n",
      "                        (default: )\r\n",
      "  --fsdp FSDP           Whether or not to use PyTorch Fully Sharded Data\r\n",
      "                        Parallel (FSDP) training (in distributed training\r\n",
      "                        only). The base option should be `full_shard`,\r\n",
      "                        `shard_grad_op` or `no_shard` and you can add CPU-\r\n",
      "                        offload to `full_shard` or `shard_grad_op` like this:\r\n",
      "                        full_shard offload` or `shard_grad_op offload`. You\r\n",
      "                        can add auto-wrap to `full_shard` or `shard_grad_op`\r\n",
      "                        with the same syntax: full_shard auto_wrap` or\r\n",
      "                        `shard_grad_op auto_wrap`. (default: )\r\n",
      "  --fsdp_min_num_params FSDP_MIN_NUM_PARAMS\r\n",
      "                        FSDP's minimum number of parameters for Default Auto\r\n",
      "                        Wrapping. (useful only when `fsdp` field is passed).\r\n",
      "                        (default: 0)\r\n",
      "  --fsdp_transformer_layer_cls_to_wrap FSDP_TRANSFORMER_LAYER_CLS_TO_WRAP\r\n",
      "                        Transformer layer class name (case-sensitive) to wrap\r\n",
      "                        ,e.g, `BertLayer`, `GPTJBlock`, `T5Block` .... (useful\r\n",
      "                        only when `fsdp` flag is passed). (default: None)\r\n",
      "  --deepspeed DEEPSPEED\r\n",
      "                        Enable deepspeed and pass the path to deepspeed json\r\n",
      "                        config file (e.g. ds_config.json) or an already loaded\r\n",
      "                        json file as a dict (default: None)\r\n",
      "  --label_smoothing_factor LABEL_SMOOTHING_FACTOR\r\n",
      "                        The label smoothing epsilon to apply (zero means no\r\n",
      "                        label smoothing). (default: 0.0)\r\n",
      "  --optim {adamw_hf,adamw_torch,adamw_torch_xla,adamw_apex_fused,adafactor,adamw_bnb_8bit,sgd,adagrad}\r\n",
      "                        The optimizer to use. (default: adamw_hf)\r\n",
      "  --adafactor [ADAFACTOR]\r\n",
      "                        Whether or not to replace AdamW by Adafactor.\r\n",
      "                        (default: False)\r\n",
      "  --group_by_length [GROUP_BY_LENGTH]\r\n",
      "                        Whether or not to group samples of roughly the same\r\n",
      "                        length together when batching. (default: False)\r\n",
      "  --length_column_name LENGTH_COLUMN_NAME\r\n",
      "                        Column name with precomputed lengths to use when\r\n",
      "                        grouping by length. (default: length)\r\n",
      "  --report_to REPORT_TO [REPORT_TO ...]\r\n",
      "                        The list of integrations to report the results and\r\n",
      "                        logs to. (default: None)\r\n",
      "  --ddp_find_unused_parameters DDP_FIND_UNUSED_PARAMETERS\r\n",
      "                        When using distributed training, the value of the flag\r\n",
      "                        `find_unused_parameters` passed to\r\n",
      "                        `DistributedDataParallel`. (default: None)\r\n",
      "  --ddp_bucket_cap_mb DDP_BUCKET_CAP_MB\r\n",
      "                        When using distributed training, the value of the flag\r\n",
      "                        `bucket_cap_mb` passed to `DistributedDataParallel`.\r\n",
      "                        (default: None)\r\n",
      "  --dataloader_pin_memory [DATALOADER_PIN_MEMORY]\r\n",
      "                        Whether or not to pin memory for DataLoader. (default:\r\n",
      "                        True)\r\n",
      "  --no_dataloader_pin_memory\r\n",
      "                        Whether or not to pin memory for DataLoader. (default:\r\n",
      "                        False)\r\n",
      "  --skip_memory_metrics [SKIP_MEMORY_METRICS]\r\n",
      "                        Whether or not to skip adding of memory profiler\r\n",
      "                        reports to metrics. (default: True)\r\n",
      "  --no_skip_memory_metrics\r\n",
      "                        Whether or not to skip adding of memory profiler\r\n",
      "                        reports to metrics. (default: False)\r\n",
      "  --use_legacy_prediction_loop [USE_LEGACY_PREDICTION_LOOP]\r\n",
      "                        Whether or not to use the legacy prediction_loop in\r\n",
      "                        the Trainer. (default: False)\r\n",
      "  --push_to_hub [PUSH_TO_HUB]\r\n",
      "                        Whether or not to upload the trained model to the\r\n",
      "                        model hub after training. (default: False)\r\n",
      "  --resume_from_checkpoint RESUME_FROM_CHECKPOINT\r\n",
      "                        The path to a folder with a valid checkpoint for your\r\n",
      "                        model. (default: None)\r\n",
      "  --hub_model_id HUB_MODEL_ID\r\n",
      "                        The name of the repository to keep in sync with the\r\n",
      "                        local `output_dir`. (default: None)\r\n",
      "  --hub_strategy {end,every_save,checkpoint,all_checkpoints}\r\n",
      "                        The hub strategy to use when `--push_to_hub` is\r\n",
      "                        activated. (default: every_save)\r\n",
      "  --hub_token HUB_TOKEN\r\n",
      "                        The token to use to push to the Model Hub. (default:\r\n",
      "                        None)\r\n",
      "  --hub_private_repo [HUB_PRIVATE_REPO]\r\n",
      "                        Whether the model repository is private or not.\r\n",
      "                        (default: False)\r\n",
      "  --gradient_checkpointing [GRADIENT_CHECKPOINTING]\r\n",
      "                        If True, use gradient checkpointing to save memory at\r\n",
      "                        the expense of slower backward pass. (default: False)\r\n",
      "  --include_inputs_for_metrics [INCLUDE_INPUTS_FOR_METRICS]\r\n",
      "                        Whether or not the inputs will be passed to the\r\n",
      "                        `compute_metrics` function. (default: False)\r\n",
      "  --fp16_backend {auto,cuda_amp,apex,cpu_amp}\r\n",
      "                        Deprecated. Use half_precision_backend instead\r\n",
      "                        (default: auto)\r\n",
      "  --push_to_hub_model_id PUSH_TO_HUB_MODEL_ID\r\n",
      "                        The name of the repository to which push the\r\n",
      "                        `Trainer`. (default: None)\r\n",
      "  --push_to_hub_organization PUSH_TO_HUB_ORGANIZATION\r\n",
      "                        The name of the organization in with to which push the\r\n",
      "                        `Trainer`. (default: None)\r\n",
      "  --push_to_hub_token PUSH_TO_HUB_TOKEN\r\n",
      "                        The token to use to push to the Model Hub. (default:\r\n",
      "                        None)\r\n",
      "  --mp_parameters MP_PARAMETERS\r\n",
      "                        Used by the SageMaker launcher to send mp-specific\r\n",
      "                        args. Ignored in Trainer (default: )\r\n",
      "  --auto_find_batch_size [AUTO_FIND_BATCH_SIZE]\r\n",
      "                        Whether to automatically decrease the batch size in\r\n",
      "                        half and rerun the training loop again each time a\r\n",
      "                        CUDA Out-of-Memory was reached (default: False)\r\n",
      "  --full_determinism [FULL_DETERMINISM]\r\n",
      "                        Whether to call enable_full_determinism instead of\r\n",
      "                        set_seed for reproducibility in distributed training\r\n",
      "                        (default: False)\r\n",
      "  --torchdynamo {eager,nvfuser,fx2trt,fx2trt-fp16}\r\n",
      "                        Sets up the backend compiler for TorchDynamo.\r\n",
      "                        TorchDynamo is a Python level JIT compiler designed to\r\n",
      "                        make unmodified PyTorch programs faster. TorchDynamo\r\n",
      "                        dynamically modifies the Python bytecode right before\r\n",
      "                        its executed. It rewrites Python bytecode to extract\r\n",
      "                        sequences of PyTorch operations and lifts them up into\r\n",
      "                        Fx graph. We can then pass these Fx graphs to other\r\n",
      "                        backend compilers. There are two options - eager and\r\n",
      "                        nvfuser. Eager defaults to pytorch eager and is useful\r\n",
      "                        for debugging. nvfuser path uses AOT Autograd and\r\n",
      "                        nvfuser compiler to optimize the models. (default:\r\n",
      "                        None)\r\n",
      "  --ray_scope RAY_SCOPE\r\n",
      "                        The scope to use when doing hyperparameter search with\r\n",
      "                        Ray. By default, `\"last\"` will be used. Ray will then\r\n",
      "                        use the last checkpoint of all trials, compare those,\r\n",
      "                        and select the best one. However, other options are\r\n",
      "                        also available. See the Ray documentation (https://doc\r\n",
      "                        s.ray.io/en/latest/tune/api_docs/analysis.html#ray.tun\r\n",
      "                        e.ExperimentAnalysis.get_best_trial) for more options.\r\n",
      "                        (default: last)\r\n",
      "  --ddp_timeout DDP_TIMEOUT\r\n",
      "                        Overrides the default timeout for distributed training\r\n",
      "                        (value should be given in seconds). (default: 1800)\r\n"
     ]
    }
   ],
   "source": [
    "!python hf_clm_training.py -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/15/2022 00:30:26 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "10/15/2022 00:30:26 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=4,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=epoch,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1.5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=passive,\n",
      "log_level_replica=passive,\n",
      "log_on_each_node=True,\n",
      "logging_dir=models/cont_large_4_256/runs/Oct15_00-30-25_torch-a100-0,\n",
      "logging_first_step=True,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=100,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=4.0,\n",
      "optim=adamw_hf,\n",
      "output_dir=models/cont_large_4_256,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=12,\n",
      "per_device_train_batch_size=6,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['mlflow', 'tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=models/cont_large_4_256,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=epoch,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "10/15/2022 00:30:26 - WARNING - datasets.builder - Using custom data configuration default-780495daeb0b5a55\n",
      "10/15/2022 00:30:26 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
      "10/15/2022 00:30:26 - INFO - datasets.info - Loading Dataset info from /home/jovyan/.cache/huggingface/datasets/csv/default-780495daeb0b5a55/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a\n",
      "10/15/2022 00:30:26 - WARNING - datasets.builder - Found cached dataset csv (/home/jovyan/.cache/huggingface/datasets/csv/default-780495daeb0b5a55/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n",
      "10/15/2022 00:30:26 - INFO - datasets.info - Loading Dataset info from /home/jovyan/.cache/huggingface/datasets/csv/default-780495daeb0b5a55/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 620.18it/s]\n",
      "10/15/2022 00:30:27 - WARNING - datasets.builder - Using custom data configuration default-780495daeb0b5a55\n",
      "10/15/2022 00:30:27 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
      "10/15/2022 00:30:27 - INFO - datasets.info - Loading Dataset info from /home/jovyan/.cache/huggingface/datasets/csv/default-780495daeb0b5a55/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a\n",
      "10/15/2022 00:30:27 - WARNING - datasets.builder - Found cached dataset csv (/home/jovyan/.cache/huggingface/datasets/csv/default-780495daeb0b5a55/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n",
      "10/15/2022 00:30:27 - INFO - datasets.info - Loading Dataset info from /home/jovyan/.cache/huggingface/datasets/csv/default-780495daeb0b5a55/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a\n",
      "10/15/2022 00:30:28 - WARNING - datasets.builder - Using custom data configuration default-780495daeb0b5a55\n",
      "10/15/2022 00:30:28 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
      "10/15/2022 00:30:28 - INFO - datasets.info - Loading Dataset info from /home/jovyan/.cache/huggingface/datasets/csv/default-780495daeb0b5a55/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a\n",
      "10/15/2022 00:30:28 - WARNING - datasets.builder - Found cached dataset csv (/home/jovyan/.cache/huggingface/datasets/csv/default-780495daeb0b5a55/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n",
      "10/15/2022 00:30:28 - INFO - datasets.info - Loading Dataset info from /home/jovyan/.cache/huggingface/datasets/csv/default-780495daeb0b5a55/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a\n",
      "[INFO|configuration_utils.py:653] 2022-10-15 00:30:28,609 >> loading configuration file config.json from cache at /home/jovyan/.cache/huggingface/hub/models--sberbank-ai--rugpt3large_based_on_gpt2/snapshots/aa2b602c1939938541eed9283347d6e08536f6f8/config.json\n",
      "[INFO|configuration_utils.py:705] 2022-10-15 00:30:28,611 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"sberbank-ai/rugpt3large_based_on_gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 2048,\n",
      "  \"n_embd\": 1536,\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"n_positions\": 2048,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:418] 2022-10-15 00:30:29,165 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:653] 2022-10-15 00:30:29,759 >> loading configuration file config.json from cache at /home/jovyan/.cache/huggingface/hub/models--sberbank-ai--rugpt3large_based_on_gpt2/snapshots/aa2b602c1939938541eed9283347d6e08536f6f8/config.json\n",
      "[INFO|configuration_utils.py:705] 2022-10-15 00:30:29,761 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"sberbank-ai/rugpt3large_based_on_gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 2048,\n",
      "  \"n_embd\": 1536,\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"n_positions\": 2048,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1773] 2022-10-15 00:30:30,883 >> loading file vocab.json from cache at /home/jovyan/.cache/huggingface/hub/models--sberbank-ai--rugpt3large_based_on_gpt2/snapshots/aa2b602c1939938541eed9283347d6e08536f6f8/vocab.json\n",
      "[INFO|tokenization_utils_base.py:1773] 2022-10-15 00:30:30,883 >> loading file merges.txt from cache at /home/jovyan/.cache/huggingface/hub/models--sberbank-ai--rugpt3large_based_on_gpt2/snapshots/aa2b602c1939938541eed9283347d6e08536f6f8/merges.txt\n",
      "[INFO|tokenization_utils_base.py:1773] 2022-10-15 00:30:30,883 >> loading file tokenizer.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1773] 2022-10-15 00:30:30,883 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1773] 2022-10-15 00:30:30,883 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1773] 2022-10-15 00:30:30,883 >> loading file tokenizer_config.json from cache at None\n",
      "[INFO|configuration_utils.py:653] 2022-10-15 00:30:30,885 >> loading configuration file config.json from cache at /home/jovyan/.cache/huggingface/hub/models--sberbank-ai--rugpt3large_based_on_gpt2/snapshots/aa2b602c1939938541eed9283347d6e08536f6f8/config.json\n",
      "[INFO|configuration_utils.py:705] 2022-10-15 00:30:30,885 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"sberbank-ai/rugpt3large_based_on_gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 2048,\n",
      "  \"n_embd\": 1536,\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"n_positions\": 2048,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:653] 2022-10-15 00:30:30,961 >> loading configuration file config.json from cache at /home/jovyan/.cache/huggingface/hub/models--sberbank-ai--rugpt3large_based_on_gpt2/snapshots/aa2b602c1939938541eed9283347d6e08536f6f8/config.json\n",
      "[INFO|configuration_utils.py:705] 2022-10-15 00:30:30,962 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"sberbank-ai/rugpt3large_based_on_gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 2048,\n",
      "  \"n_embd\": 1536,\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"n_positions\": 2048,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[WARNING|logging.py:281] 2022-10-15 00:30:31,019 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|modeling_utils.py:2156] 2022-10-15 00:30:31,041 >> loading weights file pytorch_model.bin from cache at /home/jovyan/.cache/huggingface/hub/models--sberbank-ai--rugpt3large_based_on_gpt2/snapshots/aa2b602c1939938541eed9283347d6e08536f6f8/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:2606] 2022-10-15 00:30:40,248 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:2615] 2022-10-15 00:30:40,248 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at sberbank-ai/rugpt3large_based_on_gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "Running tokenizer on dataset #0:   0%|                    | 0/4 [00:00<?, ?ba/s]\n",
      "Running tokenizer on dataset #0:  25%|███         | 1/4 [00:00<00:00,  5.71ba/s]\u001b[A\n",
      "\n",
      "Running tokenizer on dataset #2:   0%|                    | 0/4 [00:00<?, ?ba/s]\u001b[A\u001b[A\n",
      "Running tokenizer on dataset #1:  25%|███         | 1/4 [00:00<00:00,  6.02ba/s]\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on dataset #0: 100%|████████████| 4/4 [00:00<00:00, 13.00ba/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Running tokenizer on dataset #1: 100%|████████████| 4/4 [00:00<00:00, 13.65ba/s]\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on dataset #2:  25%|███         | 1/4 [00:00<00:00,  5.56ba/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on dataset #3:  25%|███         | 1/4 [00:00<00:00,  6.16ba/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Running tokenizer on dataset #2:  75%|█████████   | 3/4 [00:00<00:00, 11.10ba/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on dataset #2: 100%|████████████| 4/4 [00:00<00:00, 12.89ba/s]\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on dataset #3: 100%|████████████| 4/4 [00:00<00:00, 13.35ba/s]\n",
      "Running tokenizer on dataset #0: 100%|████████████| 1/1 [00:00<00:00, 29.17ba/s]\n",
      "\n",
      "Running tokenizer on dataset #1: 100%|████████████| 1/1 [00:00<00:00, 41.44ba/s]\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on dataset #2: 100%|████████████| 1/1 [00:00<00:00, 42.07ba/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on dataset #3: 100%|████████████| 1/1 [00:00<00:00, 51.22ba/s]\u001b[A\u001b[A\u001b[A\n",
      "Grouping texts in chunks of 256 #0:   0%|                 | 0/4 [00:00<?, ?ba/s]\n",
      "\n",
      "Grouping texts in chunks of 256 #2:   0%|                 | 0/4 [00:00<?, ?ba/s]\u001b[A\u001b[A\n",
      "Grouping texts in chunks of 256 #1:   0%|                 | 0/4 [00:00<?, ?ba/s]\u001b[A\n",
      "\n",
      "\n",
      "Grouping texts in chunks of 256 #0: 100%|█████████| 4/4 [00:00<00:00, 66.16ba/s]\u001b[A\u001b[A\u001b[A\n",
      "Grouping texts in chunks of 256 #1: 100%|█████████| 4/4 [00:00<00:00, 66.96ba/s]\n",
      "Grouping texts in chunks of 256 #2: 100%|█████████| 4/4 [00:00<00:00, 63.36ba/s]\n",
      "Grouping texts in chunks of 256 #3: 100%|█████████| 4/4 [00:00<00:00, 58.94ba/s]\n",
      "Grouping texts in chunks of 256 #0:   0%|                 | 0/1 [00:00<?, ?ba/s]\n",
      "Grouping texts in chunks of 256 #0: 100%|█████████| 1/1 [00:00<00:00, 80.90ba/s]\u001b[A\n",
      "Grouping texts in chunks of 256 #1: 100%|█████████| 1/1 [00:00<00:00, 78.69ba/s]\n",
      "\n",
      "\n",
      "Grouping texts in chunks of 256 #2:   0%|                 | 0/1 [00:00<?, ?ba/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Grouping texts in chunks of 256 #3: 100%|█████████| 1/1 [00:00<00:00, 92.60ba/s]\u001b[A\u001b[A\u001b[A\n",
      "Grouping texts in chunks of 256 #2: 100%|█████████| 1/1 [00:00<00:00, 83.87ba/s]\n",
      "/home/user/conda/lib/python3.7/site-packages/torch/utils/tensorboard/__init__.py:3: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if not hasattr(tensorboard, '__version__') or LooseVersion(tensorboard.__version__) < LooseVersion('1.15'):\n",
      "/home/user/conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:572: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  (np.object, string),\n",
      "/home/user/conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:573: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  (np.bool, bool),\n",
      "/home/user/conda/lib/python3.7/site-packages/tensorboard/util/tensor_util.py:113: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  np.object: SlowAppendObjectArrayToTensorProto,\n",
      "/home/user/conda/lib/python3.7/site-packages/tensorboard/util/tensor_util.py:114: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  np.bool: SlowAppendBoolArrayToTensorProto,\n",
      "/home/jovyan/.imgenv-torch-a100-0/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "[INFO|trainer.py:1607] 2022-10-15 00:30:49,972 >> ***** Running training *****\n",
      "[INFO|trainer.py:1608] 2022-10-15 00:30:49,972 >>   Num examples = 960\n",
      "[INFO|trainer.py:1609] 2022-10-15 00:30:49,972 >>   Num Epochs = 4\n",
      "[INFO|trainer.py:1610] 2022-10-15 00:30:49,972 >>   Instantaneous batch size per device = 6\n",
      "[INFO|trainer.py:1611] 2022-10-15 00:30:49,972 >>   Total train batch size (w. parallel, distributed & accumulation) = 6\n",
      "[INFO|trainer.py:1612] 2022-10-15 00:30:49,972 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1613] 2022-10-15 00:30:49,972 >>   Total optimization steps = 640\n",
      "{'loss': 3.196, 'learning_rate': 1.49765625e-05, 'epoch': 0.01}                 \n",
      "{'loss': 3.3292, 'learning_rate': 1.2656250000000001e-05, 'epoch': 0.62}        \n",
      " 25%|██████████▎                              | 160/640 [00:36<01:47,  4.47it/s][INFO|trainer.py:2907] 2022-10-15 00:31:27,447 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2909] 2022-10-15 00:31:27,447 >>   Num examples = 60\n",
      "[INFO|trainer.py:2912] 2022-10-15 00:31:27,447 >>   Batch size = 12\n",
      "\n",
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 40%|██████████████████                           | 2/5 [00:00<00:00, 19.29it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 3.6057395935058594, 'eval_accuracy': 0.3562091503267974, 'eval_runtime': 0.7758, 'eval_samples_per_second': 77.335, 'eval_steps_per_second': 6.445, 'epoch': 1.0}\n",
      " 25%|██████████▎                              | 160/640 [00:37<01:47,  4.47it/s]\n",
      "100%|█████████████████████████████████████████████| 5/5 [00:00<00:00, 12.13it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2656] 2022-10-15 00:31:28,244 >> Saving model checkpoint to models/cont_large_4_256/checkpoint-160\n",
      "[INFO|configuration_utils.py:447] 2022-10-15 00:31:28,249 >> Configuration saved in models/cont_large_4_256/checkpoint-160/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-10-15 00:31:38,814 >> Model weights saved in models/cont_large_4_256/checkpoint-160/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2123] 2022-10-15 00:31:38,820 >> tokenizer config file saved in models/cont_large_4_256/checkpoint-160/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2130] 2022-10-15 00:31:38,822 >> Special tokens file saved in models/cont_large_4_256/checkpoint-160/special_tokens_map.json\n",
      "{'loss': 3.2624, 'learning_rate': 1.03125e-05, 'epoch': 1.25}                   \n",
      "{'loss': 3.1319, 'learning_rate': 7.96875e-06, 'epoch': 1.88}                   \n",
      " 50%|████████████████████▌                    | 320/640 [01:43<01:11,  4.51it/s][INFO|trainer.py:2907] 2022-10-15 00:32:34,542 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2909] 2022-10-15 00:32:34,542 >>   Num examples = 60\n",
      "[INFO|trainer.py:2912] 2022-10-15 00:32:34,542 >>   Batch size = 12\n",
      "\n",
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 40%|██████████████████                           | 2/5 [00:00<00:00, 19.25it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 3.622950792312622, 'eval_accuracy': 0.354640522875817, 'eval_runtime': 0.7639, 'eval_samples_per_second': 78.549, 'eval_steps_per_second': 6.546, 'epoch': 2.0}\n",
      " 50%|████████████████████▌                    | 320/640 [01:44<01:11,  4.51it/s]\n",
      "100%|█████████████████████████████████████████████| 5/5 [00:00<00:00, 12.07it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2656] 2022-10-15 00:32:35,335 >> Saving model checkpoint to models/cont_large_4_256/checkpoint-320\n",
      "[INFO|configuration_utils.py:447] 2022-10-15 00:32:35,339 >> Configuration saved in models/cont_large_4_256/checkpoint-320/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-10-15 00:32:45,441 >> Model weights saved in models/cont_large_4_256/checkpoint-320/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2123] 2022-10-15 00:32:45,443 >> tokenizer config file saved in models/cont_large_4_256/checkpoint-320/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2130] 2022-10-15 00:32:45,444 >> Special tokens file saved in models/cont_large_4_256/checkpoint-320/special_tokens_map.json\n",
      "{'loss': 3.0583, 'learning_rate': 5.625e-06, 'epoch': 2.5}                      \n",
      " 75%|██████████████████████████████▊          | 480/640 [02:50<00:36,  4.44it/s][INFO|trainer.py:2907] 2022-10-15 00:33:41,658 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2909] 2022-10-15 00:33:41,658 >>   Num examples = 60\n",
      "[INFO|trainer.py:2912] 2022-10-15 00:33:41,658 >>   Batch size = 12\n",
      "\n",
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 40%|██████████████████                           | 2/5 [00:00<00:00, 19.17it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 3.6421091556549072, 'eval_accuracy': 0.3565359477124183, 'eval_runtime': 0.755, 'eval_samples_per_second': 79.468, 'eval_steps_per_second': 6.622, 'epoch': 3.0}\n",
      " 75%|██████████████████████████████▊          | 480/640 [02:51<00:36,  4.44it/s]\n",
      "100%|█████████████████████████████████████████████| 5/5 [00:00<00:00, 12.08it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2656] 2022-10-15 00:33:42,433 >> Saving model checkpoint to models/cont_large_4_256/checkpoint-480\n",
      "[INFO|configuration_utils.py:447] 2022-10-15 00:33:42,436 >> Configuration saved in models/cont_large_4_256/checkpoint-480/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-10-15 00:33:52,573 >> Model weights saved in models/cont_large_4_256/checkpoint-480/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2123] 2022-10-15 00:33:52,576 >> tokenizer config file saved in models/cont_large_4_256/checkpoint-480/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2130] 2022-10-15 00:33:52,577 >> Special tokens file saved in models/cont_large_4_256/checkpoint-480/special_tokens_map.json\n",
      "{'loss': 3.0308, 'learning_rate': 3.28125e-06, 'epoch': 3.12}                   \n",
      "{'loss': 2.9745, 'learning_rate': 9.375e-07, 'epoch': 3.75}                     \n",
      "100%|█████████████████████████████████████████| 640/640 [03:58<00:00,  4.44it/s][INFO|trainer.py:2907] 2022-10-15 00:34:49,367 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2909] 2022-10-15 00:34:49,367 >>   Num examples = 60\n",
      "[INFO|trainer.py:2912] 2022-10-15 00:34:49,367 >>   Batch size = 12\n",
      "\n",
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 40%|██████████████████                           | 2/5 [00:00<00:00, 19.24it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 3.6513829231262207, 'eval_accuracy': 0.3573202614379085, 'eval_runtime': 0.7565, 'eval_samples_per_second': 79.312, 'eval_steps_per_second': 6.609, 'epoch': 4.0}\n",
      "100%|█████████████████████████████████████████| 640/640 [03:59<00:00,  4.44it/s]\n",
      "100%|█████████████████████████████████████████████| 5/5 [00:00<00:00, 12.04it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2656] 2022-10-15 00:34:50,142 >> Saving model checkpoint to models/cont_large_4_256/checkpoint-640\n",
      "[INFO|configuration_utils.py:447] 2022-10-15 00:34:50,146 >> Configuration saved in models/cont_large_4_256/checkpoint-640/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-10-15 00:35:00,197 >> Model weights saved in models/cont_large_4_256/checkpoint-640/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2123] 2022-10-15 00:35:00,201 >> tokenizer config file saved in models/cont_large_4_256/checkpoint-640/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2130] 2022-10-15 00:35:00,202 >> Special tokens file saved in models/cont_large_4_256/checkpoint-640/special_tokens_map.json\n",
      "[INFO|trainer.py:1852] 2022-10-15 00:35:20,420 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 270.448, 'train_samples_per_second': 14.199, 'train_steps_per_second': 2.366, 'train_loss': 3.1238669071346523, 'epoch': 4.0}\n",
      "100%|█████████████████████████████████████████| 640/640 [04:29<00:00,  2.37it/s]\n",
      "[INFO|trainer.py:2656] 2022-10-15 00:35:20,459 >> Saving model checkpoint to models/cont_large_4_256\n",
      "[INFO|configuration_utils.py:447] 2022-10-15 00:35:20,463 >> Configuration saved in models/cont_large_4_256/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-10-15 00:35:28,715 >> Model weights saved in models/cont_large_4_256/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2123] 2022-10-15 00:35:28,719 >> tokenizer config file saved in models/cont_large_4_256/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2130] 2022-10-15 00:35:28,721 >> Special tokens file saved in models/cont_large_4_256/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        4.0\n",
      "  train_loss               =     3.1239\n",
      "  train_runtime            = 0:04:30.44\n",
      "  train_samples            =        960\n",
      "  train_samples_per_second =     14.199\n",
      "  train_steps_per_second   =      2.366\n",
      "10/15/2022 00:35:28 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:2907] 2022-10-15 00:35:28,855 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2909] 2022-10-15 00:35:28,855 >>   Num examples = 60\n",
      "[INFO|trainer.py:2912] 2022-10-15 00:35:28,855 >>   Batch size = 12\n",
      "100%|█████████████████████████████████████████████| 5/5 [00:00<00:00,  7.88it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        4.0\n",
      "  eval_accuracy           =     0.3573\n",
      "  eval_loss               =     3.6514\n",
      "  eval_runtime            = 0:00:00.84\n",
      "  eval_samples            =         60\n",
      "  eval_samples_per_second =     71.407\n",
      "  eval_steps_per_second   =      5.951\n",
      "  perplexity              =    38.5279\n",
      "[INFO|modelcard.py:444] 2022-10-15 00:35:30,378 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.3573202614379085}]}\n"
     ]
    }
   ],
   "source": [
    "!python hf_clm_training.py \\\n",
    "    --model_name_or_path sberbank-ai/rugpt3large_based_on_gpt2 \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --block_size 256 \\\n",
    "    --lr_scheduler_type linear \\\n",
    "    --train_file data/one_col_input.csv \\\n",
    "    --validation_split_percentage 5 \\\n",
    "    --preprocessing_num_workers 4 \\\n",
    "    --dataloader_num_workers 4 \\\n",
    "    --output_dir models/cont_large_4_256 \\\n",
    "    --per_device_train_batch_size 6 \\\n",
    "    --per_device_eval_batch_size 12 \\\n",
    "    --learning_rate 1.5e-5 \\\n",
    "    --logging_strategy steps \\\n",
    "    --overwrite_output_dir \\\n",
    "    --logging_steps 100 \\\n",
    "    --evaluation_strategy epoch \\\n",
    "    --logging_first_step \\\n",
    "    --save_strategy epoch \\\n",
    "    --num_train_epochs 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/cont_medium_4_256/\n",
      "models/cont_medium_4_256/runs/\n",
      "models/cont_medium_4_256/runs/Oct15_00-18-57_torch-a100-0/\n",
      "models/cont_medium_4_256/runs/Oct15_00-18-57_torch-a100-0/events.out.tfevents.1665782355.torch-a100-0.6111.0\n",
      "models/cont_medium_4_256/runs/Oct15_00-18-57_torch-a100-0/1665782355.9400141/\n",
      "models/cont_medium_4_256/runs/Oct15_00-18-57_torch-a100-0/1665782355.9400141/events.out.tfevents.1665782355.torch-a100-0.6111.1\n",
      "models/cont_medium_4_256/runs/Oct15_00-18-57_torch-a100-0/events.out.tfevents.1665782497.torch-a100-0.6111.2\n",
      "models/cont_medium_4_256/config.json\n",
      "models/cont_medium_4_256/pytorch_model.bin\n",
      "models/cont_medium_4_256/tokenizer_config.json\n",
      "models/cont_medium_4_256/special_tokens_map.json\n",
      "models/cont_medium_4_256/added_tokens.json\n",
      "models/cont_medium_4_256/vocab.json\n",
      "models/cont_medium_4_256/merges.txt\n",
      "models/cont_medium_4_256/tokenizer.json\n",
      "models/cont_medium_4_256/training_args.bin\n",
      "models/cont_medium_4_256/train_results.json\n",
      "models/cont_medium_4_256/all_results.json\n",
      "models/cont_medium_4_256/trainer_state.json\n",
      "models/cont_medium_4_256/eval_results.json\n",
      "models/cont_medium_4_256/README.md\n",
      "models/cont_medium_4_256/.ipynb_checkpoints/\n"
     ]
    }
   ],
   "source": [
    "!tar -czvf models/cont_256_4.tar.gz models/cont_medium_4_256/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[_CudaDeviceProperties(name='A100-PCIE-40GB', major=8, minor=0, total_memory=40537MB, multi_processor_count=108)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "[torch.cuda.get_device_properties(i) for i in range(torch.cuda.device_count())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"models/cont_medium_4_256/\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name_or_path).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<|endoftext|>',\n",
       " 'eos_token': '<|endoftext|>',\n",
       " 'unk_token': '<|endoftext|>'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'355kk params'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(sum(p.numel() for p in model.parameters()) // 1000 // 1000) + \"kk params\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(203, clean_up_tokenization_spaces=False) # new line token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[225]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 19])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"- Я полагаю, что революционное движение масс является основополагающей силой в развитии общества.\\n -\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").cpu()\n",
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.7 s, sys: 19.5 ms, total: 31.7 s\n",
      "Wall time: 2.65 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['- Я полагаю, что революционное движение масс является основополагающей силой в развитии общества.\\n - Да, но мне кажется, что это не так. Если массы могут жить, если они способны подняться до сознательной жизни, то, значит, они еще не доросли до того, чтобы создать что-нибудь стоящее.\\n',\n",
       " '- Я полагаю, что революционное движение масс является основополагающей силой в развитии общества.\\n - Оно должно быть распространено по всему земному шару. Это вопрос жизни и смерти. Для этого оно должно стать общественным.  Сущность буржуазного общества состоит в том, что существует общественный интерес.\\n    ',\n",
       " '- Я полагаю, что революционное движение масс является основополагающей силой в развитии общества.\\n -...Какое же направление вы избрали? Я имею в виду революционное. Каковы ваши идеалы? Кто ваш кумир? Кто ваши кумиры?\\n            ']"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "out = model.generate(input_ids,\n",
    "#                      max_length=150,\n",
    "#                      length_penalty=-10.0,\n",
    "                     max_new_tokens=150,\n",
    "                     min_length=50,\n",
    "                     pad_token_id=225,\n",
    "                     eos_token_id=203,\n",
    "#                      num_beams=3,\n",
    "#                      repetition_penalty=5.0,\n",
    "                     temperature=0.75,\n",
    "#                      top_k=100,\n",
    "                     #top_p=1.2,\n",
    "                     do_sample=True,\n",
    "                     num_return_sequences=3\n",
    "                    )\n",
    "tokenizer.batch_decode(out, skip_special_tokens=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
