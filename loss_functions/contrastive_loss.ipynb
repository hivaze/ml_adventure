{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, FloatTensor, IntTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "def create_metric(metric):\n",
    "    lambdas_dict = {\n",
    "        'euclidian': lambda x, y: F.pairwise_distance(x, y, p=2),\n",
    "        'manhattan': lambda x, y: F.pairwise_distance(x, y, p=1),\n",
    "        'cosine': lambda x, y: 1 - F.cosine_similarity(x, y)\n",
    "    }\n",
    "    assert metric.lower() in lambdas_dict.keys(), \"This metric is not supported\"\n",
    "    return lambdas_dict[metric.lower()]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    Contrastive loss. Expects as input two embeddings and a label of either 0 or 1. If the label == 1, then the distance between the\n",
    "    two embeddings is reduced. If the label == 0, then the distance between the embeddings is increased.\n",
    "\n",
    "    :param distance_metric: Can be 'euclidian', 'manhattan' or 'cosine' (inverted to be distance).\n",
    "    :param positive_margin: The distance over which positive pairs will contribute to the loss.\n",
    "    :param negative_margin: The distance under which negative pairs will contribute to the loss.\n",
    "    :param do_average: Average by mean in batch or summation\n",
    "\n",
    "    | Further information:\n",
    "    | http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    | https://kevinmusgrave.github.io/pytorch-metric-learning/losses/#contrastiveloss\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 distance_metric='cosine',\n",
    "                 positive_margin=0,\n",
    "                 negative_margin=1,\n",
    "                 do_average=True):\n",
    "\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.distance_metric = create_metric(distance_metric)\n",
    "        self.positive_margin = positive_margin\n",
    "        self.negative_margin = negative_margin\n",
    "        self.do_average = do_average\n",
    "\n",
    "    def forward(self, x1: FloatTensor, x2: FloatTensor, label: IntTensor):\n",
    "        \"\"\"\n",
    "        :param x1: embeddings of size [bs, emd_dim]\n",
    "        :param x2: embeddings of size [bs, emd_dim]\n",
    "        :param label: labels in range [0, 1] of size [bs,]\n",
    "        \"\"\"\n",
    "\n",
    "        distances = self.distance_metric(x1, x2)\n",
    "        losses = 0.5 * (\n",
    "                label.float() * F.relu(self.negative_margin - distances).pow(2) +\n",
    "                (1 - label).float() * F.relu(distances - self.positive_margin).pow(2)\n",
    "        )  # RELU can be replaced with torch.clamp; 0.5 scaling is optional\n",
    "\n",
    "        return losses.mean() if self.do_average else losses.sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0, 1, 1, 1, 1, 1, 0, 1, 1, 1])"
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = torch.randn([10, 128])\n",
    "x2 = torch.randn([10, 128])\n",
    "label = torch.randint(low=0, high=2, size=[10])\n",
    "label"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([1.1794, 0.9802, 1.1684, 0.9810, 1.0024, 1.0226, 1.0147, 0.8088, 0.8799,\n        1.0057])"
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances = 1 - F.cosine_similarity(x1, x2)\n",
    "distances"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(0.2574)"
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ContrastiveLoss(distance_metric='cosine', positive_margin=0, negative_margin=1)(x1, x2, label)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
