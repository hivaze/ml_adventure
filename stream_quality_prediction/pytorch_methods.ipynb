{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "random_seed = 228\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cuda', index=0)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load test and train data into separate variables so that the test data does not affect training in any way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "   fps_mean   fps_std  rtt_mean   rtt_std  dropped_frames_mean  \\\n0  0.744824  0.025512  0.786908  0.013918              0.00323   \n1  0.744824  0.025512  0.810122  0.055803              0.00323   \n2  0.734408  0.076537  0.791670  0.031781              0.00323   \n3  0.750031  0.000000  0.826193  0.015573              0.00323   \n4  0.703162  0.159856  0.816669  0.005438              0.00323   \n\n   dropped_frames_std  dropped_frames_max  bitrate_mean  bitrate_std  \\\n0                 0.0             0.00323      0.066147     0.010390   \n1                 0.0             0.00323      0.077022     0.041797   \n2                 0.0             0.00323      0.069172     0.017070   \n3                 0.0             0.00323      0.061703     0.021221   \n4                 0.0             0.00323      0.040957     0.062898   \n\n   packet_loss_rate  packet_loss_std    y  \n0          0.000250         0.000000  1.0  \n1          0.031492         0.153055  1.0  \n2          0.000250         0.000000  1.0  \n3          0.003121         0.014067  1.0  \n4          0.003121         0.014067  1.0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fps_mean</th>\n      <th>fps_std</th>\n      <th>rtt_mean</th>\n      <th>rtt_std</th>\n      <th>dropped_frames_mean</th>\n      <th>dropped_frames_std</th>\n      <th>dropped_frames_max</th>\n      <th>bitrate_mean</th>\n      <th>bitrate_std</th>\n      <th>packet_loss_rate</th>\n      <th>packet_loss_std</th>\n      <th>y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.744824</td>\n      <td>0.025512</td>\n      <td>0.786908</td>\n      <td>0.013918</td>\n      <td>0.00323</td>\n      <td>0.0</td>\n      <td>0.00323</td>\n      <td>0.066147</td>\n      <td>0.010390</td>\n      <td>0.000250</td>\n      <td>0.000000</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.744824</td>\n      <td>0.025512</td>\n      <td>0.810122</td>\n      <td>0.055803</td>\n      <td>0.00323</td>\n      <td>0.0</td>\n      <td>0.00323</td>\n      <td>0.077022</td>\n      <td>0.041797</td>\n      <td>0.031492</td>\n      <td>0.153055</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.734408</td>\n      <td>0.076537</td>\n      <td>0.791670</td>\n      <td>0.031781</td>\n      <td>0.00323</td>\n      <td>0.0</td>\n      <td>0.00323</td>\n      <td>0.069172</td>\n      <td>0.017070</td>\n      <td>0.000250</td>\n      <td>0.000000</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.750031</td>\n      <td>0.000000</td>\n      <td>0.826193</td>\n      <td>0.015573</td>\n      <td>0.00323</td>\n      <td>0.0</td>\n      <td>0.00323</td>\n      <td>0.061703</td>\n      <td>0.021221</td>\n      <td>0.003121</td>\n      <td>0.014067</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.703162</td>\n      <td>0.159856</td>\n      <td>0.816669</td>\n      <td>0.005438</td>\n      <td>0.00323</td>\n      <td>0.0</td>\n      <td>0.00323</td>\n      <td>0.040957</td>\n      <td>0.062898</td>\n      <td>0.003121</td>\n      <td>0.014067</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stream_df = pd.read_csv('data/stream_quality_train.csv')\n",
    "stream_dft = pd.read_csv('data/stream_quality_test.csv')\n",
    "stream_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the data to make sure it's already normalized, doesn't contain categorical features (although this doesn't matter for trees and random forests), and generally doesn't need much preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "            fps_mean        fps_std      rtt_mean        rtt_std  \\\ncount  760552.000000  760552.000000  7.605520e+05  760552.000000   \nmean        0.573862       0.084140  2.319465e-01       0.087737   \nstd         0.169349       0.071507  1.681769e-01       0.069711   \nmin         0.000009       0.000000  8.172605e-08       0.000000   \n25%         0.480095       0.026579  9.748398e-02       0.029069   \n50%         0.561044       0.070193  1.978639e-01       0.072856   \n75%         0.673666       0.127213  3.351587e-01       0.133428   \nmax         1.000000       0.510244  1.000000e+00       0.510244   \n\n       dropped_frames_mean  dropped_frames_std  dropped_frames_max  \\\ncount        760552.000000       760552.000000       760552.000000   \nmean              0.375107            0.013855            0.426036   \nstd               0.471648            0.053630            0.477459   \nmin               0.000002            0.000000            0.000002   \n25%               0.001744            0.000000            0.002109   \n50%               0.024770            0.000000            0.049641   \n75%               1.000000            0.000000            1.000000   \nmax               1.000000            0.510743            1.000000   \n\n       bitrate_mean    bitrate_std  packet_loss_rate  packet_loss_std  \\\ncount  7.605520e+05  760552.000000     760552.000000    760552.000000   \nmean   3.947766e-01       0.113410          0.210568         0.054017   \nstd    1.828205e-01       0.071459          0.359459         0.086743   \nmin    1.596526e-08       0.000000          0.000010         0.000000   \n25%    2.608380e-01       0.060778          0.000999         0.000000   \n50%    4.497636e-01       0.109997          0.041906         0.000000   \n75%    5.325386e-01       0.161189          0.143510         0.095150   \nmax    1.000000e+00       0.503595          1.000000         0.510626   \n\n                   y  \ncount  760552.000000  \nmean        1.194584  \nstd         0.846783  \nmin         0.000000  \n25%         0.000000  \n50%         1.000000  \n75%         2.000000  \nmax         2.000000  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fps_mean</th>\n      <th>fps_std</th>\n      <th>rtt_mean</th>\n      <th>rtt_std</th>\n      <th>dropped_frames_mean</th>\n      <th>dropped_frames_std</th>\n      <th>dropped_frames_max</th>\n      <th>bitrate_mean</th>\n      <th>bitrate_std</th>\n      <th>packet_loss_rate</th>\n      <th>packet_loss_std</th>\n      <th>y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>760552.000000</td>\n      <td>760552.000000</td>\n      <td>7.605520e+05</td>\n      <td>760552.000000</td>\n      <td>760552.000000</td>\n      <td>760552.000000</td>\n      <td>760552.000000</td>\n      <td>7.605520e+05</td>\n      <td>760552.000000</td>\n      <td>760552.000000</td>\n      <td>760552.000000</td>\n      <td>760552.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.573862</td>\n      <td>0.084140</td>\n      <td>2.319465e-01</td>\n      <td>0.087737</td>\n      <td>0.375107</td>\n      <td>0.013855</td>\n      <td>0.426036</td>\n      <td>3.947766e-01</td>\n      <td>0.113410</td>\n      <td>0.210568</td>\n      <td>0.054017</td>\n      <td>1.194584</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.169349</td>\n      <td>0.071507</td>\n      <td>1.681769e-01</td>\n      <td>0.069711</td>\n      <td>0.471648</td>\n      <td>0.053630</td>\n      <td>0.477459</td>\n      <td>1.828205e-01</td>\n      <td>0.071459</td>\n      <td>0.359459</td>\n      <td>0.086743</td>\n      <td>0.846783</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000009</td>\n      <td>0.000000</td>\n      <td>8.172605e-08</td>\n      <td>0.000000</td>\n      <td>0.000002</td>\n      <td>0.000000</td>\n      <td>0.000002</td>\n      <td>1.596526e-08</td>\n      <td>0.000000</td>\n      <td>0.000010</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.480095</td>\n      <td>0.026579</td>\n      <td>9.748398e-02</td>\n      <td>0.029069</td>\n      <td>0.001744</td>\n      <td>0.000000</td>\n      <td>0.002109</td>\n      <td>2.608380e-01</td>\n      <td>0.060778</td>\n      <td>0.000999</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.561044</td>\n      <td>0.070193</td>\n      <td>1.978639e-01</td>\n      <td>0.072856</td>\n      <td>0.024770</td>\n      <td>0.000000</td>\n      <td>0.049641</td>\n      <td>4.497636e-01</td>\n      <td>0.109997</td>\n      <td>0.041906</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.673666</td>\n      <td>0.127213</td>\n      <td>3.351587e-01</td>\n      <td>0.133428</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>5.325386e-01</td>\n      <td>0.161189</td>\n      <td>0.143510</td>\n      <td>0.095150</td>\n      <td>2.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.000000</td>\n      <td>0.510244</td>\n      <td>1.000000e+00</td>\n      <td>0.510244</td>\n      <td>1.000000</td>\n      <td>0.510743</td>\n      <td>1.000000</td>\n      <td>1.000000e+00</td>\n      <td>0.503595</td>\n      <td>1.000000</td>\n      <td>0.510626</td>\n      <td>2.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stream_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "            fps_mean        fps_std      rtt_mean        rtt_std  \\\ncount  129978.000000  129978.000000  1.299780e+05  129978.000000   \nmean        0.562242       0.081462  2.193421e-01       0.085001   \nstd         0.164251       0.070950  1.625906e-01       0.068021   \nmin         0.000012       0.000000  1.391982e-07       0.000000   \n25%         0.477536       0.022696  8.891946e-02       0.028232   \n50%         0.550761       0.067979  1.878190e-01       0.070574   \n75%         0.658131       0.124792  3.152932e-01       0.128603   \nmax         1.000000       0.510244  1.000000e+00       0.508469   \n\n       dropped_frames_mean  dropped_frames_std  dropped_frames_max  \\\ncount        129978.000000       129978.000000       129978.000000   \nmean              0.339517            0.015303            0.393941   \nstd               0.460384            0.057180            0.471005   \nmin               0.000003            0.000000            0.000003   \n25%               0.000999            0.000000            0.000999   \n50%               0.018182            0.000000            0.037107   \n75%               1.000000            0.000000            1.000000   \nmax               1.000000            0.510743            1.000000   \n\n       bitrate_mean    bitrate_std  packet_loss_rate  packet_loss_std  \\\ncount  1.299780e+05  129978.000000     129978.000000    129978.000000   \nmean   3.972422e-01       0.112772          0.177990         0.055387   \nstd    1.813835e-01       0.070860          0.335288         0.087149   \nmin    1.596526e-08       0.000000          0.000011         0.000000   \n25%    2.676697e-01       0.060610          0.000999         0.000000   \n50%    4.502050e-01       0.109068          0.031705         0.000000   \n75%    5.335235e-01       0.160607          0.106635         0.101940   \nmax    1.000000e+00       0.441708          1.000000         0.505441   \n\n                   y  \ncount  129978.000000  \nmean        1.217383  \nstd         0.857186  \nmin         0.000000  \n25%         0.000000  \n50%         1.000000  \n75%         2.000000  \nmax         2.000000  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fps_mean</th>\n      <th>fps_std</th>\n      <th>rtt_mean</th>\n      <th>rtt_std</th>\n      <th>dropped_frames_mean</th>\n      <th>dropped_frames_std</th>\n      <th>dropped_frames_max</th>\n      <th>bitrate_mean</th>\n      <th>bitrate_std</th>\n      <th>packet_loss_rate</th>\n      <th>packet_loss_std</th>\n      <th>y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>129978.000000</td>\n      <td>129978.000000</td>\n      <td>1.299780e+05</td>\n      <td>129978.000000</td>\n      <td>129978.000000</td>\n      <td>129978.000000</td>\n      <td>129978.000000</td>\n      <td>1.299780e+05</td>\n      <td>129978.000000</td>\n      <td>129978.000000</td>\n      <td>129978.000000</td>\n      <td>129978.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.562242</td>\n      <td>0.081462</td>\n      <td>2.193421e-01</td>\n      <td>0.085001</td>\n      <td>0.339517</td>\n      <td>0.015303</td>\n      <td>0.393941</td>\n      <td>3.972422e-01</td>\n      <td>0.112772</td>\n      <td>0.177990</td>\n      <td>0.055387</td>\n      <td>1.217383</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.164251</td>\n      <td>0.070950</td>\n      <td>1.625906e-01</td>\n      <td>0.068021</td>\n      <td>0.460384</td>\n      <td>0.057180</td>\n      <td>0.471005</td>\n      <td>1.813835e-01</td>\n      <td>0.070860</td>\n      <td>0.335288</td>\n      <td>0.087149</td>\n      <td>0.857186</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000012</td>\n      <td>0.000000</td>\n      <td>1.391982e-07</td>\n      <td>0.000000</td>\n      <td>0.000003</td>\n      <td>0.000000</td>\n      <td>0.000003</td>\n      <td>1.596526e-08</td>\n      <td>0.000000</td>\n      <td>0.000011</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.477536</td>\n      <td>0.022696</td>\n      <td>8.891946e-02</td>\n      <td>0.028232</td>\n      <td>0.000999</td>\n      <td>0.000000</td>\n      <td>0.000999</td>\n      <td>2.676697e-01</td>\n      <td>0.060610</td>\n      <td>0.000999</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.550761</td>\n      <td>0.067979</td>\n      <td>1.878190e-01</td>\n      <td>0.070574</td>\n      <td>0.018182</td>\n      <td>0.000000</td>\n      <td>0.037107</td>\n      <td>4.502050e-01</td>\n      <td>0.109068</td>\n      <td>0.031705</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.658131</td>\n      <td>0.124792</td>\n      <td>3.152932e-01</td>\n      <td>0.128603</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>5.335235e-01</td>\n      <td>0.160607</td>\n      <td>0.106635</td>\n      <td>0.101940</td>\n      <td>2.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.000000</td>\n      <td>0.510244</td>\n      <td>1.000000e+00</td>\n      <td>0.508469</td>\n      <td>1.000000</td>\n      <td>0.510743</td>\n      <td>1.000000</td>\n      <td>1.000000e+00</td>\n      <td>0.441708</td>\n      <td>1.000000</td>\n      <td>0.505441</td>\n      <td>2.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stream_dft.describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also take a look at the report to see feature correlation and class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pandas_profiling import ProfileReport\n",
    "# report = ProfileReport(stream_df)\n",
    "# report.to_file('data_profile_report.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_selection import SelectFromModel, SelectKBest"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def generate_pairwise_features(df, select_p = 0.5):\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    all_pairs = np.array(list(set(combinations(list(df.columns), 2))))\n",
    "    mask = np.random.random(len(all_pairs)) < select_p\n",
    "    selected_pairs = all_pairs[mask]\n",
    "\n",
    "    for (f, s) in selected_pairs:\n",
    "        d = dict()\n",
    "\n",
    "        d[f'{f}_plus_{s}'] = df[f] + df[s]\n",
    "        d[f'{f}_minus_{s}'] = df[f] - df[s]\n",
    "        # d[f'{f}_mult_{s}'] = df[f] * df[s]\n",
    "\n",
    "        new_features = pd.DataFrame(d)\n",
    "        df = pd.concat([df, new_features], axis=1)\n",
    "\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def generate_math_features(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    for c in df.columns:\n",
    "        d = dict()\n",
    "\n",
    "        d[c + '_log'] = np.log(df[c].abs() + 1)\n",
    "        # d[c + '_exp'] = np.exp(df[c])\n",
    "        d[c + '_sin'] = np.sin(df[c])\n",
    "        d[c + '_near_q0.5'] = np.abs(df[c] - df[c].quantile(0.5)) < 0.1\n",
    "        d[c + '_gt_q0.8'] = df[c] > df[c].quantile(0.8)\n",
    "        # d[c + '_lt_q0.9'] = df[c] < df[c].quantile(0.9)\n",
    "        # d[c + '_gt_q0.1'] = df[c] > df[c].quantile(0.1)\n",
    "        d[c + '_lt_q0.2'] = df[c] < df[c].quantile(0.2)\n",
    "\n",
    "        new_features = pd.DataFrame(d)\n",
    "        df = pd.concat([df, new_features], axis=1)\n",
    "\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "stream_df = stream_df[(np.abs(stats.zscore(stream_df)) < 3).all(axis=1)]\n",
    "# stream_dft = stream_dft[(np.abs(stats.zscore(stream_dft)) < 3).all(axis=1)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "(698758, 121)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = generate_pairwise_features(stream_df.drop('y', axis=1), select_p=1)\n",
    "x_train.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "(129978, 121)"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = generate_pairwise_features(stream_dft.drop('y', axis=1), select_p=1)\n",
    "x_test.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "(698758,)"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = stream_df['y']\n",
    "y_train.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "(129978,)"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = stream_dft['y']\n",
    "y_test.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "<AxesSubplot: ylabel='Count'>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(pd.concat([y_train.rename(\"y_train\"), y_test.rename(\"y_test\")], axis=1), bins=3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  30 out of  30 | elapsed:   24.5s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": "(41,)"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sfm1 = SelectFromModel(threshold=\"0.3*mean\", estimator=RandomForestClassifier(n_estimators=30, random_state=228, n_jobs=12, verbose=1, max_depth=12, class_weight='balanced'), max_features=50)\n",
    "# sfm1 = SelectKBest(k=30)\n",
    "sfm1.fit(x_train, y_train)\n",
    "sfm1.get_feature_names_out().shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "(698758, 41)"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = pd.DataFrame(data=sfm1.transform(x_train), columns=sfm1.get_feature_names_out())\n",
    "x_train.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "(129978, 41)"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = pd.DataFrame(data=sfm1.transform(x_test), columns=sfm1.get_feature_names_out())\n",
    "x_test.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "(698758, 246)"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = generate_math_features(x_train)\n",
    "x_train.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "(129978, 246)"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = generate_math_features(x_test)\n",
    "x_test.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  30 out of  30 | elapsed:   23.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": "(73,)"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sfm2 = SelectFromModel(threshold=\"0.3*mean\", estimator=RandomForestClassifier(n_estimators=30, random_state=228, n_jobs=12, verbose=1, max_depth=12, class_weight='balanced'), max_features=100)\n",
    "# sfm2 = SelectKBest(k=70)\n",
    "sfm2.fit(x_train, y_train)\n",
    "sfm2.get_feature_names_out().shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "(698758, 73)"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = pd.DataFrame(data=sfm2.transform(x_train), columns=sfm2.get_feature_names_out())\n",
    "x_train.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "(129978, 73)"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = pd.DataFrame(data=sfm2.transform(x_test), columns=sfm2.get_feature_names_out())\n",
    "x_test.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "from skorch import NeuralNetClassifier, NeuralNetBinaryClassifier\n",
    "from skorch.callbacks import EpochScoring\n",
    "from skorch.dataset import ValidSplit\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import StackingClassifier, VotingClassifier, BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.82      0.81     36695\n",
      "         1.0       0.81      0.88      0.85     28333\n",
      "         2.0       0.90      0.85      0.88     64950\n",
      "\n",
      "    accuracy                           0.85    129978\n",
      "   macro avg       0.84      0.85      0.84    129978\n",
      "weighted avg       0.85      0.85      0.85    129978\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Done  32 out of  32 | elapsed:   14.4s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  32 out of  32 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "# fake_x_train = pd.concat([x_train, pd.Series(name='random', data=np.random.randn(x_train.shape[0]))], axis=1)\n",
    "rf = RandomForestClassifier(n_estimators=32, n_jobs=16, max_depth=10, verbose=1, class_weight='balanced', criterion='gini')\n",
    "rf.fit(x_train, y_train)\n",
    "print(classification_report(y_test, rf.predict(x_test)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                      1\n0                                                      \nrtt_mean_minus_dropped_frames_std_log          0.100993\nbitrate_mean_sin                               0.064202\nrtt_mean                                       0.063982\nrtt_mean_sin                                   0.055933\ndropped_frames_std_plus_bitrate_mean           0.053825\n...                                                 ...\nrtt_mean_near_q0.5                             0.000512\ndropped_frames_std_minus_bitrate_mean_gt_q0.8  0.000469\nrtt_mean_plus_dropped_frames_mean              0.000442\nrtt_mean_gt_q0.8                               0.000312\nrtt_mean_plus_dropped_frames_std_near_q0.5     0.000263\n\n[73 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>1</th>\n    </tr>\n    <tr>\n      <th>0</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>rtt_mean_minus_dropped_frames_std_log</th>\n      <td>0.100993</td>\n    </tr>\n    <tr>\n      <th>bitrate_mean_sin</th>\n      <td>0.064202</td>\n    </tr>\n    <tr>\n      <th>rtt_mean</th>\n      <td>0.063982</td>\n    </tr>\n    <tr>\n      <th>rtt_mean_sin</th>\n      <td>0.055933</td>\n    </tr>\n    <tr>\n      <th>dropped_frames_std_plus_bitrate_mean</th>\n      <td>0.053825</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>rtt_mean_near_q0.5</th>\n      <td>0.000512</td>\n    </tr>\n    <tr>\n      <th>dropped_frames_std_minus_bitrate_mean_gt_q0.8</th>\n      <td>0.000469</td>\n    </tr>\n    <tr>\n      <th>rtt_mean_plus_dropped_frames_mean</th>\n      <td>0.000442</td>\n    </tr>\n    <tr>\n      <th>rtt_mean_gt_q0.8</th>\n      <td>0.000312</td>\n    </tr>\n    <tr>\n      <th>rtt_mean_plus_dropped_frames_std_near_q0.5</th>\n      <td>0.000263</td>\n    </tr>\n  </tbody>\n</table>\n<p>73 rows  1 columns</p>\n</div>"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(zip(rf.feature_names_in_, rf.feature_importances_)).set_index(0).sort_values(by=1, ascending=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hivaze/miniconda3/envs/torch/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/home/hivaze/miniconda3/envs/torch/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.50497 |  0:00:04s\n",
      "epoch 1  | loss: 0.4174  |  0:00:08s\n",
      "epoch 2  | loss: 0.40783 |  0:00:12s\n",
      "epoch 3  | loss: 0.40107 |  0:00:15s\n",
      "epoch 4  | loss: 0.39601 |  0:00:19s\n",
      "epoch 5  | loss: 0.39473 |  0:00:23s\n",
      "epoch 6  | loss: 0.3937  |  0:00:27s\n",
      "epoch 7  | loss: 0.38971 |  0:00:30s\n",
      "epoch 8  | loss: 0.39038 |  0:00:34s\n",
      "epoch 9  | loss: 0.38895 |  0:00:38s\n",
      "epoch 10 | loss: 0.3888  |  0:00:42s\n",
      "epoch 11 | loss: 0.38732 |  0:00:45s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.78      0.80     36695\n",
      "         1.0       0.84      0.85      0.84     28333\n",
      "         2.0       0.87      0.90      0.88     64950\n",
      "\n",
      "    accuracy                           0.85    129978\n",
      "   macro avg       0.85      0.84      0.84    129978\n",
      "weighted avg       0.85      0.85      0.85    129978\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "# tabnet_clf = TabNetClassifier(n_d=8)\n",
    "# tabnet_clf.fit(x_test.values, y_test.values, max_epochs=12)\n",
    "# print(classification_report(y_true=y_test, y_pred=tabnet_clf.predict(x_test.values)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "class SingleClassModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 dropout_p=0.1,\n",
    "                 n_layers=3,\n",
    "                 num_of_params=x_train.shape[1],\n",
    "                 layer_size=15 * x_train.shape[1],\n",
    "                 n_classes=3\n",
    "                 ):\n",
    "\n",
    "        super(SingleClassModel, self).__init__()\n",
    "\n",
    "        layers = [nn.Sequential(\n",
    "            nn.BatchNorm1d(num_of_params if i == 0 else layer_size),\n",
    "            nn.Linear(num_of_params if i == 0 else layer_size, layer_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_p)\n",
    "        ) for i in range(n_layers)]\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            *layers,\n",
    "            nn.BatchNorm1d(layer_size),\n",
    "            nn.Linear(layer_size, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "net = NeuralNetClassifier(\n",
    "    SingleClassModel,\n",
    "    max_epochs=200,\n",
    "    lr=0.005,\n",
    "    train_split=ValidSplit(cv=5),\n",
    "    # train_split=None,\n",
    "    batch_size=1024,\n",
    "    device='cuda',\n",
    "    optimizer=torch.optim.AdamW,\n",
    "    criterion=torch.nn.CrossEntropyLoss,\n",
    "    callbacks=[EpochScoring(scoring='f1_macro', lower_is_better=False)],\n",
    "    # Shuffle training data on each epoch\n",
    "    iterator_train__shuffle=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    f1_macro    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ----------  ------------  -----------  ------------  ------\n",
      "      1      \u001B[36m0.8440\u001B[0m        \u001B[32m0.4523\u001B[0m       \u001B[35m0.8581\u001B[0m        \u001B[31m0.3815\u001B[0m  1.0344\n",
      "      2      \u001B[36m0.8491\u001B[0m        \u001B[32m0.3962\u001B[0m       \u001B[35m0.8644\u001B[0m        \u001B[31m0.3669\u001B[0m  0.8579\n",
      "      3      0.8487        0.4008       0.8630        0.3671  0.8650\n",
      "      4      0.8479        0.3979       0.8632        0.3683  0.8656\n",
      "      5      \u001B[36m0.8497\u001B[0m        \u001B[32m0.3926\u001B[0m       0.8633        0.3729  0.8455\n",
      "      6      \u001B[36m0.8503\u001B[0m        0.3936       \u001B[35m0.8652\u001B[0m        \u001B[31m0.3634\u001B[0m  0.8474\n",
      "      7      0.8443        \u001B[32m0.3908\u001B[0m       0.8607        0.3704  0.8452\n",
      "      8      0.8472        0.3913       0.8639        0.3739  0.8454\n",
      "      9      0.8379        \u001B[32m0.3905\u001B[0m       0.8581        0.3809  0.8511\n",
      "     10      0.8485        \u001B[32m0.3869\u001B[0m       0.8626        0.3742  0.8604\n",
      "     11      0.8475        \u001B[32m0.3839\u001B[0m       0.8640        0.3679  1.1319\n",
      "     12      0.8483        0.3862       0.8651        0.3643  0.8474\n",
      "     13      0.8476        \u001B[32m0.3838\u001B[0m       0.8636        0.3677  0.8552\n",
      "     14      0.8464        0.3846       0.8625        0.3637  0.8464\n",
      "     15      0.8485        0.3847       0.8637        0.3671  0.9324\n",
      "     16      0.8461        \u001B[32m0.3828\u001B[0m       0.8630        0.3673  0.8407\n",
      "     17      0.8480        \u001B[32m0.3804\u001B[0m       0.8637        0.3653  0.8472\n",
      "     18      0.8485        0.3827       0.8637        \u001B[31m0.3632\u001B[0m  0.8497\n",
      "     19      0.8474        0.3805       0.8621        0.3646  0.8751\n",
      "     20      0.8480        0.3806       0.8633        \u001B[31m0.3627\u001B[0m  0.8535\n",
      "     21      0.8493        \u001B[32m0.3779\u001B[0m       0.8641        \u001B[31m0.3611\u001B[0m  0.8528\n",
      "     22      0.8476        \u001B[32m0.3774\u001B[0m       0.8633        0.3642  0.8393\n",
      "     23      0.8464        \u001B[32m0.3761\u001B[0m       0.8626        0.3620  0.8479\n",
      "     24      0.8464        \u001B[32m0.3756\u001B[0m       0.8623        0.3679  0.8440\n",
      "     25      0.8495        \u001B[32m0.3741\u001B[0m       0.8638        0.3637  0.8481\n",
      "     26      0.8483        \u001B[32m0.3740\u001B[0m       0.8632        0.3636  1.0600\n",
      "     27      0.8472        \u001B[32m0.3715\u001B[0m       0.8630        0.3669  0.8402\n",
      "     28      0.8466        \u001B[32m0.3704\u001B[0m       0.8622        0.3644  0.8418\n",
      "     29      0.8458        0.3706       0.8605        0.3734  0.8531\n",
      "     30      0.8466        \u001B[32m0.3700\u001B[0m       0.8619        0.3662  0.8948\n",
      "     31      0.8452        \u001B[32m0.3680\u001B[0m       0.8614        0.3725  0.8412\n",
      "     32      0.8455        0.3686       0.8603        0.3672  0.8463\n",
      "     33      0.8439        \u001B[32m0.3657\u001B[0m       0.8596        0.3752  0.8413\n",
      "     34      0.8416        \u001B[32m0.3637\u001B[0m       0.8570        0.3782  0.8414\n",
      "     35      0.8428        \u001B[32m0.3620\u001B[0m       0.8572        0.3817  0.8482\n",
      "     36      0.8397        \u001B[32m0.3614\u001B[0m       0.8562        0.3746  0.8403\n",
      "     37      0.8367        \u001B[32m0.3601\u001B[0m       0.8549        0.3826  0.8416\n",
      "     38      0.8414        \u001B[32m0.3580\u001B[0m       0.8597        0.3757  0.8531\n",
      "     39      0.8411        \u001B[32m0.3570\u001B[0m       0.8576        0.3762  0.8419\n",
      "     40      0.8387        \u001B[32m0.3519\u001B[0m       0.8557        0.3841  0.8408\n",
      "     41      0.8377        \u001B[32m0.3517\u001B[0m       0.8551        0.3841  1.0519\n",
      "     42      0.8404        \u001B[32m0.3496\u001B[0m       0.8566        0.3878  0.8464\n",
      "     43      0.8396        \u001B[32m0.3465\u001B[0m       0.8574        0.3890  0.8379\n",
      "     44      0.8350        \u001B[32m0.3435\u001B[0m       0.8514        0.4039  0.8525\n",
      "     45      0.8374        \u001B[32m0.3389\u001B[0m       0.8547        0.3963  0.8897\n",
      "     46      0.8392        \u001B[32m0.3373\u001B[0m       0.8554        0.4044  0.8451\n",
      "     47      0.8364        \u001B[32m0.3321\u001B[0m       0.8528        0.3996  0.8367\n",
      "     48      0.8355        \u001B[32m0.3292\u001B[0m       0.8521        0.4047  0.8374\n",
      "     49      0.8351        \u001B[32m0.3238\u001B[0m       0.8536        0.4083  0.8436\n",
      "     50      0.8337        \u001B[32m0.3228\u001B[0m       0.8520        0.4151  0.8459\n",
      "     51      0.8297        \u001B[32m0.3160\u001B[0m       0.8497        0.4238  0.8487\n",
      "     52      0.8348        \u001B[32m0.3146\u001B[0m       0.8492        0.4363  0.8463\n",
      "     53      0.8293        \u001B[32m0.3068\u001B[0m       0.8469        0.4528  0.8417\n",
      "     54      0.8278        \u001B[32m0.3058\u001B[0m       0.8468        0.4471  0.8457\n",
      "     55      0.8318        \u001B[32m0.3000\u001B[0m       0.8472        0.4483  0.8659\n",
      "     56      0.8266        \u001B[32m0.2937\u001B[0m       0.8432        0.4589  1.0583\n",
      "     57      0.8299        \u001B[32m0.2890\u001B[0m       0.8475        0.4723  0.8437\n",
      "     58      0.8192        \u001B[32m0.2820\u001B[0m       0.8369        0.4880  0.8459\n",
      "     59      0.8258        \u001B[32m0.2792\u001B[0m       0.8434        0.4862  0.8502\n",
      "     60      0.8255        \u001B[32m0.2729\u001B[0m       0.8442        0.5151  0.8537\n",
      "     61      0.8215        \u001B[32m0.2672\u001B[0m       0.8401        0.5107  0.8866\n",
      "     62      0.8227        \u001B[32m0.2631\u001B[0m       0.8397        0.5208  0.9208\n",
      "     63      0.8195        \u001B[32m0.2581\u001B[0m       0.8382        0.5511  0.8553\n",
      "     64      0.8199        \u001B[32m0.2498\u001B[0m       0.8380        0.5486  0.8399\n",
      "     65      0.8195        \u001B[32m0.2449\u001B[0m       0.8406        0.5723  0.8416\n",
      "     66      0.8215        \u001B[32m0.2405\u001B[0m       0.8386        0.5683  0.8373\n",
      "     67      0.8182        \u001B[32m0.2334\u001B[0m       0.8361        0.5899  0.8417\n",
      "     68      0.8109        \u001B[32m0.2278\u001B[0m       0.8293        0.5985  0.8422\n",
      "     69      0.8162        \u001B[32m0.2248\u001B[0m       0.8348        0.6021  0.8411\n",
      "     70      0.8117        \u001B[32m0.2191\u001B[0m       0.8295        0.6526  1.0481\n",
      "     71      0.8124        \u001B[32m0.2165\u001B[0m       0.8325        0.6562  0.8298\n",
      "     72      0.8150        \u001B[32m0.2082\u001B[0m       0.8331        0.6700  0.8506\n",
      "     73      0.8175        0.2083       0.8336        0.6801  0.8418\n",
      "     74      0.8111        \u001B[32m0.1985\u001B[0m       0.8304        0.7168  0.8478\n",
      "     75      0.8062        \u001B[32m0.1938\u001B[0m       0.8267        0.7324  0.8433\n",
      "     76      0.8080        \u001B[32m0.1930\u001B[0m       0.8241        0.7104  0.8383\n",
      "     77      0.8123        \u001B[32m0.1849\u001B[0m       0.8304        0.7612  0.8560\n",
      "     78      0.8048        0.1853       0.8235        0.7680  0.8510\n",
      "     79      0.8080        \u001B[32m0.1781\u001B[0m       0.8279        0.7638  0.8790\n",
      "     80      0.8016        \u001B[32m0.1772\u001B[0m       0.8198        0.7568  0.8779\n",
      "     81      0.8040        \u001B[32m0.1711\u001B[0m       0.8238        0.7964  0.8493\n",
      "     82      0.8019        \u001B[32m0.1661\u001B[0m       0.8207        0.7992  0.8424\n",
      "     83      0.8079        \u001B[32m0.1642\u001B[0m       0.8271        0.8542  0.8419\n",
      "     84      0.8022        \u001B[32m0.1568\u001B[0m       0.8221        0.8400  0.8537\n"
     ]
    },
    {
     "data": {
      "text/plain": "<class 'skorch.classifier.NeuralNetClassifier'>[initialized](\n  module_=SingleClassModel(\n    (network): Sequential(\n      (0): Sequential(\n        (0): BatchNorm1d(73, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (1): Linear(in_features=73, out_features=1095, bias=True)\n        (2): GELU(approximate=none)\n        (3): Dropout(p=0.0, inplace=False)\n      )\n      (1): Sequential(\n        (0): BatchNorm1d(1095, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (1): Linear(in_features=1095, out_features=1095, bias=True)\n        (2): GELU(approximate=none)\n        (3): Dropout(p=0.0, inplace=False)\n      )\n      (2): Sequential(\n        (0): BatchNorm1d(1095, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (1): Linear(in_features=1095, out_features=1095, bias=True)\n        (2): GELU(approximate=none)\n        (3): Dropout(p=0.0, inplace=False)\n      )\n      (3): BatchNorm1d(1095, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (4): Linear(in_features=1095, out_features=3, bias=True)\n    )\n  ),\n)"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.fit(x_train.values.astype(np.float32), y_train.values.astype(np.int64))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.92      0.90     36695\n",
      "         1.0       0.93      0.92      0.93     28333\n",
      "         2.0       0.95      0.93      0.94     64950\n",
      "\n",
      "    accuracy                           0.93    129978\n",
      "   macro avg       0.92      0.92      0.92    129978\n",
      "weighted avg       0.93      0.93      0.93    129978\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true=y_test, y_pred=net.predict(x_test.values.astype(np.float32))))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.76      0.79      0.77    195754\n",
      "         1.0       0.83      0.81      0.82    167126\n",
      "         2.0       0.86      0.85      0.85    335878\n",
      "\n",
      "    accuracy                           0.82    698758\n",
      "   macro avg       0.81      0.81      0.81    698758\n",
      "weighted avg       0.82      0.82      0.82    698758\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true=y_train, y_pred=net.predict(x_train.values.astype(np.float32))))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    f1_macro    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ----------  ------------  -----------  ------------  ------\n",
      "      1      \u001B[36m0.8279\u001B[0m        \u001B[32m0.3954\u001B[0m       \u001B[35m0.8386\u001B[0m        \u001B[31m0.4023\u001B[0m  1.8357\n",
      "      2      \u001B[36m0.8406\u001B[0m        \u001B[32m0.3802\u001B[0m       \u001B[35m0.8494\u001B[0m        \u001B[31m0.3834\u001B[0m  1.5916\n",
      "      3      \u001B[36m0.8487\u001B[0m        \u001B[32m0.3791\u001B[0m       \u001B[35m0.8549\u001B[0m        0.3840  1.6521\n",
      "      4      \u001B[36m0.8491\u001B[0m        \u001B[32m0.3786\u001B[0m       0.8545        0.3845  1.5498\n",
      "      5      0.8476        \u001B[32m0.3777\u001B[0m       0.8534        \u001B[31m0.3821\u001B[0m  1.6076\n",
      "      6      0.8488        \u001B[32m0.3757\u001B[0m       \u001B[35m0.8556\u001B[0m        \u001B[31m0.3741\u001B[0m  1.5657\n",
      "      7      0.8479        0.3758       0.8550        0.3881  1.6223\n",
      "  epoch    f1_macro    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ----------  ------------  -----------  ------------  ------\n",
      "      1      \u001B[36m0.8417\u001B[0m        \u001B[32m0.3940\u001B[0m       \u001B[35m0.8481\u001B[0m        \u001B[31m0.3989\u001B[0m  1.7882\n",
      "      2      \u001B[36m0.8484\u001B[0m        \u001B[32m0.3819\u001B[0m       \u001B[35m0.8551\u001B[0m        \u001B[31m0.3805\u001B[0m  1.6438\n",
      "      3      \u001B[36m0.8491\u001B[0m        \u001B[32m0.3784\u001B[0m       0.8548        \u001B[31m0.3752\u001B[0m  1.5659\n",
      "      4      \u001B[36m0.8495\u001B[0m        \u001B[32m0.3780\u001B[0m       \u001B[35m0.8556\u001B[0m        0.3774  1.6263\n",
      "      5      0.8465        \u001B[32m0.3766\u001B[0m       0.8548        0.3843  1.6230\n",
      "      6      0.8485        0.3767       0.8540        0.3834  1.5501\n",
      "      7      0.8483        \u001B[32m0.3759\u001B[0m       0.8548        0.3837  1.6309\n",
      "  epoch    f1_macro    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ----------  ------------  -----------  ------------  ------\n",
      "      1      \u001B[36m0.8412\u001B[0m        \u001B[32m0.3939\u001B[0m       \u001B[35m0.8518\u001B[0m        \u001B[31m0.3898\u001B[0m  1.5580\n",
      "      2      \u001B[36m0.8427\u001B[0m        \u001B[32m0.3784\u001B[0m       \u001B[35m0.8531\u001B[0m        \u001B[31m0.3832\u001B[0m  1.6253\n",
      "      3      \u001B[36m0.8516\u001B[0m        \u001B[32m0.3773\u001B[0m       \u001B[35m0.8584\u001B[0m        \u001B[31m0.3767\u001B[0m  1.5405\n",
      "      4      0.8487        \u001B[32m0.3750\u001B[0m       0.8565        \u001B[31m0.3757\u001B[0m  1.6269\n",
      "      5      0.8479        0.3758       0.8565        \u001B[31m0.3713\u001B[0m  1.5803\n",
      "      6      0.8482        \u001B[32m0.3739\u001B[0m       0.8558        0.3764  1.7227\n",
      "      7      0.8480        \u001B[32m0.3734\u001B[0m       0.8568        0.3748  1.8292\n",
      "  epoch    f1_macro    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ----------  ------------  -----------  ------------  ------\n",
      "      1      \u001B[36m0.8491\u001B[0m        \u001B[32m0.3926\u001B[0m       \u001B[35m0.8547\u001B[0m        \u001B[31m0.3766\u001B[0m  1.6397\n",
      "      2      0.8463        \u001B[32m0.3793\u001B[0m       0.8524        0.3884  1.5807\n",
      "      3      \u001B[36m0.8513\u001B[0m        \u001B[32m0.3776\u001B[0m       \u001B[35m0.8577\u001B[0m        \u001B[31m0.3764\u001B[0m  1.6319\n",
      "      4      0.8505        \u001B[32m0.3769\u001B[0m       0.8574        \u001B[31m0.3716\u001B[0m  1.5798\n",
      "      5      0.8410        \u001B[32m0.3751\u001B[0m       0.8504        0.3822  1.6479\n",
      "      6      0.8509        0.3757       \u001B[35m0.8583\u001B[0m        0.3740  1.5784\n",
      "      7      0.8505        \u001B[32m0.3735\u001B[0m       0.8562        0.3735  1.6340\n",
      "  epoch    f1_macro    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ----------  ------------  -----------  ------------  ------\n",
      "      1      \u001B[36m0.8505\u001B[0m        \u001B[32m0.3966\u001B[0m       \u001B[35m0.8565\u001B[0m        \u001B[31m0.3848\u001B[0m  1.6114\n",
      "      2      0.8481        \u001B[32m0.3815\u001B[0m       0.8551        \u001B[31m0.3839\u001B[0m  1.5900\n",
      "      3      0.8431        \u001B[32m0.3796\u001B[0m       0.8518        0.3868  1.6120\n",
      "      4      \u001B[36m0.8536\u001B[0m        \u001B[32m0.3787\u001B[0m       \u001B[35m0.8591\u001B[0m        \u001B[31m0.3741\u001B[0m  1.5737\n",
      "      5      0.8493        \u001B[32m0.3779\u001B[0m       0.8561        0.3761  1.7278\n",
      "      6      0.8525        \u001B[32m0.3761\u001B[0m       0.8588        \u001B[31m0.3718\u001B[0m  1.5631\n",
      "      7      0.8509        \u001B[32m0.3753\u001B[0m       0.8577        0.3789  1.6140\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.79      0.81     33710\n",
      "         1.0       0.85      0.84      0.85     25249\n",
      "         2.0       0.87      0.90      0.89     60222\n",
      "\n",
      "    accuracy                           0.86    119181\n",
      "   macro avg       0.85      0.84      0.85    119181\n",
      "weighted avg       0.86      0.86      0.86    119181\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bc = BaggingClassifier(base_estimator=net, n_estimators=5, max_samples=0.5, n_jobs=1)\n",
    "bc.fit(x_train.values.astype(np.float32), y_train.values.astype(np.float32))\n",
    "print(classification_report(y_true=y_test, y_pred=bc.predict(x_test.values.astype(np.float32))))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    f1_macro    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ----------  ------------  -----------  ------------  ------\n",
      "      1      \u001B[36m0.8501\u001B[0m        \u001B[32m0.3865\u001B[0m       \u001B[35m0.8580\u001B[0m        \u001B[31m0.3785\u001B[0m  4.0644\n",
      "      2      \u001B[36m0.8504\u001B[0m        \u001B[32m0.3787\u001B[0m       \u001B[35m0.8585\u001B[0m        \u001B[31m0.3674\u001B[0m  3.8430\n",
      "      3      0.8494        \u001B[32m0.3764\u001B[0m       0.8578        0.3705  3.8482\n",
      "      4      \u001B[36m0.8525\u001B[0m        \u001B[32m0.3760\u001B[0m       \u001B[35m0.8599\u001B[0m        0.3692  3.8354\n",
      "      5      0.8524        \u001B[32m0.3747\u001B[0m       \u001B[35m0.8606\u001B[0m        0.3681  3.8521\n",
      "  epoch    f1_macro    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ----------  ------------  -----------  ------------  ------\n",
      "      1      \u001B[36m0.8501\u001B[0m        \u001B[32m0.3870\u001B[0m       \u001B[35m0.8585\u001B[0m        \u001B[31m0.3694\u001B[0m  3.8308\n",
      "      2      \u001B[36m0.8512\u001B[0m        \u001B[32m0.3784\u001B[0m       \u001B[35m0.8596\u001B[0m        0.3703  3.8509\n",
      "      3      \u001B[36m0.8518\u001B[0m        \u001B[32m0.3772\u001B[0m       \u001B[35m0.8599\u001B[0m        0.3702  3.8947\n",
      "      4      \u001B[36m0.8529\u001B[0m        \u001B[32m0.3758\u001B[0m       \u001B[35m0.8605\u001B[0m        \u001B[31m0.3686\u001B[0m  3.7782\n",
      "      5      0.8522        \u001B[32m0.3753\u001B[0m       0.8602        \u001B[31m0.3664\u001B[0m  3.8404\n",
      "  epoch    f1_macro    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ----------  ------------  -----------  ------------  ------\n",
      "      1      \u001B[36m0.8520\u001B[0m        \u001B[32m0.3870\u001B[0m       \u001B[35m0.8592\u001B[0m        \u001B[31m0.3672\u001B[0m  3.8326\n",
      "      2      0.8504        \u001B[32m0.3780\u001B[0m       \u001B[35m0.8593\u001B[0m        \u001B[31m0.3662\u001B[0m  3.8288\n",
      "      3      0.8508        \u001B[32m0.3769\u001B[0m       \u001B[35m0.8595\u001B[0m        0.3674  3.8102\n",
      "      4      \u001B[36m0.8524\u001B[0m        \u001B[32m0.3762\u001B[0m       0.8591        0.3694  3.8383\n",
      "      5      \u001B[36m0.8528\u001B[0m        \u001B[32m0.3751\u001B[0m       \u001B[35m0.8604\u001B[0m        0.3677  3.8532\n",
      "  epoch    f1_macro    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ----------  ------------  -----------  ------------  ------\n",
      "      1      \u001B[36m0.8502\u001B[0m        \u001B[32m0.3862\u001B[0m       \u001B[35m0.8583\u001B[0m        \u001B[31m0.3771\u001B[0m  3.8187\n",
      "      2      0.8487        \u001B[32m0.3789\u001B[0m       0.8541        \u001B[31m0.3758\u001B[0m  3.8527\n",
      "      3      \u001B[36m0.8520\u001B[0m        \u001B[32m0.3767\u001B[0m       \u001B[35m0.8596\u001B[0m        \u001B[31m0.3664\u001B[0m  3.7818\n",
      "      4      0.8518        \u001B[32m0.3763\u001B[0m       0.8589        0.3692  3.8143\n",
      "      5      0.8516        \u001B[32m0.3750\u001B[0m       \u001B[35m0.8600\u001B[0m        0.3694  3.8093\n",
      "  epoch    f1_macro    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ----------  ------------  -----------  ------------  ------\n",
      "      1      \u001B[36m0.8507\u001B[0m        \u001B[32m0.3862\u001B[0m       \u001B[35m0.8570\u001B[0m        \u001B[31m0.3703\u001B[0m  3.9979\n",
      "      2      0.8504        \u001B[32m0.3787\u001B[0m       \u001B[35m0.8581\u001B[0m        \u001B[31m0.3701\u001B[0m  4.0067\n",
      "      3      0.8485        \u001B[32m0.3773\u001B[0m       0.8573        0.3723  3.9808\n",
      "      4      0.8488        \u001B[32m0.3755\u001B[0m       0.8567        0.3712  3.8448\n",
      "      5      \u001B[36m0.8530\u001B[0m        \u001B[32m0.3754\u001B[0m       \u001B[35m0.8601\u001B[0m        \u001B[31m0.3662\u001B[0m  3.8844\n",
      "  epoch    f1_macro    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ----------  ------------  -----------  ------------  ------\n",
      "      1      \u001B[36m0.8488\u001B[0m        \u001B[32m0.3885\u001B[0m       \u001B[35m0.8570\u001B[0m        \u001B[31m0.3746\u001B[0m  2.7019\n",
      "      2      0.8471        \u001B[32m0.3789\u001B[0m       0.8543        0.3806  2.7206\n",
      "      3      \u001B[36m0.8491\u001B[0m        \u001B[32m0.3775\u001B[0m       0.8563        0.3761  2.7087\n",
      "      4      0.8468        \u001B[32m0.3761\u001B[0m       0.8568        \u001B[31m0.3732\u001B[0m  2.6215\n",
      "      5      \u001B[36m0.8502\u001B[0m        \u001B[32m0.3760\u001B[0m       \u001B[35m0.8585\u001B[0m        \u001B[31m0.3721\u001B[0m  2.7390\n",
      "  epoch    f1_macro    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ----------  ------------  -----------  ------------  ------\n",
      "      1      \u001B[36m0.8533\u001B[0m        \u001B[32m0.3884\u001B[0m       \u001B[35m0.8606\u001B[0m        \u001B[31m0.3689\u001B[0m  2.7189\n",
      "      2      0.8530        \u001B[32m0.3794\u001B[0m       0.8606        \u001B[31m0.3678\u001B[0m  2.7956\n",
      "      3      0.8529        \u001B[32m0.3777\u001B[0m       \u001B[35m0.8608\u001B[0m        \u001B[31m0.3637\u001B[0m  2.7002\n",
      "      4      0.8524        \u001B[32m0.3765\u001B[0m       0.8604        0.3682  2.7779\n",
      "      5      \u001B[36m0.8534\u001B[0m        \u001B[32m0.3764\u001B[0m       \u001B[35m0.8609\u001B[0m        0.3641  2.7635\n",
      "  epoch    f1_macro    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ----------  ------------  -----------  ------------  ------\n",
      "      1      \u001B[36m0.8528\u001B[0m        \u001B[32m0.3883\u001B[0m       \u001B[35m0.8604\u001B[0m        \u001B[31m0.3665\u001B[0m  2.7768\n",
      "      2      \u001B[36m0.8539\u001B[0m        \u001B[32m0.3783\u001B[0m       \u001B[35m0.8612\u001B[0m        \u001B[31m0.3659\u001B[0m  2.7685\n",
      "      3      0.8531        \u001B[32m0.3775\u001B[0m       0.8604        0.3663  2.6954\n",
      "      4      0.8532        \u001B[32m0.3760\u001B[0m       0.8609        0.3663  2.7445\n",
      "      5      0.8506        \u001B[32m0.3752\u001B[0m       0.8583        0.3680  2.7520\n",
      "  epoch    f1_macro    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ----------  ------------  -----------  ------------  ------\n",
      "      1      \u001B[36m0.8512\u001B[0m        \u001B[32m0.3873\u001B[0m       \u001B[35m0.8583\u001B[0m        \u001B[31m0.3690\u001B[0m  2.6139\n",
      "      2      \u001B[36m0.8530\u001B[0m        \u001B[32m0.3782\u001B[0m       \u001B[35m0.8610\u001B[0m        \u001B[31m0.3636\u001B[0m  2.7700\n",
      "      3      0.8523        \u001B[32m0.3765\u001B[0m       0.8607        0.3652  2.7314\n",
      "      4      0.8516        \u001B[32m0.3751\u001B[0m       0.8586        0.3662  2.7666\n",
      "      5      0.8527        \u001B[32m0.3743\u001B[0m       \u001B[35m0.8612\u001B[0m        0.3644  2.6412\n",
      "  epoch    f1_macro    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ----------  ------------  -----------  ------------  ------\n",
      "      1      \u001B[36m0.8534\u001B[0m        \u001B[32m0.3893\u001B[0m       \u001B[35m0.8610\u001B[0m        \u001B[31m0.3666\u001B[0m  2.7087\n",
      "      2      0.8520        \u001B[32m0.3800\u001B[0m       0.8606        \u001B[31m0.3649\u001B[0m  2.7275\n",
      "      3      0.8517        \u001B[32m0.3785\u001B[0m       0.8599        \u001B[31m0.3642\u001B[0m  2.6618\n",
      "      4      0.8518        \u001B[32m0.3780\u001B[0m       0.8594        0.3694  2.7183\n",
      "      5      0.8523        \u001B[32m0.3771\u001B[0m       0.8599        0.3651  2.7727\n",
      "  epoch    f1_macro    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ----------  ------------  -----------  ------------  ------\n",
      "      1      \u001B[36m0.8493\u001B[0m        \u001B[32m0.3883\u001B[0m       \u001B[35m0.8560\u001B[0m        \u001B[31m0.3791\u001B[0m  2.7682\n",
      "      2      \u001B[36m0.8495\u001B[0m        \u001B[32m0.3789\u001B[0m       \u001B[35m0.8566\u001B[0m        \u001B[31m0.3743\u001B[0m  2.7519\n",
      "      3      \u001B[36m0.8506\u001B[0m        \u001B[32m0.3769\u001B[0m       \u001B[35m0.8582\u001B[0m        \u001B[31m0.3730\u001B[0m  2.6754\n",
      "      4      0.8474        \u001B[32m0.3763\u001B[0m       0.8566        \u001B[31m0.3729\u001B[0m  2.8674\n",
      "      5      0.8498        \u001B[32m0.3753\u001B[0m       \u001B[35m0.8583\u001B[0m        \u001B[31m0.3708\u001B[0m  2.8095\n",
      "  epoch    f1_macro    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ----------  ------------  -----------  ------------  ------\n",
      "      1      \u001B[36m0.8532\u001B[0m        \u001B[32m0.3874\u001B[0m       \u001B[35m0.8611\u001B[0m        \u001B[31m0.3653\u001B[0m  2.7739\n",
      "      2      0.8521        \u001B[32m0.3787\u001B[0m       0.8607        0.3687  2.7784\n",
      "      3      \u001B[36m0.8535\u001B[0m        \u001B[32m0.3778\u001B[0m       0.8600        \u001B[31m0.3649\u001B[0m  2.6687\n",
      "      4      0.8524        \u001B[32m0.3766\u001B[0m       0.8603        0.3650  2.7518\n",
      "      5      0.8533        \u001B[32m0.3754\u001B[0m       0.8605        0.3655  2.7650\n",
      "  epoch    f1_macro    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ----------  ------------  -----------  ------------  ------\n",
      "      1      \u001B[36m0.8478\u001B[0m        \u001B[32m0.3878\u001B[0m       \u001B[35m0.8573\u001B[0m        \u001B[31m0.3749\u001B[0m  2.7691\n",
      "      2      \u001B[36m0.8533\u001B[0m        \u001B[32m0.3782\u001B[0m       \u001B[35m0.8607\u001B[0m        \u001B[31m0.3675\u001B[0m  2.6475\n",
      "      3      0.8515        \u001B[32m0.3764\u001B[0m       0.8597        0.3692  2.7864\n",
      "      4      \u001B[36m0.8536\u001B[0m        \u001B[32m0.3756\u001B[0m       \u001B[35m0.8610\u001B[0m        \u001B[31m0.3668\u001B[0m  2.8207\n",
      "      5      0.8523        \u001B[32m0.3754\u001B[0m       0.8609        \u001B[31m0.3662\u001B[0m  2.8281\n",
      "  epoch    f1_macro    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ----------  ------------  -----------  ------------  ------\n",
      "      1      \u001B[36m0.8512\u001B[0m        \u001B[32m0.3878\u001B[0m       \u001B[35m0.8578\u001B[0m        \u001B[31m0.3680\u001B[0m  2.7632\n",
      "      2      \u001B[36m0.8538\u001B[0m        \u001B[32m0.3783\u001B[0m       \u001B[35m0.8613\u001B[0m        \u001B[31m0.3645\u001B[0m  2.9197\n",
      "      3      0.8505        \u001B[32m0.3771\u001B[0m       0.8585        0.3691  2.7807\n",
      "      4      0.8535        \u001B[32m0.3754\u001B[0m       0.8604        0.3659  2.6898\n",
      "      5      0.8517        \u001B[32m0.3743\u001B[0m       0.8600        0.3649  2.7778\n",
      "  epoch    f1_macro    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ----------  ------------  -----------  ------------  ------\n",
      "      1      \u001B[36m0.8496\u001B[0m        \u001B[32m0.3892\u001B[0m       \u001B[35m0.8571\u001B[0m        \u001B[31m0.3755\u001B[0m  2.7661\n",
      "      2      \u001B[36m0.8503\u001B[0m        \u001B[32m0.3804\u001B[0m       \u001B[35m0.8597\u001B[0m        \u001B[31m0.3657\u001B[0m  2.7659\n",
      "      3      \u001B[36m0.8519\u001B[0m        \u001B[32m0.3793\u001B[0m       \u001B[35m0.8607\u001B[0m        0.3664  2.8230\n",
      "      4      \u001B[36m0.8523\u001B[0m        \u001B[32m0.3778\u001B[0m       0.8594        0.3663  2.6382\n",
      "      5      \u001B[36m0.8540\u001B[0m        \u001B[32m0.3770\u001B[0m       \u001B[35m0.8615\u001B[0m        \u001B[31m0.3632\u001B[0m  2.7183\n",
      "  epoch    f1_macro    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ----------  ------------  -----------  ------------  ------\n",
      "      1      \u001B[36m0.8476\u001B[0m        \u001B[32m0.3885\u001B[0m       \u001B[35m0.8570\u001B[0m        \u001B[31m0.3738\u001B[0m  2.7233\n",
      "      2      \u001B[36m0.8499\u001B[0m        \u001B[32m0.3796\u001B[0m       \u001B[35m0.8572\u001B[0m        0.3746  2.6353\n",
      "      3      \u001B[36m0.8503\u001B[0m        \u001B[32m0.3779\u001B[0m       \u001B[35m0.8580\u001B[0m        \u001B[31m0.3729\u001B[0m  2.7111\n",
      "      4      0.8487        \u001B[32m0.3764\u001B[0m       0.8567        0.3777  2.7541\n",
      "      5      0.8496        \u001B[32m0.3756\u001B[0m       0.8577        \u001B[31m0.3724\u001B[0m  2.6342\n",
      "  epoch    f1_macro    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ----------  ------------  -----------  ------------  ------\n",
      "      1      \u001B[36m0.8522\u001B[0m        \u001B[32m0.3880\u001B[0m       \u001B[35m0.8603\u001B[0m        \u001B[31m0.3671\u001B[0m  2.9188\n",
      "      2      \u001B[36m0.8525\u001B[0m        \u001B[32m0.3789\u001B[0m       0.8594        0.3704  2.8244\n",
      "      3      \u001B[36m0.8533\u001B[0m        \u001B[32m0.3775\u001B[0m       \u001B[35m0.8611\u001B[0m        \u001B[31m0.3664\u001B[0m  2.7665\n",
      "      4      0.8437        \u001B[32m0.3767\u001B[0m       0.8499        0.3799  2.7151\n",
      "      5      \u001B[36m0.8541\u001B[0m        \u001B[32m0.3759\u001B[0m       \u001B[35m0.8616\u001B[0m        \u001B[31m0.3634\u001B[0m  2.8355\n",
      "  epoch    f1_macro    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ----------  ------------  -----------  ------------  ------\n",
      "      1      \u001B[36m0.8517\u001B[0m        \u001B[32m0.3872\u001B[0m       \u001B[35m0.8592\u001B[0m        \u001B[31m0.3695\u001B[0m  2.6849\n",
      "      2      \u001B[36m0.8521\u001B[0m        \u001B[32m0.3783\u001B[0m       \u001B[35m0.8592\u001B[0m        \u001B[31m0.3673\u001B[0m  2.7810\n",
      "      3      0.8521        \u001B[32m0.3771\u001B[0m       \u001B[35m0.8593\u001B[0m        \u001B[31m0.3656\u001B[0m  2.7932\n",
      "      4      \u001B[36m0.8528\u001B[0m        \u001B[32m0.3755\u001B[0m       \u001B[35m0.8607\u001B[0m        \u001B[31m0.3644\u001B[0m  2.7019\n",
      "      5      \u001B[36m0.8536\u001B[0m        \u001B[32m0.3748\u001B[0m       0.8606        \u001B[31m0.3636\u001B[0m  2.7997\n",
      "  epoch    f1_macro    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ----------  ------------  -----------  ------------  ------\n",
      "      1      \u001B[36m0.8525\u001B[0m        \u001B[32m0.3878\u001B[0m       \u001B[35m0.8598\u001B[0m        \u001B[31m0.3723\u001B[0m  2.8460\n",
      "      2      0.8502        \u001B[32m0.3784\u001B[0m       0.8591        \u001B[31m0.3664\u001B[0m  2.7134\n",
      "      3      0.8512        \u001B[32m0.3768\u001B[0m       0.8593        0.3684  2.7325\n",
      "      4      \u001B[36m0.8534\u001B[0m        \u001B[32m0.3758\u001B[0m       \u001B[35m0.8604\u001B[0m        \u001B[31m0.3662\u001B[0m  2.7229\n",
      "      5      \u001B[36m0.8535\u001B[0m        \u001B[32m0.3743\u001B[0m       \u001B[35m0.8612\u001B[0m        \u001B[31m0.3645\u001B[0m  2.7191\n",
      "  epoch    f1_macro    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ----------  ------------  -----------  ------------  ------\n",
      "      1      \u001B[36m0.8497\u001B[0m        \u001B[32m0.3901\u001B[0m       \u001B[35m0.8587\u001B[0m        \u001B[31m0.3693\u001B[0m  2.7183\n",
      "      2      \u001B[36m0.8515\u001B[0m        \u001B[32m0.3807\u001B[0m       0.8579        0.3707  2.6468\n",
      "      3      \u001B[36m0.8534\u001B[0m        \u001B[32m0.3794\u001B[0m       \u001B[35m0.8613\u001B[0m        \u001B[31m0.3636\u001B[0m  2.7304\n",
      "      4      \u001B[36m0.8542\u001B[0m        \u001B[32m0.3775\u001B[0m       \u001B[35m0.8617\u001B[0m        0.3647  2.7332\n",
      "      5      0.8527        \u001B[32m0.3768\u001B[0m       0.8599        0.3687  2.7434\n",
      "  epoch    f1_macro    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ----------  ------------  -----------  ------------  ------\n",
      "      1      \u001B[36m0.8463\u001B[0m        \u001B[32m0.3889\u001B[0m       \u001B[35m0.8549\u001B[0m        \u001B[31m0.3767\u001B[0m  2.7741\n",
      "      2      \u001B[36m0.8475\u001B[0m        \u001B[32m0.3800\u001B[0m       \u001B[35m0.8553\u001B[0m        0.3804  2.7796\n",
      "      3      0.8444        \u001B[32m0.3780\u001B[0m       0.8548        \u001B[31m0.3745\u001B[0m  2.7471\n",
      "      4      \u001B[36m0.8490\u001B[0m        \u001B[32m0.3763\u001B[0m       \u001B[35m0.8557\u001B[0m        \u001B[31m0.3744\u001B[0m  2.6872\n",
      "      5      0.8478        \u001B[32m0.3750\u001B[0m       \u001B[35m0.8571\u001B[0m        \u001B[31m0.3727\u001B[0m  2.7779\n",
      "  epoch    f1_macro    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ----------  ------------  -----------  ------------  ------\n",
      "      1      \u001B[36m0.8516\u001B[0m        \u001B[32m0.3883\u001B[0m       \u001B[35m0.8603\u001B[0m        \u001B[31m0.3683\u001B[0m  2.7472\n",
      "      2      0.8509        \u001B[32m0.3804\u001B[0m       0.8586        0.3712  2.8119\n",
      "      3      \u001B[36m0.8535\u001B[0m        \u001B[32m0.3776\u001B[0m       \u001B[35m0.8614\u001B[0m        \u001B[31m0.3646\u001B[0m  2.7567\n",
      "      4      \u001B[36m0.8543\u001B[0m        \u001B[32m0.3761\u001B[0m       \u001B[35m0.8622\u001B[0m        \u001B[31m0.3630\u001B[0m  2.7537\n",
      "      5      0.8537        \u001B[32m0.3756\u001B[0m       0.8618        0.3637  2.7582\n",
      "  epoch    f1_macro    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ----------  ------------  -----------  ------------  ------\n",
      "      1      \u001B[36m0.8519\u001B[0m        \u001B[32m0.3875\u001B[0m       \u001B[35m0.8591\u001B[0m        \u001B[31m0.3678\u001B[0m  2.7419\n",
      "      2      \u001B[36m0.8521\u001B[0m        \u001B[32m0.3793\u001B[0m       0.8585        0.3726  2.6256\n",
      "      3      \u001B[36m0.8530\u001B[0m        \u001B[32m0.3766\u001B[0m       \u001B[35m0.8595\u001B[0m        0.3689  2.7677\n",
      "      4      0.8529        \u001B[32m0.3750\u001B[0m       \u001B[35m0.8607\u001B[0m        \u001B[31m0.3650\u001B[0m  2.7633\n",
      "      5      0.8527        0.3754       \u001B[35m0.8608\u001B[0m        0.3684  2.7459\n",
      "  epoch    f1_macro    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ----------  ------------  -----------  ------------  ------\n",
      "      1      \u001B[36m0.8524\u001B[0m        \u001B[32m0.3872\u001B[0m       \u001B[35m0.8608\u001B[0m        \u001B[31m0.3678\u001B[0m  2.7396\n",
      "      2      0.8520        \u001B[32m0.3785\u001B[0m       0.8600        0.3681  2.7329\n",
      "      3      0.8518        \u001B[32m0.3762\u001B[0m       0.8592        \u001B[31m0.3648\u001B[0m  2.7656\n",
      "      4      \u001B[36m0.8533\u001B[0m        \u001B[32m0.3756\u001B[0m       0.8606        0.3668  2.6307\n",
      "      5      \u001B[36m0.8536\u001B[0m        \u001B[32m0.3745\u001B[0m       0.8603        0.3651  2.9744\n",
      "  epoch    f1_macro    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ----------  ------------  -----------  ------------  ------\n",
      "      1      \u001B[36m0.8537\u001B[0m        \u001B[32m0.3903\u001B[0m       \u001B[35m0.8607\u001B[0m        \u001B[31m0.3695\u001B[0m  2.7292\n",
      "      2      0.8514        \u001B[32m0.3803\u001B[0m       0.8599        \u001B[31m0.3656\u001B[0m  2.6449\n",
      "      3      0.8507        \u001B[32m0.3787\u001B[0m       0.8598        0.3660  2.7514\n",
      "      4      0.8485        \u001B[32m0.3780\u001B[0m       0.8583        0.3696  2.7194\n",
      "      5      0.8522        \u001B[32m0.3773\u001B[0m       0.8591        0.3656  2.7422\n",
      "  epoch    f1_macro    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ----------  ------------  -----------  ------------  ------\n",
      "      1      \u001B[36m0.8471\u001B[0m        \u001B[32m0.3884\u001B[0m       \u001B[35m0.8562\u001B[0m        \u001B[31m0.3774\u001B[0m  2.7688\n",
      "      2      \u001B[36m0.8493\u001B[0m        \u001B[32m0.3787\u001B[0m       \u001B[35m0.8573\u001B[0m        \u001B[31m0.3748\u001B[0m  2.6746\n",
      "      3      \u001B[36m0.8504\u001B[0m        \u001B[32m0.3776\u001B[0m       \u001B[35m0.8576\u001B[0m        \u001B[31m0.3730\u001B[0m  3.2061\n",
      "      4      0.8497        \u001B[32m0.3767\u001B[0m       \u001B[35m0.8581\u001B[0m        \u001B[31m0.3719\u001B[0m  3.1662\n",
      "      5      0.8492        \u001B[32m0.3753\u001B[0m       0.8578        \u001B[31m0.3718\u001B[0m  2.7224\n",
      "  epoch    f1_macro    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ----------  ------------  -----------  ------------  ------\n",
      "      1      \u001B[36m0.8524\u001B[0m        \u001B[32m0.3890\u001B[0m       \u001B[35m0.8596\u001B[0m        \u001B[31m0.3669\u001B[0m  2.7481\n",
      "      2      \u001B[36m0.8543\u001B[0m        \u001B[32m0.3799\u001B[0m       \u001B[35m0.8616\u001B[0m        0.3688  3.0640\n",
      "      3      0.8497        \u001B[32m0.3768\u001B[0m       0.8584        0.3675  3.1287\n",
      "      4      0.8538        0.3770       0.8611        \u001B[31m0.3642\u001B[0m  2.7296\n",
      "      5      0.8533        \u001B[32m0.3755\u001B[0m       0.8614        \u001B[31m0.3636\u001B[0m  2.7626\n",
      "  epoch    f1_macro    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ----------  ------------  -----------  ------------  ------\n",
      "      1      \u001B[36m0.8512\u001B[0m        \u001B[32m0.3877\u001B[0m       \u001B[35m0.8588\u001B[0m        \u001B[31m0.3715\u001B[0m  2.9111\n",
      "      2      \u001B[36m0.8530\u001B[0m        \u001B[32m0.3780\u001B[0m       \u001B[35m0.8606\u001B[0m        \u001B[31m0.3684\u001B[0m  2.6452\n",
      "      3      0.8506        \u001B[32m0.3770\u001B[0m       0.8571        0.3691  2.6565\n",
      "      4      \u001B[36m0.8534\u001B[0m        \u001B[32m0.3751\u001B[0m       \u001B[35m0.8608\u001B[0m        \u001B[31m0.3641\u001B[0m  2.5908\n",
      "      5      0.8529        \u001B[32m0.3751\u001B[0m       \u001B[35m0.8608\u001B[0m        0.3708  2.6737\n",
      "  epoch    f1_macro    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ----------  ------------  -----------  ------------  ------\n",
      "      1      \u001B[36m0.8531\u001B[0m        \u001B[32m0.3869\u001B[0m       \u001B[35m0.8608\u001B[0m        \u001B[31m0.3658\u001B[0m  2.6376\n",
      "      2      0.8497        \u001B[32m0.3787\u001B[0m       0.8590        0.3709  2.6366\n",
      "      3      0.8522        \u001B[32m0.3760\u001B[0m       0.8602        0.3671  2.5824\n",
      "      4      0.8524        \u001B[32m0.3757\u001B[0m       0.8607        \u001B[31m0.3649\u001B[0m  2.7309\n",
      "      5      0.8527        \u001B[32m0.3744\u001B[0m       0.8601        0.3659  2.7648\n",
      "  epoch    f1_macro    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ----------  ------------  -----------  ------------  ------\n",
      "      1      \u001B[36m0.8514\u001B[0m        \u001B[32m0.3897\u001B[0m       \u001B[35m0.8578\u001B[0m        \u001B[31m0.3747\u001B[0m  2.6415\n",
      "      2      0.8511        \u001B[32m0.3805\u001B[0m       0.8575        \u001B[31m0.3704\u001B[0m  3.0604\n",
      "      3      \u001B[36m0.8538\u001B[0m        \u001B[32m0.3787\u001B[0m       \u001B[35m0.8605\u001B[0m        \u001B[31m0.3652\u001B[0m  3.1389\n",
      "      4      0.8525        \u001B[32m0.3773\u001B[0m       0.8602        0.3670  2.6384\n",
      "      5      \u001B[36m0.8539\u001B[0m        \u001B[32m0.3771\u001B[0m       \u001B[35m0.8616\u001B[0m        0.3669  2.6350\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.79      0.80      0.80     36695\n",
      "         1.0       0.80      0.89      0.84     28333\n",
      "         2.0       0.90      0.85      0.87     64950\n",
      "\n",
      "    accuracy                           0.84    129978\n",
      "   macro avg       0.83      0.85      0.84    129978\n",
      "weighted avg       0.85      0.84      0.84    129978\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sc = StackingClassifier(estimators=[('model_1', net), ('model_2', net), ('model_3', net), ('model_4', net), ('model_5', net)],\n",
    "                        n_jobs=1, final_estimator=DecisionTreeClassifier(max_depth=3, class_weight='balanced'))\n",
    "sc.fit(x_train.values.astype(np.float32), y_train.values.astype(np.float32))\n",
    "print(classification_report(y_true=y_test, y_pred=sc.predict(x_test.values.astype(np.float32))))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    f1_macro    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ----------  ------------  -----------  ------------  ------\n",
      "      1      \u001B[36m0.8466\u001B[0m        \u001B[32m0.3860\u001B[0m       \u001B[35m0.8529\u001B[0m        \u001B[31m0.3758\u001B[0m  4.3343\n",
      "      2      \u001B[36m0.8528\u001B[0m        \u001B[32m0.3784\u001B[0m       \u001B[35m0.8600\u001B[0m        \u001B[31m0.3697\u001B[0m  3.8582\n",
      "      3      0.8513        \u001B[32m0.3768\u001B[0m       0.8592        \u001B[31m0.3680\u001B[0m  3.7865\n",
      "      4      0.8515        \u001B[32m0.3761\u001B[0m       0.8590        \u001B[31m0.3672\u001B[0m  3.8389\n",
      "      5      0.8523        \u001B[32m0.3746\u001B[0m       \u001B[35m0.8604\u001B[0m        \u001B[31m0.3662\u001B[0m  3.8969\n",
      "  epoch    f1_macro    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ----------  ------------  -----------  ------------  ------\n",
      "      1      \u001B[36m0.8513\u001B[0m        \u001B[32m0.3868\u001B[0m       \u001B[35m0.8590\u001B[0m        \u001B[31m0.3714\u001B[0m  3.8650\n",
      "      2      0.8448        \u001B[32m0.3786\u001B[0m       0.8533        0.3746  3.9192\n",
      "      3      \u001B[36m0.8517\u001B[0m        \u001B[32m0.3770\u001B[0m       \u001B[35m0.8599\u001B[0m        \u001B[31m0.3669\u001B[0m  3.8662\n",
      "      4      0.8507        \u001B[32m0.3755\u001B[0m       0.8578        0.3685  4.1609\n",
      "      5      0.8515        \u001B[32m0.3748\u001B[0m       0.8598        0.3671  3.9118\n",
      "  epoch    f1_macro    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ----------  ------------  -----------  ------------  ------\n",
      "      1      \u001B[36m0.8507\u001B[0m        \u001B[32m0.3866\u001B[0m       \u001B[35m0.8591\u001B[0m        \u001B[31m0.3701\u001B[0m  3.8481\n",
      "      2      \u001B[36m0.8522\u001B[0m        \u001B[32m0.3790\u001B[0m       \u001B[35m0.8602\u001B[0m        0.3713  3.8596\n",
      "      3      0.8483        \u001B[32m0.3772\u001B[0m       0.8574        0.3744  4.0925\n",
      "      4      0.8521        \u001B[32m0.3758\u001B[0m       0.8600        \u001B[31m0.3651\u001B[0m  3.9934\n",
      "      5      0.8507        \u001B[32m0.3751\u001B[0m       0.8594        0.3665  4.0579\n",
      "  epoch    f1_macro    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ----------  ------------  -----------  ------------  ------\n",
      "      1      \u001B[36m0.8493\u001B[0m        \u001B[32m0.3861\u001B[0m       \u001B[35m0.8550\u001B[0m        \u001B[31m0.3784\u001B[0m  3.8592\n",
      "      2      \u001B[36m0.8507\u001B[0m        \u001B[32m0.3786\u001B[0m       \u001B[35m0.8595\u001B[0m        \u001B[31m0.3667\u001B[0m  3.8455\n",
      "      3      0.8497        \u001B[32m0.3771\u001B[0m       0.8571        0.3742  3.8464\n",
      "      4      \u001B[36m0.8526\u001B[0m        \u001B[32m0.3760\u001B[0m       \u001B[35m0.8598\u001B[0m        0.3671  3.8946\n",
      "      5      0.8523        \u001B[32m0.3752\u001B[0m       \u001B[35m0.8598\u001B[0m        \u001B[31m0.3660\u001B[0m  3.8360\n",
      "  epoch    f1_macro    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ----------  ------------  -----------  ------------  ------\n",
      "      1      \u001B[36m0.8504\u001B[0m        \u001B[32m0.3866\u001B[0m       \u001B[35m0.8579\u001B[0m        \u001B[31m0.3739\u001B[0m  3.8073\n",
      "      2      \u001B[36m0.8512\u001B[0m        \u001B[32m0.3787\u001B[0m       \u001B[35m0.8598\u001B[0m        \u001B[31m0.3671\u001B[0m  4.0381\n",
      "      3      0.8496        \u001B[32m0.3766\u001B[0m       0.8567        0.3745  3.9287\n",
      "      4      \u001B[36m0.8517\u001B[0m        \u001B[32m0.3761\u001B[0m       \u001B[35m0.8598\u001B[0m        \u001B[31m0.3669\u001B[0m  3.9308\n",
      "      5      \u001B[36m0.8528\u001B[0m        \u001B[32m0.3749\u001B[0m       \u001B[35m0.8608\u001B[0m        \u001B[31m0.3649\u001B[0m  3.8502\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.78      0.80     36695\n",
      "         1.0       0.86      0.83      0.85     28333\n",
      "         2.0       0.86      0.91      0.89     64950\n",
      "\n",
      "    accuracy                           0.85    129978\n",
      "   macro avg       0.85      0.84      0.84    129978\n",
      "weighted avg       0.85      0.85      0.85    129978\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vc = VotingClassifier([('model_1', net), ('model_2', net), ('model_3', net), ('model_4', net), ('model_5', net)])\n",
    "vc.fit(x_train.values.astype(np.float32), y_train.values.astype(np.float32))\n",
    "print(classification_report(y_true=y_test, y_pred=vc.predict(x_test.values.astype(np.float32))))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "136d3de6453a21debda57ed276c6fcce71fde84e7957a47fdb8e529b49986e4b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
