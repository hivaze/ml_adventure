{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, balanced_accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, Normalizer, KBinsDiscretizer, LabelEncoder, StandardScaler, normalize\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from skorch.classifier import NeuralNetClassifier\n",
    "from skorch.callbacks import EpochScoring\n",
    "\n",
    "from functools import lru_cache\n",
    "from faiss import IndexLSH, IndexFlatL2, IndexIVFFlat, index_factory, omp_set_num_threads\n",
    "omp_set_num_threads(16) # faiss parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5586011 entries, 0 to 5586010\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Dtype  \n",
      "---  ------     -----  \n",
      " 0   x          float64\n",
      " 1   y          float64\n",
      " 2   z          float64\n",
      " 3   intensity  float64\n",
      " 4   class      int64  \n",
      "dtypes: float64(4), int64(1)\n",
      "memory usage: 213.1 MB\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('data/my_data_train_val.csv')\n",
    "train_df.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "                x           y           z   intensity       class\ncount  5586011.00  5586011.00  5586011.00  5586011.00  5586011.00\nmean      -237.41        2.78       -5.86        0.29       47.79\nstd        120.91       14.37        2.32        0.14       23.82\nmin       -536.61      -88.92      -29.01        0.00        0.00\n25%       -326.85       -7.12       -7.67        0.23       40.00\n50%       -223.92        1.63       -5.74        0.31       48.00\n75%       -137.77       12.85       -4.05        0.37       72.00\nmax        -50.00       94.58        4.34        0.99       80.00",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>x</th>\n      <th>y</th>\n      <th>z</th>\n      <th>intensity</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>5586011.00</td>\n      <td>5586011.00</td>\n      <td>5586011.00</td>\n      <td>5586011.00</td>\n      <td>5586011.00</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>-237.41</td>\n      <td>2.78</td>\n      <td>-5.86</td>\n      <td>0.29</td>\n      <td>47.79</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>120.91</td>\n      <td>14.37</td>\n      <td>2.32</td>\n      <td>0.14</td>\n      <td>23.82</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>-536.61</td>\n      <td>-88.92</td>\n      <td>-29.01</td>\n      <td>0.00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>-326.85</td>\n      <td>-7.12</td>\n      <td>-7.67</td>\n      <td>0.23</td>\n      <td>40.00</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>-223.92</td>\n      <td>1.63</td>\n      <td>-5.74</td>\n      <td>0.31</td>\n      <td>48.00</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>-137.77</td>\n      <td>12.85</td>\n      <td>-4.05</td>\n      <td>0.37</td>\n      <td>72.00</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>-50.00</td>\n      <td>94.58</td>\n      <td>4.34</td>\n      <td>0.99</td>\n      <td>80.00</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe().round(2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "                 x            y            z    intensity        class\ncount  5586011.000  5586011.000  5586011.000  5586011.000  5586011.000\nmean         0.615        0.500        0.694        0.295       47.786\nstd          0.248        0.078        0.069        0.140       23.824\nmin          0.000        0.000        0.000        0.000        0.000\n25%          0.431        0.446        0.640        0.232       40.000\n50%          0.643        0.493        0.698        0.313       48.000\n75%          0.820        0.555        0.748        0.374       72.000\nmax          1.000        1.000        1.000        1.000       80.000",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>x</th>\n      <th>y</th>\n      <th>z</th>\n      <th>intensity</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>5586011.000</td>\n      <td>5586011.000</td>\n      <td>5586011.000</td>\n      <td>5586011.000</td>\n      <td>5586011.000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.615</td>\n      <td>0.500</td>\n      <td>0.694</td>\n      <td>0.295</td>\n      <td>47.786</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.248</td>\n      <td>0.078</td>\n      <td>0.069</td>\n      <td>0.140</td>\n      <td>23.824</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.431</td>\n      <td>0.446</td>\n      <td>0.640</td>\n      <td>0.232</td>\n      <td>40.000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.643</td>\n      <td>0.493</td>\n      <td>0.698</td>\n      <td>0.313</td>\n      <td>48.000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.820</td>\n      <td>0.555</td>\n      <td>0.748</td>\n      <td>0.374</td>\n      <td>72.000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.000</td>\n      <td>1.000</td>\n      <td>1.000</td>\n      <td>1.000</td>\n      <td>80.000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "_train_df = pd.DataFrame(scaler.fit_transform(train_df.drop('class', axis=1)), columns=scaler.feature_names_in_)\n",
    "train_df = pd.concat([_train_df, train_df['class']], axis=1)\n",
    "train_df.describe().round(3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 827513 entries, 0 to 827512\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count   Dtype  \n",
      "---  ------     --------------   -----  \n",
      " 0   x          827513 non-null  float64\n",
      " 1   y          827513 non-null  float64\n",
      " 2   z          827513 non-null  float64\n",
      " 3   intensity  827513 non-null  float64\n",
      "dtypes: float64(4)\n",
      "memory usage: 25.3 MB\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv('data/my_data_test.csv')\n",
    "test_df.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "               x          y          z  intensity\ncount  827513.00  827513.00  827513.00  827513.00\nmean      -29.80       6.23      -1.88       0.31\nstd        12.13      12.17       0.93       0.14\nmin       -50.00     -59.65     -10.70       0.00\n25%       -40.04      -3.86      -2.54       0.24\n50%       -29.43       3.27      -2.18       0.32\n75%       -19.22      15.05      -1.34       0.38\nmax       -10.00      69.48       3.86       0.99",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>x</th>\n      <th>y</th>\n      <th>z</th>\n      <th>intensity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>827513.00</td>\n      <td>827513.00</td>\n      <td>827513.00</td>\n      <td>827513.00</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>-29.80</td>\n      <td>6.23</td>\n      <td>-1.88</td>\n      <td>0.31</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>12.13</td>\n      <td>12.17</td>\n      <td>0.93</td>\n      <td>0.14</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>-50.00</td>\n      <td>-59.65</td>\n      <td>-10.70</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>-40.04</td>\n      <td>-3.86</td>\n      <td>-2.54</td>\n      <td>0.24</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>-29.43</td>\n      <td>3.27</td>\n      <td>-2.18</td>\n      <td>0.32</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>-19.22</td>\n      <td>15.05</td>\n      <td>-1.34</td>\n      <td>0.38</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>-10.00</td>\n      <td>69.48</td>\n      <td>3.86</td>\n      <td>0.99</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.describe().round(2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "                x           y           z   intensity\ncount  827513.000  827513.000  827513.000  827513.000\nmean        1.042       0.519       0.813       0.309\nstd         0.025       0.066       0.028       0.143\nmin         1.000       0.160       0.549       0.000\n25%         1.020       0.464       0.794       0.242\n50%         1.042       0.502       0.804       0.323\n75%         1.063       0.567       0.830       0.384\nmax         1.082       0.863       0.985       1.000",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>x</th>\n      <th>y</th>\n      <th>z</th>\n      <th>intensity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>827513.000</td>\n      <td>827513.000</td>\n      <td>827513.000</td>\n      <td>827513.000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>1.042</td>\n      <td>0.519</td>\n      <td>0.813</td>\n      <td>0.309</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.025</td>\n      <td>0.066</td>\n      <td>0.028</td>\n      <td>0.143</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.000</td>\n      <td>0.160</td>\n      <td>0.549</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>1.020</td>\n      <td>0.464</td>\n      <td>0.794</td>\n      <td>0.242</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>1.042</td>\n      <td>0.502</td>\n      <td>0.804</td>\n      <td>0.323</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>1.063</td>\n      <td>0.567</td>\n      <td>0.830</td>\n      <td>0.384</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.082</td>\n      <td>0.863</td>\n      <td>0.985</td>\n      <td>1.000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.DataFrame(scaler.transform(test_df), columns=scaler.feature_names_in_)\n",
    "test_df.describe().round(3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(nrows=2, ncols=1, sharex=True, figsize=(12, 8))\n",
    "# sns.histplot(train_df.drop('class', axis=1).sample(frac=0.4), ax=axes[0]).set_title('Train')\n",
    "# sns.histplot(test_df, ax=axes[1]).set_title('Test')\n",
    "# # plt.xlim([-300, 50])\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 0, 10, 40, 44, 48, 50, 51, 70, 71, 72, 80])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(train_df['class'])\n",
    "label_encoder.classes_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Modeling"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from torch.utils.data.sampler import RandomSampler, BatchSampler\n",
    "import torch.nn.functional as F\n",
    "torch.cuda.is_available()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "use_cols = ['x', 'y', 'z']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "class PointCloudDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df, use_cols, n_neigh, use_neighs, q_bins, is_train = True):\n",
    "\n",
    "        self.df = df\n",
    "        self.is_train = is_train\n",
    "        self.use_cols = use_cols\n",
    "        if is_train:\n",
    "            self.target = label_encoder.transform(self.df['class'])\n",
    "            self.df = self.df.drop(['class'], axis=1)\n",
    "\n",
    "        self.nn_index = index_factory(len(self.use_cols), \"HNSW86,Flat\")\n",
    "        self.nn_index.parallel_mode = 1\n",
    "        self.nn_index.train(self.df[self.use_cols])\n",
    "        self.nn_index.add(self.df[self.use_cols])\n",
    "\n",
    "        self.n_neigh = n_neigh + 1\n",
    "        self.use_neighs = use_neighs\n",
    "\n",
    "        sample = self.nn_index.search(self.df[use_cols].iloc[np.random.randint(0, len(self.df), len(self.df) // 10)], self.n_neigh)[0][:, 1:]\n",
    "        self.quantilies  = torch.tensor(np.quantile(sample, np.linspace(0, 1, q_bins)))\n",
    "\n",
    "        # self.left_rule = torch.tensor(quantilies)\n",
    "        # self.right_rule = torch.tensor(np.concatenate([quantilies, [[[np.inf]]]])[1:])\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch = self.df.iloc[idx]\n",
    "        nbrs_dists, _ = self.nn_index.search(batch[self.use_cols], self.n_neigh)\n",
    "        nbrs_dists = torch.tensor(nbrs_dists[:, 1:])\n",
    "\n",
    "        quantile_features = torch.stack([torch.bitwise_and(nbrs_dists > q1, nbrs_dists < q2).sum(1) for q1, q2 in zip(self.quantilies, self.quantilies[1:])], dim=1)\n",
    "        intensity = torch.tensor((batch['intensity'] > 0.35).values, dtype=torch.float32).unsqueeze(1)\n",
    "        feat_array = torch.concat([nbrs_dists[:, :self.use_neighs], quantile_features, intensity], dim=1)\n",
    "\n",
    "        # quantile_features = torch.bitwise_and(nbrs_dists > self.left_rule, nbrs_dists < self.right_rule).sum(2).T\n",
    "        # intensity = torch.tensor((batch['intensity'] > 0.35).values, dtype=torch.float32).unsqueeze(1)\n",
    "        # feat_array = torch.concat([nbrs_dists[:, :self.use_neighs], quantile_features, intensity], dim=1)\n",
    "\n",
    "        if self.is_train:\n",
    "            return feat_array, torch.tensor(self.target[idx])\n",
    "        else:\n",
    "            return feat_array\n",
    "\n",
    "    @property\n",
    "    def n_features_in(self):\n",
    "        return self.use_neighs + self.quantilies.shape[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "split_ratio = 0.85\n",
    "train_set = train_df.sample(frac=split_ratio)\n",
    "val_set = train_df.iloc[~train_df.index.isin(train_set.index)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "4468809"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = PointCloudDataset(df=train_set, use_cols=use_cols, is_train=True, q_bins=15, n_neigh=200, use_neighs=10)\n",
    "len(train_dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "1117202"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset = PointCloudDataset(df=val_set, use_cols=use_cols, is_train=True, q_bins=15, n_neigh=200, use_neighs=10)\n",
    "len(val_dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "class TorchMLPClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 out_dim,\n",
    "                 dropout_p = 0.1,\n",
    "                 n_layers = 3\n",
    "                 ):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        hidden_dim = 2 * input_dim\n",
    "\n",
    "        layers = [nn.Sequential(\n",
    "            nn.BatchNorm1d(input_dim if i == 0 else hidden_dim),\n",
    "            nn.Linear(input_dim if i == 0 else hidden_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_p)\n",
    "        ) for i in range(n_layers)]\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            *layers,\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Linear(hidden_dim, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "TorchMLPClassifier(\n  (network): Sequential(\n    (0): Sequential(\n      (0): BatchNorm1d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (1): Linear(in_features=45, out_features=90, bias=True)\n      (2): GELU(approximate=none)\n      (3): Dropout(p=0.01, inplace=False)\n    )\n    (1): Sequential(\n      (0): BatchNorm1d(90, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (1): Linear(in_features=90, out_features=90, bias=True)\n      (2): GELU(approximate=none)\n      (3): Dropout(p=0.01, inplace=False)\n    )\n    (2): Sequential(\n      (0): BatchNorm1d(90, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (1): Linear(in_features=90, out_features=90, bias=True)\n      (2): GELU(approximate=none)\n      (3): Dropout(p=0.01, inplace=False)\n    )\n    (3): BatchNorm1d(90, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (4): Linear(in_features=90, out_features=11, bias=True)\n  )\n)"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TorchMLPClassifier(input_dim=train_dataset.n_features_in, out_dim=11)\n",
    "model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def validate(model, device, batch_size, criterion, p_bar):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        val_losses = []\n",
    "        val_f1 = []\n",
    "        val_acc = []\n",
    "\n",
    "        val_sampler = BatchSampler(RandomSampler(val_dataset), batch_size=batch_size, drop_last=True)\n",
    "        for b, batch_idx in enumerate(val_sampler):\n",
    "            p_bar.set_description(f'Validation | Batch {b}/{len(val_sampler)} | Epoch')\n",
    "\n",
    "            x, target = val_dataset[batch_idx]\n",
    "            x, target = x.to(device), target\n",
    "\n",
    "            preds = model(x).detach().cpu()\n",
    "\n",
    "            val_losses += [criterion(preds, target).item()]\n",
    "            val_f1 += [f1_score(target, preds.argmax(1), average='weighted')]\n",
    "            val_acc += [accuracy_score(target, preds.argmax(1))]\n",
    "\n",
    "        return np.array(val_losses).mean(), np.array(val_f1).mean(), np.array(val_acc).mean()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def train(model, device='cuda', batch_size=1024, epochs=10, lr=0.007):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr, weight_decay=0)\n",
    "    model.to(device)\n",
    "\n",
    "    p_bar = tqdm(enumerate(range(epochs)), total=epochs)\n",
    "    for i, epoch in p_bar:\n",
    "        epoch_losses = []\n",
    "\n",
    "        model.train()\n",
    "        train_sampler = BatchSampler(RandomSampler(train_dataset), batch_size=batch_size, drop_last=True)\n",
    "        for b, batch_idx in enumerate(train_sampler):\n",
    "            p_bar.set_description(f'Train | Batch {b}/{len(train_sampler)} | Epoch')\n",
    "\n",
    "            x, target = train_dataset[batch_idx]\n",
    "            x, target = x.to(device), target.to(device)\n",
    "\n",
    "            preds = model(x)\n",
    "            loss = criterion(preds, target)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_losses += [loss.item()]\n",
    "\n",
    "        val_l, val_f1, val_acc = validate(model, device, batch_size, criterion, p_bar)\n",
    "\n",
    "        print(f'Epoch {i} |'\n",
    "              f' Train Loss: {round(np.array(epoch_losses).mean(), 4)} |'\n",
    "              f' Val Loss: {round(val_l, 4)} |'\n",
    "              f' Val F1: {round(val_f1, 4)} |'\n",
    "              f' Val Acc: {round(val_acc, 4)}'\n",
    "              )\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/30 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b8c441cd0f9f4298acbacc92b0474833"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Train Loss: 1.5109 | Val Loss: 1.5725 | Val F1: 0.3735 | Val Acc: 0.4092\n",
      "Epoch 1 | Train Loss: 1.493 | Val Loss: 1.5716 | Val F1: 0.3718 | Val Acc: 0.412\n",
      "Epoch 2 | Train Loss: 1.4915 | Val Loss: 1.5676 | Val F1: 0.3758 | Val Acc: 0.4098\n",
      "Epoch 3 | Train Loss: 1.4903 | Val Loss: 1.5845 | Val F1: 0.3728 | Val Acc: 0.403\n",
      "Epoch 4 | Train Loss: 1.4898 | Val Loss: 1.5726 | Val F1: 0.3785 | Val Acc: 0.4136\n",
      "Epoch 5 | Train Loss: 1.4894 | Val Loss: 1.5614 | Val F1: 0.3756 | Val Acc: 0.4162\n",
      "Epoch 6 | Train Loss: 1.4889 | Val Loss: 1.5719 | Val F1: 0.3744 | Val Acc: 0.4108\n",
      "Epoch 7 | Train Loss: 1.4885 | Val Loss: 1.5666 | Val F1: 0.3706 | Val Acc: 0.4143\n",
      "Epoch 8 | Train Loss: 1.4886 | Val Loss: 1.5723 | Val F1: 0.3717 | Val Acc: 0.4104\n",
      "Epoch 9 | Train Loss: 1.4886 | Val Loss: 1.577 | Val F1: 0.3741 | Val Acc: 0.4108\n",
      "Epoch 10 | Train Loss: 1.4882 | Val Loss: 1.5704 | Val F1: 0.3754 | Val Acc: 0.4128\n",
      "Epoch 11 | Train Loss: 1.488 | Val Loss: 1.5711 | Val F1: 0.3771 | Val Acc: 0.4112\n",
      "Epoch 12 | Train Loss: 1.4879 | Val Loss: 1.5748 | Val F1: 0.3772 | Val Acc: 0.4085\n",
      "Epoch 13 | Train Loss: 1.4877 | Val Loss: 1.5866 | Val F1: 0.372 | Val Acc: 0.4046\n",
      "Epoch 14 | Train Loss: 1.4879 | Val Loss: 1.5689 | Val F1: 0.3716 | Val Acc: 0.4144\n",
      "Epoch 15 | Train Loss: 1.4878 | Val Loss: 1.5683 | Val F1: 0.3759 | Val Acc: 0.4139\n",
      "Epoch 16 | Train Loss: 1.4879 | Val Loss: 1.5753 | Val F1: 0.378 | Val Acc: 0.4143\n",
      "Epoch 17 | Train Loss: 1.4876 | Val Loss: 1.5674 | Val F1: 0.377 | Val Acc: 0.4153\n",
      "Epoch 18 | Train Loss: 1.4874 | Val Loss: 1.5723 | Val F1: 0.3743 | Val Acc: 0.4097\n",
      "Epoch 19 | Train Loss: 1.4874 | Val Loss: 1.5717 | Val F1: 0.3777 | Val Acc: 0.4116\n",
      "Epoch 20 | Train Loss: 1.4876 | Val Loss: 1.5646 | Val F1: 0.3811 | Val Acc: 0.4144\n",
      "Epoch 21 | Train Loss: 1.4872 | Val Loss: 1.5694 | Val F1: 0.3785 | Val Acc: 0.4092\n",
      "Epoch 22 | Train Loss: 1.4872 | Val Loss: 1.5641 | Val F1: 0.3808 | Val Acc: 0.4166\n",
      "Epoch 23 | Train Loss: 1.4872 | Val Loss: 1.5738 | Val F1: 0.3787 | Val Acc: 0.4115\n",
      "Epoch 24 | Train Loss: 1.4871 | Val Loss: 1.5635 | Val F1: 0.3808 | Val Acc: 0.414\n",
      "Epoch 25 | Train Loss: 1.4872 | Val Loss: 1.5771 | Val F1: 0.3766 | Val Acc: 0.4089\n",
      "Epoch 26 | Train Loss: 1.4871 | Val Loss: 1.5658 | Val F1: 0.3756 | Val Acc: 0.4136\n",
      "Epoch 27 | Train Loss: 1.487 | Val Loss: 1.5691 | Val F1: 0.3751 | Val Acc: 0.4106\n",
      "Epoch 28 | Train Loss: 1.4869 | Val Loss: 1.5815 | Val F1: 0.377 | Val Acc: 0.4083\n",
      "Epoch 29 | Train Loss: 1.4872 | Val Loss: 1.5743 | Val F1: 0.3705 | Val Acc: 0.4107\n"
     ]
    }
   ],
   "source": [
    "train(model, device='cuda', epochs=30, batch_size=8096)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "TorchMLPClassifier(\n  (network): Sequential(\n    (0): Sequential(\n      (0): BatchNorm1d(26, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (1): Linear(in_features=26, out_features=52, bias=True)\n      (2): GELU(approximate=none)\n      (3): Dropout(p=0.1, inplace=False)\n    )\n    (1): Sequential(\n      (0): BatchNorm1d(52, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (1): Linear(in_features=52, out_features=52, bias=True)\n      (2): GELU(approximate=none)\n      (3): Dropout(p=0.1, inplace=False)\n    )\n    (2): BatchNorm1d(52, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): Linear(in_features=52, out_features=11, bias=True)\n  )\n)"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cpu')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "<torch.utils.data.dataset.Subset at 0x7fca7e766f70>"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sample_sub = pd.read_csv('data/SampleSubmission.csv')\n",
    "sample_sub['—Ålass'] = test_predict\n",
    "sample_sub.to_csv('data/my_submission.csv', index=None)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
